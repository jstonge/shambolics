[
  {
    "objectID": "posts/netsci2023/index.html#section",
    "href": "posts/netsci2023/index.html#section",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "Contagion dynamics on higher-order networks\n\n\n\nIn class we learn that classic models are simple. We assume a well-mixed population where agents become infected when bouncing off each other."
  },
  {
    "objectID": "posts/netsci2023/index.html#section-1",
    "href": "posts/netsci2023/index.html#section-1",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "Contagion dynamics on higher-order networks\n\n\n\nOnce we are tired of taking people for molecules, we learn about agent-based models. We can include any detail we think that can influence a contagion. But the analysis part is tricky.\nBoth of these approaches lead to think in particular ways about contagion; for example how the structure of the network changes in the context of contagions. Think about how contact tracing during the COVID-19 pandemic. We wanted to trace the spread among individuals and how it was changing the network."
  },
  {
    "objectID": "posts/netsci2023/index.html#section-2",
    "href": "posts/netsci2023/index.html#section-2",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "Contagion dynamics on higher-order networks\n\n\n\nInstead of tweaking the network structure to see how individuals adapt to a contagion, we will keep the network as it is, and ask how changes in group norms and policies can hinder contagion (q1).\nDifferent groups have different norms about the right behaviors to adopt during a contagion.\nHere we see groups as collection of individuals who can put in place policies that limit the spread of the contagion. With this very general definition, we like to think of individuals sharing environment as a group. If they share a bad ventilation systems, for example, this will impact how they are gonna do during a contagion.\np.s. Related to what Caroline talked about yesterday. The contact rates will differ across population, and one way it happens is via group-level policies."
  },
  {
    "objectID": "posts/netsci2023/index.html#section-3",
    "href": "posts/netsci2023/index.html#section-3",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "Contagion dynamics on higher-order networks\n\n\n\nWe will assume that these change as a result of groups copying each other when trying to control the contagion.\nInstead of focusing on individual dynamics we’ll assume that groups have dynamics. And that groups are the relevant levels of abstract to study interventions in a contagion"
  },
  {
    "objectID": "posts/netsci2023/index.html#section-4",
    "href": "posts/netsci2023/index.html#section-4",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "The problem\n\nwhat if institutions of varying strength can hinder contagions?\n\n\n\nwhat is the impact of timescales and intergroup coupling on the co-evolution of contagions and institutions?\n\n\n\n\nWith that in mind, we ask the following questions.\nWhat if institutions try to reduce the spread of a contagion? Here this is really a question about the co-evolution of policy and collective dynamics.\n[READ QUESTION 2]\nHere I want to highlight on the importance of timescales for our approach. When I said that more of a bad things can be good, it really depends on the timescales at which institutions adapt to the epidemic. Fast and slow institutions have different impact on different epidemic scenarios."
  },
  {
    "objectID": "posts/netsci2023/index.html#section-5",
    "href": "posts/netsci2023/index.html#section-5",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "A group-based model with policy selection of contagion\n\nThe cartoon version of our model."
  },
  {
    "objectID": "posts/netsci2023/index.html#section-6",
    "href": "posts/netsci2023/index.html#section-6",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "Group-based model with policy selection of contagion\n\n\n\\(R = \\rho \\sum _{i,\\ell} i G_{i,\\ell}\\) and \\(Z_\\ell = \\dfrac{\\sum_{i} \\textrm{exp}\\left(-bi - c\\ell\\right) G_{i,\\ell}}{\\sum_{i} G_{i,\\ell}} \\;\\)\n\nWe keep track of infected \\(\\color{#800016ff}{i}\\) in groups of fixed size.\nThe rate of infection depends on institutional level \\(\\color{#00a0ffff}{\\ell}\\)\n\n\nIn this cartoon we show a single group in 3 different states. You can move to the right and get one more infected person, as shown in red. This happens at a certain rate, beta naught to the power of minus alpha, as indicated on top of the arrow. This is rate of infection within groups, or the local infection rate, is dependent on group institutional level, \\(\\ell\\). A larger alpha means more effective policies. Then when you move on the left, people recover. Simple SIS dynamics.\nThe term at the bottom say that people can get infected across groups too. We assume a simple mean-field approximation where people between groups can randomly bounce on each other. This is given by the product of inter-group coupling \\(\\rho\\) and the expected number of infected units in a randomly selected group."
  },
  {
    "objectID": "posts/netsci2023/index.html#section-7",
    "href": "posts/netsci2023/index.html#section-7",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "Group-based model with policy selection of contagion\n\n\n\\(R = \\rho \\sum _{i,\\ell} i G_{i,\\ell}\\) and \\(\\color{#e6872e}{Z_\\ell} = \\dfrac{\\sum_{i} \\textrm{exp}\\left(-bi - c\\ell\\right) G_{i,\\ell}}{\\sum_{i} G_{i,\\ell}} \\;\\)\n\nWe keep track of infected \\(\\color{#800016ff}{i}\\) in groups of fixed size.\nThe rate of infection depends on institutional level \\(\\color{#00a0ffff}{\\ell}\\)\n\n\nGroups can invest in policies of varying strength to become better at reducing transmission. For example a public health department enforcing mask mandates is stronger than simply encouraging social distancing.\nThis new dimension of institutional strength is represented by depth; groups having weak policies are the front and stronger policies are at the back.\nGroups explore institutional strategies, or policires with different impact and cost. They do so by copying each other in proportion to the relativeness fitness of each level. This relative fitness is given by the potential benefits of your level in reducing local infection rate.\nP.s. What LHD calls the Laboratory of group mind & rationality, norms, and values."
  },
  {
    "objectID": "posts/netsci2023/index.html#section-8",
    "href": "posts/netsci2023/index.html#section-8",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "Overall dynamics of our model\n\n\\[\n\\frac{d}{dt}G_{i,\\ell}= \\frac{d}{dt} \\mathbin{\\color{#004F80}{G_{i,\\ell}^{\\textrm{epi}}}} + \\frac{d}{dt} \\color{#e6872e}{G_{i,\\ell}^{\\textrm{sel}}} \\\n\\]\n\n\n\n\\(\\color{#004F80}{\\text{epi}} = \\text{epidemic dynamics}\\)\n\\(\\color{#e6872e}{\\text{sel}} = \\text{institutional selection process}\\)\n\n\nNow we have an infinite number of groups \\(G\\). We track the fraction of groups G with institional level \\(\\ell\\) that has \\(i\\) infected people. The overall dynamics is the composition of the epidemic dynamics and the policy selection process.\n[WAIT 2 sec.]\nOk now that we have seen the why and how, lets turn to the results.."
  },
  {
    "objectID": "posts/netsci2023/index.html#section-9",
    "href": "posts/netsci2023/index.html#section-9",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "Results"
  },
  {
    "objectID": "posts/netsci2023/index.html#section-10",
    "href": "posts/netsci2023/index.html#section-10",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "Thinking, fast and slow\n\n \n\nLets start with the ability of our model to reproduce a diversity of real-world scenarios.\nEach plot is going to be different combination of institutional copying rate, relative to transmission rate (fast or slow). On the vertical axis we have the proportion of people infected and the proportion of groups of that institutional strength. On the horizontal axis we have time. We’ll be interested the final outbreak size such as eradication and endemicity.\nIn this first case, the red curves show that fast institutions are able to contain a weak epidemic. The dotted line is the average prevalence rates across institutional level. The blue curves show you that in this case only relatively weak institutions were necessary to contain the epidemic. In the steady states, we mostly have institutions of level 1 and 2."
  },
  {
    "objectID": "posts/netsci2023/index.html#section-11",
    "href": "posts/netsci2023/index.html#section-11",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "Thinking, fast and slow\n \n\nNow what if have very slow institutions with a higher beta. They are able to control an emerging epidemic, but their relative slowness cause an initial large wave of cases. Then we have stronger institutions who keep exploring ways to save ressources, meaning that they try to spent time at lower, less costly institution levels. This can be seen as insitutions relaxing policing when they think that the epidemic is under control."
  },
  {
    "objectID": "posts/netsci2023/index.html#section-12",
    "href": "posts/netsci2023/index.html#section-12",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "Thinking, fast and slow\n \n\nCompare this situation with slightly faster institutions. As before, here strong institutions temporarily control an existing outbreak before partially relaxing. But now strong institutions spend more time in low institutions, without taking the time to assess policy effectiveness before removing them. This, a fast rate of reducing your policies while the pandemic there, lead to a situation where prevalence rate stay relatively high.\nFor those who care, the relative impact of institutional adaptation is where timescale separation appears."
  },
  {
    "objectID": "posts/netsci2023/index.html#section-13",
    "href": "posts/netsci2023/index.html#section-13",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "Thinking, fast and slow\n \n\nFinally we look at highly endemicity scenario, or when we accept to live with relatively high rate. The interplay of fast imitation and a stronger epidemic prevents strong institutions to emerge (compared to upper right plot). This worst case scenario means that the emerging epidemic becomes highly endemic.\nYou’ll note that in all cases there is a damped osciallatory patterns. This is the result of negative feedback loop between infection spread and institutional level. In a nutshell, we see that weaker institutions copy stronger institutions but it takes time for the policies to kick in. This makes this institutional level looks bad, bringing other institutions to level down.\nOk, institutions explore institutional strategies to reduce local infection rate by copying each other. What else might happen when some groups invest their ressources and other can see it.."
  },
  {
    "objectID": "posts/netsci2023/index.html#section-14",
    "href": "posts/netsci2023/index.html#section-14",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "Institutional Free-riding\n\n\n\n\\(I_\\ell\\) is the equilibrium prevalence\n\\(\\beta\\) transmission rate\n\\(\\eta=0.05\\) is a relatively fast copying rate!\n\n\n\nHere we plot how the equilibrium prevalence, I ell, changes as a function of transmission rate beta. With relatively fast imitation rate, we show how as you increase beta, you get a monotonic increase of prevalence rate. Typical results that you would normally expect."
  },
  {
    "objectID": "posts/netsci2023/index.html#section-15",
    "href": "posts/netsci2023/index.html#section-15",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "Institutional Free-riding\n\n\n\n\\(\\tilde{I}_\\ell\\): prevalence rate if only institutions of your level existed\n\n\n\n[bullet point]\nIn other words, how would a world where we can see how weak institutions would do if they didn’t benefit from stronger institutions.\nYou can see the bumps, or the onset, of stronger levels. Different level emerge at different transmission rate. When higher level activates, being able to fight show the value of investing. But we’ll see it varies alot by copying rate."
  },
  {
    "objectID": "posts/netsci2023/index.html#section-16",
    "href": "posts/netsci2023/index.html#section-16",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "Institutional Free-riding\n\n\n\n\\(\\Delta_\\ell &lt; 0\\): Institutional Free-riding\n\\(\\Delta_\\ell &gt; 0\\): Being exploited\n\nwhere \\(\\Delta_\\ell = (I_\\ell - \\tilde{I}_\\ell)/(I_\\ell + \\tilde{I}_\\ell)\\)\n\n\nBy comparing the world in which you are only with institutions of your level to a world where you can benefit from other groups, we can have a sense of institutional free-riding.\nYou can see that if the prevalence of this second plot, the one you are by yourself, is bigger than the first plot, you get a negative value. You are doing better when other, stronger institutions are there than where you are by yourself. You are free-riding."
  },
  {
    "objectID": "posts/netsci2023/index.html#section-17",
    "href": "posts/netsci2023/index.html#section-17",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "Institutional Free-riding\n\n\n\n\\(\\Delta_\\ell &lt; 0\\): Institutional Free-riding\n\\(\\Delta_\\ell &gt; 0\\): Being exploited\n\nwhere \\(\\Delta_\\ell = (I_\\ell - \\tilde{I}_\\ell)/(I_\\ell + \\tilde{I}_\\ell)\\)\n\n\nOn the other hand, if you are doing worst when other institutions are in the system, you are being exploited.\nValue of 1 here means that if you only had the strongest institutions, you wouldn’t have any case. Value between 0 and 1 indicate that weaker institutions are bringing the system down.\nThen there is a hierarchy of free-riding where stronger institution get exploited by weaker institutions, with middle ones exploiting the one above and being exploited by the one below."
  },
  {
    "objectID": "posts/netsci2023/index.html#section-18",
    "href": "posts/netsci2023/index.html#section-18",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "Call for action\n\n\nTo see the call for action, forget the red curves and focus on the dotted lines for varying copying rate…\n\n\nOur final result is about the virtue of institutional patience. We have global prevalence rate on the vertical axis as a function of transmission rate. As before, higher transmission rate means higher prevalence rate."
  },
  {
    "objectID": "posts/netsci2023/index.html#section-19",
    "href": "posts/netsci2023/index.html#section-19",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "Call for action\n\n\nTo see the call for action, forget the red curves and focus on the dotted lines for varying copying rate…\n\n\nWe know that by slowing down copying rate, we give more time for institutions to assess the effectiveness of policies."
  },
  {
    "objectID": "posts/netsci2023/index.html#section-20",
    "href": "posts/netsci2023/index.html#section-20",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "Call for action\n\n\nTo see the call for action, forget the red curves and focus on the dotted lines for varying copying rate…\n\n\nWe saw that slower institutions did better in the endemic scenario. Here we can clearly see this happenning by taking the difference of equilibrium prevalence rate of the faster and slower curves."
  },
  {
    "objectID": "posts/netsci2023/index.html#section-21",
    "href": "posts/netsci2023/index.html#section-21",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "Call for action\n\n\nTo see the call for action, forget the red curves and focus on the dotted lines for varying copying rate.\n\n\nIn the middle regime, something interesting is happenning. You can see that faster institutions, the black curve, are too reactive in a sense. They don’t wait to see if copying make sense or not.\nBut slower institution “need” more difference in fitness to explore higher levels. Another way to say it is that slower institutions give enough time for that fitness gap to emerge, meaning that higher levels will be explored.\nOur punchline is that higher “transmission rate increase the need for institutions more than it increases contagon”.\nOne last thing, why this shape? Remember what I said about the onset. When it goes down, higher level are getting activated. The call for actions mean that weakest institutions will copy, when slow enough, because it is worth it."
  },
  {
    "objectID": "posts/netsci2023/index.html#section-22",
    "href": "posts/netsci2023/index.html#section-22",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "Food for thought\n\nRich dynamics emerge from group-based models without the need for complex mechanisms (but complicated math).\nSlower institutions help because they do not relax when the epidemic peaks.\nBut we can’t engineer that. How can we get closer to that scenario?\nWhat do we say to policy makers? Can we test that?"
  },
  {
    "objectID": "posts/netsci2023/index.html#section-23",
    "href": "posts/netsci2023/index.html#section-23",
    "title": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions",
    "section": "",
    "text": "Thanks\nKeep an eye on the oncoming preprint.\nBig thanks to collaborators.\n\n\n\n\n\nPlay with the model @joint-lab/call-for-action\nCode for the model jstonge/InstitutionalDynamics.jl/\n\n\n\n\n\nJonathan St-Onge - CCSS 2023"
  },
  {
    "objectID": "posts/mongoDB/index.html",
    "href": "posts/mongoDB/index.html",
    "title": "How to MongoDB",
    "section": "",
    "text": "To work with mongoDB, I highly recommend using the mongoDB Compass, mongossh and pymongo.\nKey refs:"
  },
  {
    "objectID": "posts/mongoDB/index.html#basics",
    "href": "posts/mongoDB/index.html#basics",
    "title": "How to MongoDB",
    "section": "Basics",
    "text": "Basics\nConnect to papersDB\n\nmongoshpymongo\n\n\n[direct: mongos] test&gt; use('papersDB');\n\n\nfrom pymongo import MongoClient\nuri = f\"mongodb://cwward:{pw}@wranglerdb01a.uvm.edu:27017/?authSource=admin&readPreference=primary&appname=MongoDB%20Compass&directConnection=true&ssl=false\"\nclient = MongoClient(uri)\ndb = client[\"papersDB\"]\n\n\n\nLooking at collections within our DB\n[direct: mongos] test&gt; show collections;\nLook at already existing indexes\n\nmongoshpython\n\n\n[direct: mongos] papersDB&gt; db.metadata.getIndexes()\n\n\ndb.metadata.index_information()\n\n\n\nestimate total count documents…\n[direct: mongos] papersDB&gt; db.metadata.estimatedDocumentCount()\nor count documents based on a particular query:\n[direct: mongos] papersDB&gt; db.metadata.countDocuments({year: 2018})\nyou can ‘explain’ queries. Super useful to understand how mongoDB works and query performances.\n[direct: mongos] papersDB&gt; var exp = db.metadata.explain(\"executionStats\")\n[direct: mongos] papersDB&gt; exp.find({title: \"Scale-free networks are rare\"}) //  totalDocsExamined: 19,786,006\n[direct: mongos] papersDB&gt; db.metadata.createIndex({year: -1}); // create index based on \n[direct: mongos] papersDB&gt; exp.find({title: \"Scale-free networks are rare\", year: 2018}).limit(1) // executionTimeMillis: 3851; totalKeysExamined: 786,497\ndropping indexes\n\nmongoshpython\n\n\n[direct: mongos] papersDB&gt; db.metadata.dropIndex(\"year_-1\")\n\n\ndb.metadata.drop_index(\"year_-1\")"
  },
  {
    "objectID": "posts/mongoDB/index.html#useful-queries",
    "href": "posts/mongoDB/index.html#useful-queries",
    "title": "How to MongoDB",
    "section": "Useful queries",
    "text": "Useful queries\n1-find papers based on paper_id\n\nmongoshpython\n\n\n[direct: mongos] papersDB&gt; db.pdf_parses.findOne({ paper_ID: \"77497072\"});\n\n\ndb.metadata.find_one({ \"paper_ID\": \"77497072\"})\n\n\n\n2-find papers based on paper_id and year\n[direct: mongos] papersDB&gt; db.metadata.findOne({ year: {$gt: 2015, $lt: 2022}, paper_id: \"f1b4361a1978e93018c5fdfe4856250152676ffb\" })\n3-Query papers with body_text see stack overflow\n[direct: mongos] papersDB&gt; db.pdf_parses.findOne({ body_text: { $gt: true, $type: 'array', $ne: [] }})\n4-Query authors in an array\n\nmongoshsql\n\n\n[direct: mongos] papersDB&gt; db.metadata.findOne({ 'authors.0.first': \"Aaron\" })\n[direct: mongos] papersDB&gt; db.metadata.findOne({ 'authors.first': \"Aaron\" })\n[direct: mongos] papersDB&gt; db.metadata.findOne({ 'authors.last': \"Clauset\" })\n[direct: mongos] papersDB&gt; db.metadata.aggregate({ $filter : { authors.last : { $eq : \"Clauset\" } } });\n\n\nSELECT * FROM metadata\nWHERE authors.last = \"Clauset\";\n\n\n\n5-Query concept_oa based on common ancestors\n\ncompass\n\n\n{$and: [{'ancestors.display_name': 'Ecology'}, {'ancestors.display_name': 'Computer science'}]}\n\n\n\n\nRegex queries\n1-Testing regex with sample in a pipeline\ndb.s2orc.aggregate([{ $sample: { size: 1 } }, { $addFields: { result: { $regexFindAll: { input: '$content.text', regex: /data/i } } } }])\ndb.s2orc.find({\n   content.text: {\n      $regex: /data availability statement/i\n   }\n})"
  },
  {
    "objectID": "posts/mongoDB/index.html#creating-index",
    "href": "posts/mongoDB/index.html#creating-index",
    "title": "How to MongoDB",
    "section": "Creating index",
    "text": "Creating index\nCreate index based on descending year\n[direct: mongos] papersDB&gt; db.metadata.createIndex({year: -1});\nFrom Percona, this allows to improve all the queries that find documents with a condition and the year field, like the following:\n[direct: mongos] papersDB&gt; db.metadata.find( { year : 2018 } ) \n[direct: mongos] papersDB&gt; db.metadata.find( { title : \"Scale-free networks are rare\", year : 2018 } )\n[direct: mongos] papersDB&gt; db.metadata.find( { year : { $gt : 2020} } )\n[direct: mongos] papersDB&gt; db.metadata.find().sort( { year: -1} ).limit(10)\nCreate index based on authors (Multikey indexes)\n[direct: mongos] papersDB&gt; db.metadata.createIndex( { authors: 1 } )\n[direct: mongos] papersDB&gt; db.metadata.find( { authors.last: \"Clauset\" } )\nCreate index based on year and has_body_text (include a Partial indexes and Unique) In order for the partial index to be used the queries must contain a condition on the year and body_text field.\n[direct: mongos] papersDB&gt; db.metadata.createIndex(\n   { \"paper_id\": 1 },\n   { unique: true },\n   { partialFilterExpression: { year : { $gt: 2018 }, body_text: { $gt: true, $type: 'array', $ne: [] } } }\n)\n\n[direct: mongos] papersDB&gt; db.metadata.find( { paper_id: \"77490322\", year: { $gt: 2018}, body_text: { $gt: true, $type: 'array', $ne: []} } )\nCreate index based on year (asc) and bounded by 1950-60\n[direct: mongos] papersDB&gt; exp.find({\"year\": {$gte: 1950, $lte: 1960}, \"paper_id\": \"77490322\"}).limit(1) // executionTimeMillis: 360429; totalKeysExamined: 2024098\n[direct: mongos] papersDB&gt; db.metadata.createIndex({year:1}, { partialFilterExpression: { year : { $gte: 1950, $lte: 1960 } } });\n[direct: mongos] papersDB&gt; exp.find({\"year\": {$gte: 1950, $lte: 1960}, \"paper_id\": \"77490322\"}).limit(1) // executionTimeMillis: 68676; totalKeysExamined: 406162\nCreate index with partialFilterExpression\n\npython\n\n\n# We use \"$type\" because \"$ne\" not supported when creating PFE\ndb.metadata.create_index(\n [(\"year\", ASCENDING)], \n name=\"bucket 1950-1960\", \n partialFilterExpression={ \"year\" : { \"$gte\": 1950, \"$lte\": 1960 }, \"abstract\": {\"$type\": \"string\"} }\n)\nWith this one, we get to totalDocsExamined: 79,721 examined (v. totalDocsExamined: 720,475)."
  },
  {
    "objectID": "posts/mongoDB/index.html#text-queries",
    "href": "posts/mongoDB/index.html#text-queries",
    "title": "How to MongoDB",
    "section": "Text queries",
    "text": "Text queries\nTo do text queries, you must create an index first:\n[direct: mongos] papersDB&gt; db['publication-venues'].createIndex( { name: \"text\" } )\nthen you can query as follow\n[direct: mongos] papersDB&gt; db['publication-venues'].find({ $text: {$search: \"ecology\", $caseSensitive: false} }).limit(1)"
  },
  {
    "objectID": "posts/mongoDB/index.html#updating-documents",
    "href": "posts/mongoDB/index.html#updating-documents",
    "title": "How to MongoDB",
    "section": "Updating documents",
    "text": "Updating documents\n1- Update s2fos\n\nmongoshpython\n\n\ndb.metadata.updateOne({paper_id: '84881204', year: {$gte: 1950, $lte: 1960}}, {'$set': {'s2fos_field_of_study': ['Medicine']}}})\n\n\nq = {\"paper_id\": '84881204', \"year\": { \"$gte\": 1950, \"$lte\": 1960 }}\nnew_values = {\"$set\": { \"s2fos_field_of_study\": ['Medicine']} }\ndb.metadata.update_one(q, new_values)\n\n\n\n2- Remove a field\ndb.metadata.updateOne({paper_id: '84881204', year: {$gte: 1950, $lte: 1960}}, {$unset: {s2fos: \"\"}})"
  },
  {
    "objectID": "posts/mongoDB/index.html#useful-aggregated-queries",
    "href": "posts/mongoDB/index.html#useful-aggregated-queries",
    "title": "How to MongoDB",
    "section": "Useful aggregated queries",
    "text": "Useful aggregated queries\nFind duplicated rows (not sure it is working yet)\n[direct: mongos] papersDB&gt; const aggregation = [\n    {\"$group\" : { \"_id\": \"$paper_id\", \"count\": { \"$sum\": 1 } } },\n    {\"$match\": {\"_id\" :{ \"$ne\" : null } , \"count\" : {\"$gt\": 1} } }, \n    {\"$project\": {\"paper_id\" : \"$_id\", \"_id\" : 0} }\n]\n\n[direct: mongos] papersDB&gt; db.pdf_parses.aggregate(aggregation);\n\nLookups is an aggregate query (first way of doing it)\n[direct: mongos]db.s2orc.aggregate([\n   { $lookup: {\n      from: \"papers\", localField: \"corpusid\", foreignField: \"corpusid\", as: \"paper_metadata\"\n      } }\n   ])\n\n\nSetting a new field based on old field\n# here we add https://doi.org/ to externalids.DOI to facilitate lookup\n# with works_oa. \ndb.papers.update_many(\n    {},\n    [\n        { \"$set\": { \n            \"doi\": { \n                \"$cond\": [\n                    { \"$ne\": [\"$externalids.DOI\", None] },\n                    { \"$concat\": [\"https://doi.org/\", \"$externalids.DOI\"] },\n                    None\n                ]\n                }\n            } }\n    ]\n)"
  },
  {
    "objectID": "posts/mongoDB/index.html#advanced-query",
    "href": "posts/mongoDB/index.html#advanced-query",
    "title": "How to MongoDB",
    "section": "Advanced query",
    "text": "Advanced query\n\nComplex lookups\nOkay, we want to perform a $lookup to know which DOI in papers (s2orc) are also in works_oa. This is a costly operation, so we make sure to have the right index first (we created a doi field in papers). We need the index publication_year/year, concepts/s2fieldofstudy, and doi.\ndb.works_oa.create_index([(\"publication_year\", ASCENDING), (\"concepts.diplay_name\", ASCENDING), (\"doi\", ASCENDING)])\ndb.papers.create_index([(\"year\", ASCENDING), (\"s2fieldsofstudy.category\", ASCENDING), (\"doi\", ASCENDING)])\npipeline = [\n    {\n        \"$match\": { \n             \"$and\":  [ \n                { \"year\": 1960 },\n                # { \"s2fieldsofstudy.category\", \"Biology\"  }\n            ]\n        }\n   },\n   {\n      \"$lookup\": {\n         \"from\": \"works_oa\",\n         \"localField\": \"doi\",\n         \"foreignField\": \"doi\",\n         \"let\": { \"col1_doi\": \"$doi\" },\n         \"pipeline\": [ {\n            \"$match\": {\n               \"$expr\": { \n                \"$and\": [\n                    { \"$eq\": [ \"publication_date\", \"1960-12-30\" ] },\n                    { \"$eq\": [ \"concepts.display_name\", \"Biology\" ] },\n                    { \"$in\": [ \"$$col1_doi\", \"$doi\" ] }\n                ]\n                }\n            }\n         } ],\n         \"as\": \"matches\"\n      }\n   },\n   {\n        \"$match\": { \"matches\": { \"$ne\": [] } }\n    }\n] \n\nres = list(db.paper_test.aggregate(pipeline))"
  },
  {
    "objectID": "posts/mongoDB/index.html#document-embeddings",
    "href": "posts/mongoDB/index.html#document-embeddings",
    "title": "How to MongoDB",
    "section": "Document embeddings",
    "text": "Document embeddings\nEmbed one collection into a second collection not sure it is working yet, this is a chatgpt answer\n[direct: mongos] papersDB&gt; db.collection1.update({name: \"John Doe\"}, {$set: {address: db.collection2.findOne({address: \"123 Main St\"})}})"
  },
  {
    "objectID": "posts/rise_of_comp/index.html",
    "href": "posts/rise_of_comp/index.html",
    "title": "The rise of computational works 1",
    "section": "",
    "text": "These days there’s a computational version of everything. Computational biology, computational musicology,computational archaeology,and so on, ad infinitum. Even movies are going digital. Dan Jurafsky, 2006\nAlready in 2006, there was a feeling that science was becoming more and more computational. But the rise of computational works is not evenly distributed. While some fields have a long history of using computer science to meet their computational needs, others are concerned that the process of representing their objects of study in a digital format may distort them. Yet others think that programming, say in the humanities, is mostly about acquiring marketable skills that transfer to industry, and as such it goes against long-term values of humanists.\nUnlike previous technologies, programming is able to creep into any field as it permeates many facets of scientific work, e.g. statistics, communication, visualization, simulation, data collection, etc. While it is true that some objects of study are more challenging to represent on computers, the rate at which computational methods are nevertheless being adopted by computational enthusiasts could create a wedge between them and more reluctant fields. This imbalance might even lead to a situation where the adoption of computational tools gives certain individuals or groups a disproportionate amount of influence, especially in fields undergoing a significant shift towards computational works. For example, there is evidence that institutions with more resources to fund larger research groups already increase disproportionately faculty productivity. What if institutions with a greater labor advantage also benefit the most from this computational turn. Those who embrace computational tools may have an advantage in terms of visibility and funding, potentially allowing them to more easily disseminate their ideas.\nA strong disparity in methods might have important consequences on the evolution of ideas in science. We ask ourselves, is the rise of computational work is a source of epistemic inequality? Does it benefit groups already favored because of their greater labor resources and institutional prestige? If the cost of learning programming is low enough for some individuals, and those individuals cluster together, will we see a gap forming even within the field? Will field of studies that are more computational spill over into other fields?\nTo assess whether the rise of computational work has an effect on the evolution of ideas in science, we need to be able to identify computational works. Without this first step, we won’t be able to measure the effects of groups adopting computational methods, or how labor advantages interact with the rise of programming. There are surprisingly few studies quantifying the relative adoption of computational works in different fields. Recent advances in the availability of large-scale data and NLP tools make this possibility more accessible than ever before.\nWe define computational works as projects that seek to understand complex systems using visualizations, simulations, and/or inference processes that require computers and programming languages. Here a few examples, slightly biased from my experience, in chronological order:\nplot = Plot.plot({\n  y: {ticks: null},\n  width,\n  marks: [\n    Plot.text(\n      data_manual,\n      Plot.dodgeY({\n        x: \"date\",\n        text: \"author\",\n        title: d =&gt; `${d.author} (${d.date.getFullYear()}):\\n${d.desc.slice(0, 130).concat('\\n', d.desc.slice(130))}`,\n        r: 30,\n        fill: \"category\",\n        lineWidth: 7,\n        anchor: \"middle\"\n      })\n    ),\n    Plot.ruleY([0], {dy: 175}),\n    Plot.ruleX(d3.range(1960,2020,5), {x:d =&gt; (new Date(`${d}-01-01`)), opacity: 0.3, lineType: 'dashed'})\n  ]\n})\n\nplot.legend(\"color\")\nAll of these papers share the fact that they could not have been done without extensive computers and related computer skills. By extensive, I mean that these papers wouldn’t be doable without computers and programming skills because it would require too much time or effort.1 In cases where computer science and mathematics have an application, or are used in a field, it typically requires skills that go beyond a traditional curriculum. For example, Burrows’ analysis of Jane Austen’s text required skills that go beyond the literary studies curriculum.\nNote that some articles are computational in the sense that they deal with computational stuff, but they remain conceptual. We include them because papers with computational methods ought to cite them.\nOk, so how are we gonna classify computational works? Here is the plan schema:\nFirst, we look at articles containing the computational keyword, which will include variations of it because s2_search is a kind of elastic search. This is imperfect but it should be better than asking for digital or programming. Then, we project these paper embeddings into lower dimensions with umap. Finally, we use hdbscan, a density-based clustering algorithm, to cluster similar papers.\nThat’s it. The hope is that because document embeddings are informed by title, abstract, and citations, papers that are more computational ought to cluster together. We will evaluate the performance of our pipeline on the subset of papers that we manually identified as computational while reviewing the literature.\nWe already did some of that, which you can find here."
  },
  {
    "objectID": "posts/rise_of_comp/index.html#footnotes",
    "href": "posts/rise_of_comp/index.html#footnotes",
    "title": "The rise of computational works 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere was a time when women were the computers. The original Monte Carlo algorithm was calculated by them during WWII. The calculations were simple enough that it was technically possible for humans to perform them. The trajectory of the first Apollo mission was also computed by women. But as we get deeper into the second half of the 20th century, this is no longer the case↩︎"
  },
  {
    "objectID": "posts/s2orc_viz/index.html",
    "href": "posts/s2orc_viz/index.html",
    "title": "S2ORC Viz",
    "section": "",
    "text": "Abstracts and pdf parsed shows the total number of papers and the number of papers for which either the abstract or pdf is parsed.\ns2fos v. mag is a showdown between the semantic scholar classification scheme and the microsoft academic graph one. Papers are grouped by fields and year, and we keep track of the parsing extent for each.\n\n\nimport {addTooltips} from \"@mkfreeman/plot-tooltip\"\n\n\n\n\n\n\n\ndata = FileAttachment(\"count_field_and_decade.csv\").csv({ typed: true })\n\n\n\n\n\n\n\nviewof schema = Inputs.select([\"s2fos\", \"mag\", \"both\"], { label: \"Schema\" })\nviewof show_pct = Inputs.toggle({label: \"Normalize\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbstracts and pdf parseds2fos v. magraw data\n\n\n\n\np1 = Plot.plot({\n    marginLeft: 100,\n    height: 600,\n    width: 1200,\n    marginBottom: 100,\n    x: { label: null, tickRotate: 45 },\n    y: { \n      grid: true, \n      percent: show_pct ? true : false, \n      label: show_pct ? \"↑ rep. (%)\" : \"↑ n papers\"\n     },\n    marks: [\n      Plot.barY(data.filter(d =&gt; d.year &lt; 2020), Plot.groupX({y: \"sum\"}, { x: \"year\", y: d =&gt; schema == 'both' ? d[\"n\"] : d[`n_${schema}`], fill: \"parsing\", order: \"sum\", offset: show_pct ? \"expand\" : null }))\n    ]\n})\np1.legend(\"color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof mark_type = Inputs.select([\"stacked\", \"line\"], { label: \"Plot type\" })\nviewof parsing = Inputs.select([\"abstract\", \"pdf\", \"all\"], { label: \"Parsing extent\" })\nviewof chosen_group = Inputs.select([\"STEM\", \"Social Science\", \"Misc\"], { label: \"Group\" })\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata_f = data.filter(d =&gt; d.group == chosen_group && d.year &lt; 2020 && d.parsing == parsing)\n\nline_plot = () =&gt; {\n  const xy_norm = Plot.normalizeY({\n          x: \"year\",  y:  d =&gt; schema == 'both' ? d[\"n\"] : d[`n_${schema}`], stroke: \"field\", title: \"field\", basis: \"first\"\n      })\n  const xy = {\n          x: \"year\",  y:  d =&gt; schema == 'both' ? d[\"n\"] : d[`n_${schema}`], stroke: \"field\", title: \"field\"\n      }\n  return addTooltips(Plot.plot({\n      marginLeft: 100,\n      height: 600,\n      width: 1200,\n      marginBottom: 50,\n      x: { label: null, tickRotate: 45 },\n      y: { \n        percent: show_pct ? true : false, \n        grid: true, \n        label: show_pct ? \"↑ Norm. over first value, showing relative growth\" : \"↑ n papers\" },\n      marks: [\n        Plot.line(data_f, show_pct ? xy_norm : xy)\n      ]\n  }))\n}\n\nstack_plot = () =&gt; {\n  const xy = { x: \"year\", y: d =&gt; schema == 'both' ? d[\"n\"] : d[`n_${schema}`], z: \"field\", order: \"sum\",  offset: show_pct ? \"expand\" : null }\n\n  return addTooltips(Plot.plot({\n        marginLeft: 100,\n        height: 600,\n        width: 1200,\n        marginBottom: 50,\n        x: { label: null, tickRotate: 45 },\n        y: { \n          percent: show_pct ? true : false, \n          grid: true, \n          label: show_pct ? \"↑ rep. (%)\" : \"↑ tot papers\" },\n        marks: [\n          Plot.barY(data_f, Plot.stackY({...xy, fill: \"field\", title: d =&gt; `${d.field} (${d.n})` })),\n        ]\n    }))\n}\n\n\np3 = mark_type == 'stacked' ? stack_plot() : line_plot()\n\np3.legend(\"color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInputs.table(data)\n\n\n\n\n\n\n\n\n\n\nRemarks\n\nBiology is an encompassing field. I wish we could divide it further.\nIn my opinion the biggest difference beween magand s2fos schema is that the later track the emergerce of computer science, which is nice.\n\n\n\nNotes\nClassifying articles into fields is non-trivial. But at least there is work on the topic.\nFor a while, the microsoft academic graph (mag) was standard. Many researchers used the top mag fields of study as their main taxonomy. Now that the mag project at microsoft is deprecated (Dec. 31, 2021.; see here), AllenAI’s semantic scholar is arguably one of the best contender to become the next standard. Contrary to MAG, they provide a unified api for the citation graphs of papers and they released a fraction of their overall paper nodes as parsed text. Announced after mag deprecation, the semantic scholar databse did some more work on the classification scheme, which can be found here. This is what we use and compare here, as did other before us.\nWe note that all the current schemes agree that papers have overlapping fields and are hierarchical. This is something that the mag field made explicit through the use of a hierarhical algorithm for unsupervised topic classification. The s2_fos scheme drops the hierarchy and focus on the top fields using a (simpler) classifier."
  },
  {
    "objectID": "posts/complaints/index.html",
    "href": "posts/complaints/index.html",
    "title": "Airlines Complaints TripAdvisors",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/charting_a_phd/index.html",
    "href": "posts/charting_a_phd/index.html",
    "title": "Charting a PhD",
    "section": "",
    "text": "As computing becomes more affordable, data more accessible, and the Web continues to grow, a growing number of research groups are shifting towards computational approaches. Computational approaches might be attractive to many groups, as they might lead to new kinds of inquiries, and/or draw more attention and funding over traditional methods. The foundation of many computational methods is rooted in (open source) computer programming. On the one hand, more (early-career) researchers face the challenge of learning to code without the proper computational environment to support them, while on the other hand, groups may encourage or implicitly exert pressure to their members to learn to code, without realizing the personal costs involved. We investigate this growing tension between groups and individuals when learning to code, as well as the factors that drive the relative cost and benefits of learning to code at the individual and institional level.\nWe construct a theoretical model to investigate the tension between costs and benefits at both the individual and group level. We aim to develop a model in which we assess whether the prohibitive costs for individuals can be justified from the perspective of the group. Consider, for instance, a situation in the humanities where a lack of computational infrastructure results in a high cost associated with learning to code. This could exacerbate the retention rate of certain individuals in academia, but from the perspective of the group, the cost would be lower as only one individual successfully learning to code could bring long-term benefits to the group.\nWe corroborate our theoretical models with an empirical results. To estimate the relative cost-benefits of learning to code, we conduct a survey on the motivations, challenges, and institutional support for students who undertake a turn in programming at the University of Vermont. This qualitative is complemented by a bibliometric study in which we quantify the impact of the computational transition of research groups on scientific productivity, work reproducibility, and group diversity. This empirical inquiry let us highlight how the cost and benefits of learning to code depend on multiple levels of organization, that is, how specific individuals are integrated into groups and organizations. To do so, we must identify which groups experience a computational transition. We are in the process of building a database that comprise over 200 million papers and their associated metadata, including citations, authors, venues, and institutions, in order to track the shift towards computational practices. The size and richness of our database is not only relevant to us, but to other research groups interested in studying data luminosity, scientific feuds, or the rise of open source programming practices in science."
  },
  {
    "objectID": "posts/charting_a_phd/index.html#footnotes",
    "href": "posts/charting_a_phd/index.html#footnotes",
    "title": "Charting a PhD",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that some of the articles are about the gap between statistical education in ecology and current methods used nowadays. Underlying this argument lies a gap between traditional statistics and often computational methods requiring to various to know how to program.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jonathan St-Onge",
    "section": "",
    "text": "twitter\n  \n  \n    \n     Github\n  \n\n  \n  \nI am a second year Ph.D. student at UVM Complex Systems Center in Burlington. I am interested in computational social science, cultural evolution and literate programming.\n\n\n\n\n\nNatural language processing\nNetworks\nBayesian Modeling\nData visualization, visual essays, interactive articles\n\n\n\n\n\nPh.D. in Complex Systems and Datas Science | June 2021 - present University of Vermont, Burlington | VT, CA\nM.A in Philosophy | Sept 2017 - June 2019 Université du Québec à Montréal, Montreal | Quebec, CA\nB.A in Behavioral neuroscience | Sept 2015 - June 2017 McGill University, Montreal | Quebec, CA"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Jonathan St-Onge",
    "section": "",
    "text": "Natural language processing\nNetworks\nBayesian Modeling\nData visualization, visual essays, interactive articles"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jonathan St-Onge",
    "section": "",
    "text": "Ph.D. in Complex Systems and Datas Science | June 2021 - present University of Vermont, Burlington | VT, CA\nM.A in Philosophy | Sept 2017 - June 2019 Université du Québec à Montréal, Montreal | Quebec, CA\nB.A in Behavioral neuroscience | Sept 2015 - June 2017 McGill University, Montreal | Quebec, CA"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "shambolics",
    "section": "",
    "text": "Emerging call for action: The complex and paradoxical co-evolution of contagions and institutions\n\n\n\n\n\n\n\n\n\n\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nCDAE stats\n\n\n\n\n\n\n\nCDAE\n\n\nInteractive\n\n\n\n\nStatistics is hard. Computational statistics makes it a bit better.\n\n\n\n\n\n\nSep 24, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nS2ORC Viz\n\n\n\n\n\n\n\nVisualization\n\n\nNLP\n\n\nS2ORC\n\n\nInteractive\n\n\n\n\nI always forget about the s2orc database details. Lets have them here.\n\n\n\n\n\n\nSep 24, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nOpenAlex Viz\n\n\n\n\n\n\n\nVisualization\n\n\nNLP\n\n\nopenAlex\n\n\nInteractive\n\n\n\n\nI always forget about the openAlex database details. Lets have them here.\n\n\n\n\n\n\nSep 24, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nThe rise of computational works 1\n\n\n\n\n\n\n\nVisualization\n\n\nNLP\n\n\nS2ORC\n\n\nInteractive\n\n\nSciSci\n\n\n\n\nWhere, how, and when did computational stuff became popular in science?\n\n\n\n\n\n\nSep 24, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nThe rise of computational works 3\n\n\n\n\n\n\n\nVisualization\n\n\nNLP\n\n\nS2ORC\n\n\nInteractive\n\n\nSciSci\n\n\n\n\nThe computational works strike back\n\n\n\n\n\n\nSep 24, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nHow to accelerate MongoDB\n\n\n\n\n\n\n\nref\n\n\nmongoDB\n\n\n\n\nDoubt everything and verify.\n\n\n\n\n\n\nSep 24, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/mongoDB_opt/index.html",
    "href": "posts/mongoDB_opt/index.html",
    "title": "How to accelerate MongoDB",
    "section": "",
    "text": "In this case study, we want to augment metadata of papers from allenAi’s S2ORC database with metadata from openAlex. Both metadata are represented as mongoDB collections in our local database. Augmented one collection with another in mongoDB amounts to perform a$lookup from papers (s2orc) to works_oa (openAlex).\n\n\nCode\nreturn_top_5 = { \"$limit\": 5 }\nonly_keep_existing_matches = { \"$match\": { \"matches\": { \"$ne\": [] } } }\n\n\n\nIn theory\nWe’ll try to understand the following strategies from the documentation: - Create Queries that Ensure Selectivity - The ESR (Equality, Sort, Range) Rule\nBasically, selectivity is the yes/no question game again. Low selective questions are the one you use to get rid of as many people as you can. Highly selective question are basically asking if it is this guy or this guy. The documentation suggest by starting a compound index that starts with low-selectivity (is this a famous person?), then you get to the more nitti-gritty. This is supposed to be intuitive.\nIn our case, we have from lower selectivity to highest selectivity:\n\ns2fieldofstudy (n=24)\nconcepts (42K total; but lvl=0/n=21; lvl=0)\nyear (~270, but unevenly distributed)\njournals/venues\ndoi/mag\n\n\nlen(db.papers.distinct(\"year\"))\nlen(db.papers.distinct(\"s2fieldsofstudy.category\"))\nlen(db.works_oa_test.distinct(\"concepts.display_name\"))\n\n41998\n\n\n\n\n1960\nThe first step of optimizing our code is to work with simpler collections. We will limit our analysis to papers from 1960. Here is the pymongo code to create collections:\n\n\nCode\ndb.works_oa_test.insert_many( list(db.works_oa.find(\n    { \"publication_year\": { \"$in\": [1960, 1961] }},\n    { \"concepts.display_name\": 1, \"publication_year\": 1, \"doi\": 1, \"ids\": 1 }\n    )) )\n\ndb.papers_test.insert_many( list(db.papers.find(\n    { \"year\": { \"$in\": [1960, 1961] } },\n    { \"doi\": 1, \"externalids.MAG\": 1, \"s2fieldsofstudy\": 1 }\n    ))\n)\n\n\n\n\nS2orc has 811397 papers with field of study in 1960 (0.39% of corpus)\nOpenAlex has 949171 papers in 1960 (0.38% of corpus)\n\n\n1960 is .2% of our whole corpus. We should keep that in mind as we try to scale up our operations to the full dataset. A document in our simplified S2ORC collection looks like:\n\n\nNone\n\n\nA document in our simplified openAlex collection looks like:\n\n\n{'_id': ObjectId('642044928238317efbef9371'),\n 'concepts': [{'display_name': 'Political science'}],\n 'doi': 'https://doi.org/10.6028/nbs.mp.237',\n 'ids': {'doi': 'https://doi.org/10.6028/nbs.mp.237',\n         'openalex': 'https://openalex.org/W4232940716'},\n 'mag': None,\n 'publication_year': 1960}\n\n\nWe’re off to a bad start. We note that the s2orc DB don’t keep the address prefix of the DOI. We find easier to add the missing prefix in s2orc than remove it from openalex.\n\n\nCode\nstart_time = time.time()\ndb.papers_test.update_many(\n    {},\n    [\n        { \"$set\": { \n            \"doi\": { \n                \"$cond\": [\n                    { \"$ne\": [\"$externalids.DOI\", None] },\n                    { \"$concat\": [\"https://doi.org/\", \"$externalids.DOI\"] },\n                    None\n                ]\n                }\n            } }\n    ]\n)\n\nprint(\"Setting a field on ~400K took --- %s seconds ---\" % (time.time() - start_time))\n\n\nSetting a field on ~400K took --- 1.8457493782043457 seconds ---\n\n\nThis cell took about 11sec for 400k, this meens that (100/.2)*11 / 60 = 83 minutes to run this on the full corpus.\n\nprint(\"hey\")\n# res = list(db.works_oa.aggregate( [\n#    { '$addFields': { \n#       'main_concept': { \n#          \"$filter\": { \n#             \"input\": \"$concepts\", \n#             \"as\": \"c\", \n#             \"cond\": { \"$eq\": [\"$$c.level\", 0 ]}\n#             }\n#          }\n#       }\n#    },\n#    {\n#       \"$project\": { \"main_concept.display_name\": 1 }\n#    }\n# ] ))\n\n# db.works_oa.bulk_write([ \n#     UpdateOne( {'_id': ObjectId(doc['_id'])}, {'$set': {'main_concept': doc['main_concept']['display_name']}} ) \n#     for doc in res\n#     ])\n\nhey\n\n\n\n\nWhat’s the worst query we can do?\nWe can now proceed to our first lookup. As a very first step, note that we will match only on existing dois on the papers_test (s2orc) side. If not, this is not even gonna run because the output is too large:\n\nstart_time = time.time()\n\ns2orc_doi_is_string = { \"$match\": { \"doi\": { \"$type\": \"string\" }} }\n\ns2orc_to_oa_doi_lookup ={\n      \"$lookup\": {\n         \"from\": \"works_oa_test\",\n         \"localField\": \"doi\",\n         \"foreignField\": \"doi\",\n         \"as\": \"matches\"\n      }\n   }\n\npipeline = [\n   s2orc_doi_is_string,\n   s2orc_to_oa_doi_lookup,\n   only_keep_existing_matches,\n]\n\nassert 'doi' in db.papers_test.find_one().keys(), 'missing field in paper test'\nassert 'doi' in db.works_oa_test.find_one().keys(), 'missing field in oa'\n\nres = list(db.papers_test.aggregate( pipeline ))\nprint(\"Process finished --- %s seconds ---\" % (time.time() - start_time))\n\nWe ended up cancelling the task because it was still too long (&gt; 3mins is too long for me). I just wanted to show this step to mention that when we work with mongoDB (our databases in general), indexes make a world of difference.\n\n\n\n\n\n\nTip\n\n\n\nAssigning descriptive names to variables for each aggregation stage enhances the reader’s comprehension of the purpose of the stage while emphasizing the similarities between the optimization variations being performed. For example, the example below clearly demonstrates that we are executing the same pipeline both with and without indexes.\n\n\n\n\nIndex on doi\nHere’s how we go about creating indexes\n\ndb.works_oa_test.create_index([(\"doi\", ASCENDING)])\ndb.papers_test.create_index([(\"doi\", ASCENDING)])\n\n'doi_1'\n\n\nMatching papers_test on works_oa again:\n\nstart_time = time.time()\n\ns2orc_doi_is_string = { \"$match\": { \"doi\": { \"$type\": \"string\" }} }\n\ns2orc_to_oa_doi_lookup ={\n      \"$lookup\": {\n         \"from\": \"works_oa_test\",\n         \"localField\": \"doi\",\n         \"foreignField\": \"doi\",\n         \"as\": \"matches\"\n      }\n   }\n\nres = list(db.papers_test.aggregate( [\n    s2orc_doi_is_string,\n    s2orc_to_oa_doi_lookup,\n    only_keep_existing_matches\n] ))\n\nf3 = \"$type(doi)=string\"\ni3 = \"doi\"\nd3 = \"2orc =&gt; oa\"\ntime3 = time.time() - start_time\nprint(\"Process finished --- %s seconds ---\" % time3)\nprint(f\"# hit: {len(res)}\")\n\nProcess finished --- 0.04578113555908203 seconds ---\n# hit: 0\n\n\nAlso, lets check if the other way around is faster\n\nstart_time = time.time()\n\noa_doi_is_string = { \"$match\": { \"doi\": { \"$type\": \"string\" }} }\n\noa_to_s2orc_doi_lookup = {\n      \"$lookup\": {\n         \"from\": \"papers_test\",\n         \"localField\": \"doi\",\n         \"foreignField\": \"doi\",\n         \"as\": \"matches\"\n      }\n   }\n\nres = list(db.works_oa_test.aggregate( [\n    oa_doi_is_string,\n    oa_to_s2orc_doi_lookup,\n    only_keep_existing_matches\n] ))\n\ntime4 = time.time() - start_time\nprint(\"Process finished --- %s seconds ---\" % time4)\nprint(f\"# hit: {len(res)}\")\n\nProcess finished --- 23.03083562850952 seconds ---\n# hit: 0\n\n\nLooking up papers_test in works_oa took 28 seconds, while it took 30 seconds for the reverse direction. This make sense considering that papers_test contains fewer papers. If the information we are interested in resides in papers_test, it is better to prioritize that direction. For example, if our focus is on text analysis and we only have text data for papers in papers_test (S2ORC), then the additional papers in openAlex become irrelevant since we know that there is no corresponding text available for those papers.\n\nTrying out the concise method for lookup introduced in MongoDB 5.0\n\nstart_time = time.time()\n\nconcise_s2orc_to_oa_doi_lookup = {\n      \"$lookup\": {\n         \"from\": \"works_oa_test\",\n         \"localField\": \"doi\",\n         \"foreignField\": \"doi\",\n         \"let\": { \"s2orc_doi\": \"$doi\" },\n         \"pipeline\": [ {\n            \"$match\": {\n               \"$expr\": { \"$eq\": [ \"$$s2orc_doi\", \"$doi\" ] }\n            }\n         } ],\n         \"as\": \"matches\"\n      }\n   }\n\nres = list(db.papers_test.aggregate( [\n    s2orc_doi_is_string,\n    concise_s2orc_to_oa_doi_lookup,\n    only_keep_existing_matches\n] )) \n\ntime5 = time.time() - start_time\nprint(\"Process finished --- %s seconds ---\" % time5)\nprint(f\"# hit: {len(res)}\")\n\nProcess finished --- 0.039494991302490234 seconds ---\n# hit: 0\n\n\nIt took longer, which is not great. But this approach let us filter doi in the foreign collection, potentially reducing the items we have to search:\n\nstart_time = time.time()\n\noa_doi_is_string_pipeline = [ {\n    \"$match\": {\n        \"doi\": { \"$type\": \"string\" } ,\n        \"$expr\": { \"$eq\": [ \"$$s2orc_doi\", \"$doi\" ] }\n        }\n    } ]\n\nconcise_s2orc_to_oa_doi_lookup = {\n      \"$lookup\": {\n         \"from\": \"works_oa_test\",\n         \"localField\": \"doi\",\n         \"foreignField\": \"doi\",\n         \"let\": { \"s2orc_doi\": \"$doi\" },\n         \"pipeline\": oa_doi_is_string_pipeline,\n         \"as\": \"matches\"\n      }\n   }\n\nres = list(db.papers_test.aggregate( [\n    s2orc_doi_is_string,\n    concise_s2orc_to_oa_doi_lookup,\n    only_keep_existing_matches\n] )) \n\ntime6 = time.time() - start_time\nprint(\"Process finished --- %s seconds ---\" % time6)\nprint(f\"# hit: {len(res)}\")\n\nProcess finished --- 0.06404876708984375 seconds ---\n# hit: 0\n\n\nFor some reason, it didn’t help that much. Perhaps because of the change in variable done by let? In general, we will prefer the traditional method. The concise method is useful when you need more complex query (see documentation)\n\n\n\nFiltering by concepts\nHow does filtering by concept improve our query? Lets start by creating an indexes, as we know this is just better (for now):\n\ndb.works_oa_test.create_index([(\"concepts.display_name\", ASCENDING)])\n\n'concepts.display_name_1'\n\n\n\nstart_time = time.time()\n\noa_filter = { '$match': { 'concepts.display_name': 'Biology',  'doi': { \"$type\" : \"string\"} } }\n\nres = list(db.works_oa_test.aggregate( [\n    oa_filter,\n    oa_to_s2orc_doi_lookup,\n    only_keep_existing_matches\n] ))\n\nprint(\"Process finished --- %s seconds ---\" % (time.time() - start_time))\nprint(f\"# hit: {len(res)}\")\n\nProcess finished --- 3.497642993927002 seconds ---\n# hit: 0\n\n\nThis is much better, obviously! What if we only look at concepts.display_name at level=0, given that Biology happens only at that level.\n\ndb.works_oa_test.drop_index([(\"concepts.display_name\", ASCENDING)])\ndb.works_oa_test.create_index([(\"concepts.display_name\", ASCENDING)], partialFilterExpression = { \"concepts.level\" : 0})\n\n'concepts.display_name_1'\n\n\n\nstart_time = time.time()\n\nres = list(db.works_oa_test.aggregate( [\n    oa_filter,\n    oa_to_s2orc_doi_lookup,\n    only_keep_existing_matches\n] ))\n\ntime7 = time.time() - start_time\nprint(\"Process finished --- %s seconds ---\" % time7)\nprint(f\"# hit: {len(res)}\")\n\nProcess finished --- 4.796639442443848 seconds ---\n# hit: 0\n\n\nFor some reason, it didn’t really help. We’ll go back to the original index:\n\ndb.works_oa_test.drop_index([(\"concepts.display_name\", ASCENDING)])\ndb.works_oa_test.create_index([(\"concepts.display_name\", ASCENDING)])\n\n'concepts.display_name_1'\n\n\nNow, doing the same for s2fieldsofstudy from papers_test collection:\n\ndb.papers_test.create_index([(\"s2fieldsofstudy.category\", ASCENDING)])\n\n's2fieldsofstudy.category_1'\n\n\n\nstart_time = time.time()\n\ns2orc_filter = { '$match': { 's2fieldsofstudy.category': 'Biology', 'doi': { \"$type\" : \"string\"}  } }\n\nres = list(db.papers_test.aggregate( [\n    s2orc_filter,\n    s2orc_to_oa_doi_lookup,\n    only_keep_existing_matches\n] ))\n\ntime8 = time.time() - start_time\nprint(\"Process finished --- %s seconds ---\" % time8)\nprint(f\"# hit: {len(res)}\")\n\nProcess finished --- 0.17814087867736816 seconds ---\n# hit: 0\n\n\nOk, now that we have indexes dois and fields on both collections, do we save time by using concise method?\n\nstart_time = time.time()\n\noa_filter_pipeline = [ {\n    \"$match\": { \n        \"concepts.display_name\": \"Biology\",  \n        \"$expr\":  {\n            \"$eq\": [ \"$$s2orc_doi\", \"$doi\" ]\n            }\n        }\n    } ]\n\nconcise_s2orc_to_oa_doi_lookup_concise = {\n      \"$lookup\": {\n         \"from\": \"works_oa_test\",\n         \"localField\": \"doi\",\n         \"foreignField\": \"doi\",\n         \"let\": { \"s2orc_doi\": \"$doi\" },\n         \"pipeline\": oa_filter_pipeline,\n         \"as\": \"matches\"\n      }\n   }\n\nres = list(db.papers_test.aggregate( [\n   s2orc_filter,\n   concise_s2orc_to_oa_doi_lookup_concise,\n   only_keep_existing_matches\n] ))\n\ntime9 = time.time() - start_time\nprint(\"Process finished --- %s seconds ---\" % time9)\nprint(f\"# hit: {len(res)}\")\n\nProcess finished --- 0.17608332633972168 seconds ---\n# hit: 0\n\n\nIt didn’t help that much, but didn’t make it worst either.\n\n\nCompound indexes\nWhat if we add a compound indexes instead of having independent ones?\n\nprint(\"year/concept/doi\")\nreset_indexes()\ndb.papers_test.create_index([(\"doi\", ASCENDING)])\ndb.works_oa_test.create_index([(\"publication_year\", ASCENDING), (\"concepts.display_name\", ASCENDING), (\"doi\", ASCENDING)])\n\ntime_query()\ntime_lookup()\ntime_lookup(reverse_order=True)\n\n\nprint(\"concept/year/doi\")\nreset_indexes()\ndb.papers_test.create_index([(\"doi\", ASCENDING)])\ndb.works_oa_test.create_index([(\"concepts.display_name\", ASCENDING), (\"publication_year\", ASCENDING), (\"doi\", ASCENDING)])\n\ntime_query()\ntime_lookup()\ntime_lookup(reverse_order=True)\n\n\n# THIS TOOK FOREVER\n# print(\"concept/year, doi\")\n# reset_indexes()\n\n# db.works_oa_test.create_index([(\"concepts.display_name\", ASCENDING), (\"publication_year\", ASCENDING)])\n# db.works_oa_test.create_index([(\"doi\", ASCENDING)])\n\n# time_query()\n# time_lookup()\n# time_lookup(reverse_order=True)\n\nprint(\"concept, year, doi\")\nreset_indexes()\ndb.papers_test.create_index([(\"doi\", ASCENDING)])\ndb.works_oa_test.create_index([(\"publication_year\", ASCENDING)])\ndb.works_oa_test.create_index([(\"concepts.display_name\", ASCENDING)])\ndb.works_oa_test.create_index([(\"doi\", ASCENDING)])\n\ntime_query()\ntime_lookup()\ntime_lookup(reverse_order=True)\n\nyear/concept/doi\nBio then year --  finished --- 30.472084283828735 seconds ---\nYear then bio -- finished --- 31.83502769470215 seconds ---\nBio then Year -- finished --- 1.7478671073913574 seconds ---\nYear then Bio -- finished --- 1.681901454925537 seconds ---\nconcept/year/doi\nBio then year --  finished --- 37.71138882637024 seconds ---\nYear then bio -- finished --- 34.56604623794556 seconds ---\nBio then Year -- finished --- 1.807192325592041 seconds ---\nYear then Bio -- finished --- 1.911907434463501 seconds ---\nconcept, year, doi\nBio then year --  finished --- 32.86414432525635 seconds ---\nYear then bio -- finished --- 32.176031827926636 seconds ---\nBio then Year -- finished --- 1.8244223594665527 seconds ---\nYear then Bio -- finished --- 1.8679747581481934 seconds ---\n\n\nAbout the same. Maybe this is because these are small collections. We’ll go back to independent index for the moment, as it is more flexible:\n\ndb.works_oa_test.create_index([(\"publication_year\", ASCENDING)])\ndb.works_oa_test.create_index([(\"concepts.display_name\", ASCENDING)])\ndb.works_oa_test.create_index([(\"doi\", ASCENDING)])\n\ndb.papers_test.create_index([(\"year\", ASCENDING)])\ndb.papers_test.create_index([(\"s2fieldsofstudy.category\", ASCENDING)])\ndb.papers_test.create_index([(\"doi\", ASCENDING)])\n\n'doi_1'\n\n\n\n\nBeyond DOIs\nOk, remember that we have ids other than DOIs:\n\ndb.papers_test.find_one({ \"doi\":  None, \"externalids.MAG\": { \"$ne\": None } })\n\n{'_id': ObjectId('63e50688a64b9c3ca0a79ffd'),\n 'externalids': {'MAG': '580729734'},\n 's2fieldsofstudy': None,\n 'doi': None}\n\n\nIs there papers with mag ids but no doi?\n\nres = list(db.works_oa_test.find({ \"doi\":  None, \"ids.mag\": { \"$ne\": None } }, {\"ids\": 1, \"doi\": 1}))\nprint(f\"# papers with mag id but no doi : {len(res)}\")\nres[0]\n\n# papers with mag id but no doi : 391224\n\n\n{'_id': ObjectId('6420450f8238317efbef948c'),\n 'doi': None,\n 'ids': {'openalex': 'https://openalex.org/W2297889166', 'mag': 2297889166}}\n\n\nWe have enough hits that we should take that into account. But waiit a minute, we also note that both collections don’t have the same types for their mag field. We’ll fix that by converting works_oa_test field to become string (and create indexes at the same time):\n\ndb.works_oa_test.update_many(\n    {},\n    [\n        { \"$addFields\": { \"mag\": { \"$toString\": \"$ids.mag\" } } }\n    ]\n)\n\ndb.works_oa_test.create_index([(\"mag\", ASCENDING)])\ndb.papers_test.create_index([(\"externalids.MAG\", ASCENDING)])\n\n'externalids.MAG_1'\n\n\nWhat we really care about is getting papers that have mag but no doi:\n\nstart_time = time.time()\n\ns2orc_paper_filter = {\n            \"$match\": { \n               \"doi\": { \"$type\": \"null\" }, \n               \"$expr\":  { \n                    \"$eq\": [ \"$$works_oa_mag\", \"$externalids.MAG\" ] \n                }\n            }\n         }\n\nworks_oa2paper_lookup_concise = {\n      \"$lookup\": {\n         \"from\": \"papers_test\",\n         \"localField\": \"mag\",\n         \"foreignField\": \"externalids.MAG\",\n         \"let\": { \"works_oa_mag\": \"$mag\" },\n         \"pipeline\": [ s2orc_paper_filter ],\n         \"as\": \"matches\"\n      }\n   }\n\noa_bio_filter_mag_is_string = {\"$match\": {\"concepts.display_name\": \"Biology\", \"mag\": { \"$type\": \"string\" }}}\n\nres = list(db.works_oa_test.aggregate( [\n    oa_bio_filter_mag_is_string,\n    works_oa2paper_lookup_concise,\n    only_keep_existing_matches\n] ))\n\ntime11 = time.time() - start_time\nprint(\"Process finished --- %s seconds ---\" % time11)\nprint(f\"# hit: {len(res)}\")\n\nProcess finished --- 82.6011688709259 seconds ---\n# hit: 137971\n\n\nBecause this is the 1960s, note that we have many papers with MAG but no DOIs. This takes longer. Looking only from the s2orc side, we know it should be better:\n\nstart_time = time.time()\n\nworks_s2orc2oa_lookup = {\n      \"$lookup\": {\n         \"from\": \"works_oa_test\",\n         \"localField\": \"externalids.MAG\",\n         \"foreignField\": \"mag\",\n         \"as\": \"matches\"\n      }\n   }\n\ns2orc_bio_filter_mag_is_string = { \"$match\": {\"s2fieldsofstudy.category\": \"Biology\", \"externalids.MAG\": { \"$type\": \"string\" }, \"doi\": { \"$type\": \"null\" } } }\n\nres = list(db.papers_test.aggregate( [\n    s2orc_bio_filter_mag_is_string,\n    works_s2orc2oa_lookup,\n    only_keep_existing_matches\n] ))\n\ntime12 = time.time() - start_time\nprint(\"Process finished --- %s seconds ---\" % time12)\nprint(f\"# hit: {len(res)}\")\n\nProcess finished --- 53.68574142456055 seconds ---\n# hit: 90425\n\n\nAgain, using the concise metho and combining the filters:\n\nstart_time = time.time()\n\ns2orc_filter = { \"$match\": {\n   \"s2fieldsofstudy.category\": \"Biology\", \n   \"externalids.MAG\": { \"$type\": \"string\" }, \n   \"doi\": { \"$type\": \"null\" } } \n}\n\n\noa_paper_filter = {\n            \"$match\": { \n               \"doi\": { \"$type\": \"null\" }, \n               \"mag\": { \"$type\": \"string\"},\n               \"$expr\":  { \n                    \"$eq\": [ \"$$s2orc_mag\", \"$mag\" ] \n                }\n            }\n         }\n\nworks_s2orc2oa_lookup_concise = {\n      \"$lookup\": {\n         \"from\": \"works_oa_test\",\n         \"localField\": \"externalids.MAG\",\n         \"foreignField\": \"mag\",\n         \"let\": { \"s2orc_mag\": \"$externalids.MAG\" },\n         \"pipeline\": [ oa_paper_filter ],\n         \"as\": \"matches\"\n      }\n   }\n\nres = list(db.papers_test.aggregate( [\n    s2orc_filter,\n    works_s2orc2oa_lookup_concise,\n    only_keep_existing_matches\n] ))\n\ntime13 = time.time() - start_time\nprint(\"Process finished --- %s seconds ---\" % time13)\nprint(f\"# hit: {len(res)}\")\n\nProcess finished --- 30.958298444747925 seconds ---\n# hit: 43415\n\n\nThey provide similar time, but we’ll need to stick with the concise method when we work with the full dataset because we’ll filter on year too in oa_paper_filter (which ought to make a big difference).\n\n\n\n\n\n\nTip\n\n\n\nIt is useful to test filters individually on both collections. You want to make sure that something did not went wrong and you actually are giving empty collections to your lookup operation. Also, always check your types. Here externalids.MAG and ids.mag were of different types, I had to add a field to works_oa that converted ids.mag as string.\n\n\n\n\nAugmenting S2ORC with openAlex metadata\nWe’ll augment S2ORC with OA using DOIs and MAGs in two steps. First using DOIs, without consideration for mag:\n\nstart_time = time.time()\n\ns2orc_filter = { \"$match\": {\n   \"s2fieldsofstudy.category\": \"Biology\", \n   \"doi\": { \"$type\": \"string\" } \n   }\n}\n\noa_filter_pipeline = [ {\n    \"$match\": { \n        \"concepts.display_name\": \"Biology\",\n        \"doi\": { \"$type\": \"string\" },\n        \"$expr\":  {\n            \"$eq\": [ \"$$s2orc_doi\", \"$doi\" ]\n            }\n        }\n    } ]\n\nconcise_s2orc_to_oa_doi_lookup_concise = {\n      \"$lookup\": {\n         \"from\": \"works_oa_test\",\n         \"localField\": \"doi\",\n         \"foreignField\": \"doi\",\n         \"let\": { \"s2orc_doi\": \"$doi\" },\n         \"pipeline\": oa_filter_pipeline,\n         \"as\": \"matches\"\n      }\n   }\n\nres = list(db.papers_test.aggregate( [\n   s2orc_filter,\n   concise_s2orc_to_oa_doi_lookup_concise,\n   only_keep_existing_matches,\n   { '$addFields': { 'works_oa': {'$cond': [{'$ne': ['$matches', []]}, \"$matches\", None]} } },\n   { '$project': { 'matches': 0 } }\n] ))\n\nprint(\"Aggregation pipeline finished --- %s seconds ---\" % (time.time() - start_time))\n\ndb.papers_test.bulk_write([\n    UpdateOne( { 'doi': doc['doi'] }, { '$set': {'works_oa': doc['works_oa'][0]} })\n    for doc in res\n])\n\nprint(\"Process finished --- %s seconds ---\" % (time.time() - start_time))\n\nThen, we augment paper with MAG but no DOIs. This is the “big” operation:\n\nstart_time = time.time()\n\ns2orc_filter = { \"$match\": {\n   \"s2fieldsofstudy.category\": \"Biology\", \n   \"externalids.MAG\": { \"$type\": \"string\" }, \n   \"doi\": { \"$type\": \"null\" } } \n}\n\noa_paper_filter = { \"$match\": { \n   \"doi\": { \"$type\": \"null\" }, \n   \"mag\": { \"$type\": \"string\"},\n   \"$expr\":  { \n      \"$eq\": [ \"$$s2orc_mag\", \"$mag\" ] \n      }\n   }\n}\n\nconcise_s2orc_to_oa_mag_lookup = {\n      \"$lookup\": {\n         \"from\": \"works_oa_test\",\n         \"localField\": \"externalids.MAG\",\n         \"foreignField\": \"mag\",\n         \"let\": { \"s2orc_mag\": \"$externalids.MAG\" },\n         \"pipeline\": [ oa_paper_filter ],\n         \"as\": \"matches\"\n      }\n   }\n\nres = list(db.papers_test.aggregate( [\n   s2orc_filter,\n   concise_s2orc_to_oa_mag_lookup,\n   only_keep_existing_matches,\n   { '$addFields': { 'works_oa': {'$cond': [{'$ne': ['$matches', []]}, \"$matches\", None]} } },\n   { '$project': { 'matches': 0 } }\n] ))\n\nprint(\"Aggregation pipeline finished --- %s seconds ---\" % (time.time() - start_time))\nprint(f\"# hit: {len(res)}\")\n\ndb.papers_test.bulk_write([ \n    UpdateOne( {'externalids.MAG': doc['externalids'][\"MAG\"]}, {'$set': {'works_oa': doc['works_oa'][0]}} ) \n    for doc in res\n    ])\n\nprint(\"Process finished --- %s seconds ---\" % (time.time() - start_time))\n\nExamining the modified collection:\n\ndb.papers_test.find_one({'externalids.MAG':  res[0]['externalids']['MAG']})\n\n{'_id': ObjectId('63e50688a64b9c3ca0a7a375'),\n 'externalids': {'MAG': '2462552499'},\n 's2fieldsofstudy': [{'category': 'Biology', 'source': 's2-fos-model'},\n  {'category': 'Chemistry', 'source': 'external'},\n  {'category': 'Medicine', 'source': 'external'}],\n 'doi': None}\n\n\nThe next step is to apply what we learn on the full database. As we modify our main databaset, we’ll do it in a script that lives in our directory just for that.\n\nprint(\"hey\")\n# out = list(db.papers.aggregate([\n#    { \"$match\": { \n#       \"year\": 1960, \n#       \"s2fieldsofstudy.category\": \"Biology\",\n#       \"works_oa\": {\"$exists\": \"true\"},\n#       }\n#    },\n#    { '$project': { \n#       'main_concept': { \n#          \"$filter\": { \n#             \"input\": \"$works_oa.concepts\", \n#             \"as\": \"c\", \n#             \"cond\": { \"$eq\": [\"$$c.level\", 1 ]}\n#             }\n#          }\n#       }\n#    }\n   # { \"$unwind\": \"$main_concept\" }, \n   # {\"$group\": {\n   #    \"_id\": {\n   #       \"year\": \"$year\",\n   #       \"concept\": \"$main_concept.display_name\"\n   #    }, \n   #    \"n_papers\": {\"$sum\": 1}\n   #    }\n   # }\n   # ]))\n\n# pd.concat([\n#    pd.DataFrame([_['_id'] for _ in out]),\n#    pd.DataFrame({'n_papers': [_['n_papers'] for _ in out]})\n# ], axis=1)\n\nhey\n\n\n\n\nSummary table.\n\nall_time = [time3,time4,time5,time6,time7,time8,time9,\n            time11,time12,time13]\ndisplay(pd.DataFrame({\"time (sec)\": all_time}).to_html())\n\n\n\n\n\n\n\n\n\n\n\n\ntime (sec)\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\n0.045781\n\n\n\n\n\n\n\n1\n\n\n\n23.030836\n\n\n\n\n\n\n\n2\n\n\n\n0.039495\n\n\n\n\n\n\n\n3\n\n\n\n0.064049\n\n\n\n\n\n\n\n4\n\n\n\n4.796639\n\n\n\n\n\n\n\n5\n\n\n\n0.178141\n\n\n\n\n\n\n\n6\n\n\n\n0.176083\n\n\n\n\n\n\n\n7\n\n\n\n82.601169\n\n\n\n\n\n\n\n8\n\n\n\n53.685741\n\n\n\n\n\n\n\n9\n\n\n\n30.958298\n\n\n\n\n\n\n\n\n\n\n\n\nHow many PubMed without DOI or MAG ids?\n\nprint(\"hey\")\n# pubmed = list(db.papers.aggregate([\n#    {\n#       \"$match\": {\n#          \"doi\": None, \"externalids.MAG\": None, \"externalids.PubMed\": {\"$ne\": None}\n#       }\n#    },\n#    {\n#       \"$group\": {\n#          \"_id\": {\n#             \"year\": \"$year\"\n#          },\n#          \"n_papers\": {\"$sum\": 1} \n#       }\n#    },\n#    ]))\n\n# df_pubmed = pd.concat([\n#    pd.DataFrame([{'year': _['_id']['year']} for _ in pubmed]),\n#    pd.DataFrame([{'n': int(_['n_papers'])} for _ in pubmed])\n# ], axis=1)\n\n# df_pubmed = df_pubmed[~df_pubmed.year.isna()]\n# df_pubmed.year = pd.to_datetime(df_pubmed.year, format=\"%Y\")\n# df_pubmed.set_index(\"year\").plot()\n\n# acl = list(db.papers.aggregate([\n#    {\n#       \"$match\": {\n#          \"doi\": None, \"externalids.MAG\": None, \"externalids.PubMed\": None, \"externalids.ACL\": {\"$ne\": None}\n#       }\n#    },\n#    {\n#       \"$group\": {\n#          \"_id\": {\n#             \"year\": \"$year\"\n#          },\n#          \"n_papers\": {\"$sum\": 1} \n#       }\n#    },\n#    ]))\n\n# df_acl = pd.concat([\n#    pd.DataFrame([{'year': _['_id']['year']} for _ in acl]),\n#    pd.DataFrame([{'n': int(_['n_papers'])} for _ in acl])\n# ], axis=1)\n\n# df_acl = df_acl[~df_acl.year.isna()]\n# df_acl.year = pd.to_datetime(df_acl.year, format=\"%Y\")\n# df_acl.set_index(\"year\").plot()\n\nhey"
  },
  {
    "objectID": "posts/prog_and_union/index.html",
    "href": "posts/prog_and_union/index.html",
    "title": "Computer programming is not that different from unionization",
    "section": "",
    "text": "Labor unions are associations of workers who have demands aimed at improving their quality of life. From an organization point of view, the key challenge of unions is to organize themselves and recruit new members to be more effective when bargaining with the employers.\nUnions coevolve with union busting, similar to prey-predators dynamics. As a result, there is often an individual cost-benefit ratio for joining a union, which is determined by the union busting effort, the institutional capacities of groups to promote union behavior and the perceived importance of the cause by involved individuals.\nA key institutional capacity to advance union behavior is the ability to promote a sense of belonging by demonstrating the value of the cause to potentially community members.\nUnions also evolve as a form of cultural group selection, where successful tactics and strategies are copied among union groups. If a group is able to grow and reproduce, it is more able to persist over time. As with traditional cultural group selection, we might think about how competition among groups promote cooperation insofar as there is more variation within groups than across groups [to check that claim and cite relevant refs].\nIn summary, unions are a process with a cost-benefit ratio in which groups with more fitness will perform better.\nHow is all of this related to computer programming? Isn’t programming very personal:\nFor many scientists without computational or geek background, the perception about programming is that of the “lone hacker”. A lonely activity that engage you and your computer. Someone somewhere wrote something we call a programming language that you can download and use to run scripts. These scripts usually do something that scientists are interested in, like calculating the functional diversity index for ecologists or extracting word frequency from a relevant corpus for literary reviews.\nBut for other scientists who grew alongside the world of computer programming, they know that computer programming looks like the following:\nBehind the digital veil, there is often a community (ok, very often this is that one guy who did all the work, but even in this case communities can emerge from popular software) who wrote the code that you use. From the bottom of your hardware to the niche library that map onto scientific concepts, in passing by the scientific computing routines that most people use in whatever programming languages. In the Free and Open-Source Software world (F/OSS), this community is dedicated to write code that is free to use and modify. Programming in science is part of the computing world; it never is an individual activity even if it feels like it.\nIt is just that we don’t always see it, physically.\nWith this entry, I hope to convince you of the following:\nOk, wait. Am I saying that programming is always already a social activity? I hear many people responding that this is not their experience. Even if it is true that scientific software is written by someone else, does that mean the actual activity of coding has to be social too?\nWell, yes and no. This reminds me of the distinction between individual and social learning in the theory of cultural evolution. With our WEIRD brains, we tend to assume many skills are individually learned. For example, running is something you learned on your own. But in many other societies, running is a skill that is socially learn [REF], and arguably everytime they run they embody that knowledge. In our WEIRD societies, people don’t need to run in particular ways to survive. But invididuals who run as a serious hobby or professionally have learned at some point how to run. Thus, in particular modern niche, even running embody a social activity, even if you run by yourself. On top of this embodied perspective, running as a shared experience brings about collective gathering, subreddits, prestige and so on.\nI am claiming that something similar than running happen with respect to computer programming. As scientists, you can open your laptop, download R or Python, and learn to code individually. But most likely your code will be unintelligible to others and even to yourself in the near future. Learning to code is about copying others, building a mental model, and eventually tinkering with the code so that it feels natural.\nNote that you can be an outlooker of your programming community, without being actively involved in the social realm. But sooner or later, there are features or bugs that make sense only in the context of seeing programming as a group-based process, similar to that of an union.\nAs the social realm underlying programming is unveiled, the process of learning to program becomes somehow similar to that of that joining an union.\nA key similarity is that in both case, the process of becoming part of a community changes the new member worldview. As someone becomes convince that unionization is valuable, it is willing to pay stronger cost to defend the idea. As someone becomes a programmer that is actively involved in the open-source world, it changes her perspective on the unfolding history of computer programming and the internet. In both cases, new members learn a new language that reinforce the sense of community. This is not static though. As institutions get bigger, they can both become degenerate or corporate, which is another beast than earlier institutional phase. In both cases, as institutions get bigger, norms and goodwills are replaced by code of conducts and protocols. If the institutions is successful, people might join for other reasons than further the advancement of the cause. Power might become in the hand of a few privileged individuals at the top, administration and bureaucraties might add layers of complexities that make no sense, etc.."
  },
  {
    "objectID": "posts/prog_and_union/index.html#appendix-foss-success-stories-are-as-technical-as-they-are-communal",
    "href": "posts/prog_and_union/index.html#appendix-foss-success-stories-are-as-technical-as-they-are-communal",
    "title": "Computer programming is not that different from unionization",
    "section": "Appendix: F/OSS success stories are as technical as they are communal",
    "text": "Appendix: F/OSS success stories are as technical as they are communal\n\nLinux is making Apple Great Again\nHello Dolly: Democratizing the magic of ChatGPT with open models\nAlpaca: A Strong, Replicable Instruction-Following Model\nA one-year long research workshop on large multilingual models and datasets\nabout openalex"
  },
  {
    "objectID": "posts/rise_of_comp3/index.html",
    "href": "posts/rise_of_comp3/index.html",
    "title": "The rise of computational works 3",
    "section": "",
    "text": "import {Plot} from \"@mkfreeman/plot-tooltip\""
  },
  {
    "objectID": "posts/rise_of_comp3/index.html#rise-of-programming",
    "href": "posts/rise_of_comp3/index.html#rise-of-programming",
    "title": "The rise of computational works 3",
    "section": "Rise of programming?",
    "text": "Rise of programming?\n\n\n\nviewof select = Inputs.select(fields, { multiple: true, value: ['linguistics', 'philosophy', 'history'] })\nviewof show_pct = Inputs.toggle({label: \"normalize\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata_f = which(rin(data.map(d =&gt; d.field), select)).map(i =&gt; data[i]).filter(d =&gt; d.year &gt; 1970)\n\nline_plot = () =&gt; {\n  \n  const xy_norm = {\n          x: \"year\",  y:  'pct_comp', stroke: \"field\", title: \"field\"\n      }\n    \n  const xy = {\n          x: \"year\",  y:  'n_comp', stroke: \"field\", title: \"field\"\n      }\n\n  return Plot.plot({\n      marginLeft: 50,\n      height: 600,\n      width: 800,\n      marginBottom: 50,\n      x: { label: null, tickRotate: 45 },\n      y: { \n        percent: show_pct ? true : false, \n        grid: true, \n        label: show_pct ? \"↑ %\" : \"↑ n papers\" },\n      marks: [\n        Plot.line(data_f, show_pct ? xy_norm : xy)\n      ]\n  })\n}\n\nlp = line_plot()\n\nlp.legend(\"color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata = FileAttachment(\"comp_normalized.csv\").csv({ typed: true })\n\nfields = ['mathematics', 'geology', 'environmental science', 'art','history', 'linguistics', 'psychology',  'education', 'geography', 'physics', 'law', 'sociology', 'economics', 'political science', 'philosophy']\n\nfunction rin(arr1, arr2) {\n  return Array.from(arr1, (x) =&gt; {\n    return arr2.indexOf(x) == -1 ? false : true\n  })\n}\n\nfunction which(x) {\n  return x.reduce(\n      (out, bool, index) =&gt; bool ? out.concat(index) : out, \n      []\n    )\n}"
  },
  {
    "objectID": "posts/interactive-stats/index.html",
    "href": "posts/interactive-stats/index.html",
    "title": "CDAE stats",
    "section": "",
    "text": "The equation:\n\\[n^* = \\frac{n_0}{1 + \\frac{n_0}{N}}\\]\nwhere \\(n_0 = z_\\alpha^2 \\frac{S^2}{D^2}\\), \\(S^2\\) is our population variance, \\(D^2\\) is the difference between the true value and the estimated value, and \\(z_\\alpha^2\\) is the \\(z\\) value at a given confidence interval.1\n\n\n\nWe know…\n\nThere are 5,000 nonprofits in the city of reference\nFrom a previous study, we know that the the mean value of using new tools is $3,000. We also know from previous studies that the s.d. of this is $3,500.\n\nWe want…\n\nAn error rate of 10%\nA confidence interval of 95%\n\n\n\nfunction calc_n_0(z_alpha_sq, S, D) {\n    return z_alpha_sq * (S**2 / D**2)\n}\n\nfunction effective_sample_size(z_alpha_sq, S, D, N) {\n    const n_0 = calc_n_0(z_alpha_sq, S, D)\n    return +(n_0 / (1 + (n_0 / N))).toFixed(1)\n}\n\nfunction ci2z(ci) {\n     if (ci === \".68\") {\n        return 1\n     } else if (ci === \".95\") {\n        return 2\n     } else if (ci === \".99\") {\n        return 3\n     }\n}\n\nviewof conf_int = Inputs.radio([\".68\", \".95\", \".99\"], {value: \".95\", label: \"Conf. interval\"})\nviewof error_rate = Inputs.range([0.05, 1], {value: 0.1, step: 0.05, label: \"Error rate\"})\nviewof N = Inputs.range([0, 100000], {value: 5000, step: 1000, label: \"N\"})\nviewof prev_mean = Inputs.range([0, 10000], {value: 3000, step: 500, label: \"Prev mean\"})\nviewof prev_std = Inputs.range([0, 10000], {value: 3500, step: 500, label: \"Prev std\"})\nz_alpha_sq = ci2z(conf_int)**2\n\nS = prev_std\nD = error_rate * prev_mean\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, \\(S^2\\) is the previous standard deviation squared and \\(D\\) is the wanted error rate times the previous mean, that is,  x  = .\nWe find that the minimum adequate sample size, or \\(n^*\\):\n\\(n^0\\) = \n\\(n^*\\) =  / (1 +  / ) = \n\\(n_0/N\\) = \nAlso, we saw in class that \\(n^*\\) converges around \\(600\\), with the default parameters. That is, adding more data does not entail a higher \\(n^*\\). You can observe that fact with the following plot:\n\nxs = [...Array(N).keys()];\nys = xs.map(x =&gt; effective_sample_size(z_alpha_sq, S, D, x))\nPlot.lineY(ys).plot({height: 200, width: 300, y: {label: \"↑ n*\"}, x: {label: \"N →\"}})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut you can play around with other settings to see how it varies."
  },
  {
    "objectID": "posts/interactive-stats/index.html#calculate-minimum-adequate-sample-size",
    "href": "posts/interactive-stats/index.html#calculate-minimum-adequate-sample-size",
    "title": "CDAE stats",
    "section": "",
    "text": "The equation:\n\\[n^* = \\frac{n_0}{1 + \\frac{n_0}{N}}\\]\nwhere \\(n_0 = z_\\alpha^2 \\frac{S^2}{D^2}\\), \\(S^2\\) is our population variance, \\(D^2\\) is the difference between the true value and the estimated value, and \\(z_\\alpha^2\\) is the \\(z\\) value at a given confidence interval.1\n\n\n\nWe know…\n\nThere are 5,000 nonprofits in the city of reference\nFrom a previous study, we know that the the mean value of using new tools is $3,000. We also know from previous studies that the s.d. of this is $3,500.\n\nWe want…\n\nAn error rate of 10%\nA confidence interval of 95%\n\n\n\nfunction calc_n_0(z_alpha_sq, S, D) {\n    return z_alpha_sq * (S**2 / D**2)\n}\n\nfunction effective_sample_size(z_alpha_sq, S, D, N) {\n    const n_0 = calc_n_0(z_alpha_sq, S, D)\n    return +(n_0 / (1 + (n_0 / N))).toFixed(1)\n}\n\nfunction ci2z(ci) {\n     if (ci === \".68\") {\n        return 1\n     } else if (ci === \".95\") {\n        return 2\n     } else if (ci === \".99\") {\n        return 3\n     }\n}\n\nviewof conf_int = Inputs.radio([\".68\", \".95\", \".99\"], {value: \".95\", label: \"Conf. interval\"})\nviewof error_rate = Inputs.range([0.05, 1], {value: 0.1, step: 0.05, label: \"Error rate\"})\nviewof N = Inputs.range([0, 100000], {value: 5000, step: 1000, label: \"N\"})\nviewof prev_mean = Inputs.range([0, 10000], {value: 3000, step: 500, label: \"Prev mean\"})\nviewof prev_std = Inputs.range([0, 10000], {value: 3500, step: 500, label: \"Prev std\"})\nz_alpha_sq = ci2z(conf_int)**2\n\nS = prev_std\nD = error_rate * prev_mean\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, \\(S^2\\) is the previous standard deviation squared and \\(D\\) is the wanted error rate times the previous mean, that is,  x  = .\nWe find that the minimum adequate sample size, or \\(n^*\\):\n\\(n^0\\) = \n\\(n^*\\) =  / (1 +  / ) = \n\\(n_0/N\\) = \nAlso, we saw in class that \\(n^*\\) converges around \\(600\\), with the default parameters. That is, adding more data does not entail a higher \\(n^*\\). You can observe that fact with the following plot:\n\nxs = [...Array(N).keys()];\nys = xs.map(x =&gt; effective_sample_size(z_alpha_sq, S, D, x))\nPlot.lineY(ys).plot({height: 200, width: 300, y: {label: \"↑ n*\"}, x: {label: \"N →\"}})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut you can play around with other settings to see how it varies."
  },
  {
    "objectID": "posts/interactive-stats/index.html#power-analysis",
    "href": "posts/interactive-stats/index.html#power-analysis",
    "title": "CDAE stats",
    "section": "Power analysis",
    "text": "Power analysis\nSee Patrick Mineault notebook"
  },
  {
    "objectID": "posts/interactive-stats/index.html#the-many-lives-of-statistical-tests",
    "href": "posts/interactive-stats/index.html#the-many-lives-of-statistical-tests",
    "title": "CDAE stats",
    "section": "The many lives of statistical tests",
    "text": "The many lives of statistical tests\nSometimes I feel that the popularity of statistical testing is about outsourcing statistical work of busy scientists to flow charts. In research methods courses focusing on statistical testing I feel there is an understanding that these are limited, but given time and interest of students, it’s better than nothing. And if you’re sticking to experimental setups, that might be all you need. I am not going to do a rant. But I want to supplement the usual search method class with alternative perspectives explained as simply as possible:\n\nThe Frequentist approach. This if often the first encounter with inferential statistics in social socience. As long as you are in an experimental set-up this might be fine. You need to think about probability as long-run probability.\nThe linear models approach. Instead of starting from statistical tests, we start from linear models and explain which models map onto which tests. This approach promotes flexibility at the costs of having to learn the underlying ideas of linear models.\nHypothesis testing but Bayesian. No need to remember the nonsense that we “fail to reject the null” and that 0.95 confidence interval does not mean that we are “95% confident that our results are significant”.\nThe Bootstrap approach. This is a great coding exercice and saves you time from remembering all the different tests.\n\nNote that we use the following emojis to encode data types:\n\n💡 : Yes/no, 2 levels, success/failure, bias/fair… nominal data.\n📊 : Yes/no/maybe, &gt;2 levels, might be ordinal or nominal.\n📏 : continuous/scalar/uncountable data.\n\n\n💡 ~ 💡📊 ~ 💡📏 ~ 💡\n\n\n\nNHST\n\n\nR code\na &lt;- chisq.test(d_mat) # p-value &gt; 0.05\n\n\n\n\nLinear models\n\n\nR code\n# Using glm to do a log-linear model\nfull = glm(n ~ early_first_line * sex, family = poisson(), data = d_long) \nb = anova(full, test = 'Rao') #  similar to our two-way ANOVA\n\n\n\n\nSummary\n\n\n# A tibble: 2 × 2\n  p.value model     \n    &lt;dbl&gt; &lt;chr&gt;     \n1   0.927 chisq.test\n2   0.647 glm       \n\n\n\n\n\n\nNHST\n\n\nR code\na &lt;- chisq.test(d_mat) # p-value &gt; 0.05\n\n\n\n\nLinear models\n\n\nR code\nfull = glm(n ~ self_id_as_coder * sex, family = poisson(), data = d_long) # log-linear model\nb = anova(full, test = 'Rao') #  similar to our two-way ANOVA\n\n\n\n\nSummary\n\n\n# A tibble: 2 × 2\n  p.value model     \n    &lt;dbl&gt; &lt;chr&gt;     \n1  0.0235 chisq.test\n2  0.0235 glm       \n\n\n\n\n\n\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\nWarning: The `origin` argument of `stat_bin()` is deprecated as of ggplot2 2.1.0.\nℹ Please use the `boundary` argument instead.\n\n\n\n\n\n\n\n\n\nNHST\n\n\nLinear models\n\n\nSummary\n\n\n# A tibble: 3 × 5\n    p.value estimate conf.low conf.high model \n      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; \n1 0.163        -17.9    -43.4      7.73 t.test\n2 0.0000119     NA       NA       NA    glm   \n3 0.163         NA       NA       NA    glm"
  },
  {
    "objectID": "posts/interactive-stats/index.html#footnotes",
    "href": "posts/interactive-stats/index.html#footnotes",
    "title": "CDAE stats",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe mapping for the confidence interval and standard deviation comes from the properties of the normal distribution:\n\nwhere, for example, \\(z_\\alpha^2 = z_{.95}^2 = 2^2\\) because \\(34.1\\%*2 + 13.6\\%*2 = |2\\sigma| \\approx 95\\%\\) of the distribution mass.↩︎"
  },
  {
    "objectID": "posts/oa_viz/index.html",
    "href": "posts/oa_viz/index.html",
    "title": "OpenAlex Viz",
    "section": "",
    "text": "viewof select = Inputs.select([\"ratio_works_count_cited\", \"works_count\", \"cited_by_count\"])\nviewof range = Inputs.range([0, 100000], {step: 1000, label: \"min works count\"})\n\nPlot.plot({ \n    height: 1000, \n    marginLeft: 100, \n    marginRight: 500,\n    width: 700, \n    color: { legend: true },\n    marks: [\n        Plot.barX(transpose(data).filter(d =&gt; d.works_count &gt; range), {\n            y: \"issn_l\", \n            x: select, \n            fill: \"country_code\",\n            stroke: \"black\",\n            sort: {\n                y: \"x\", \n                reverse: true, \n                limit: 50}\n        }),\n        Plot.textX(transpose(data).filter(d =&gt; d.works_count &gt; range), {\n            y: \"issn_l\", \n            x: select, \n            text: d =&gt; `${d.display_name} (# works ${d.works_count} )`,\n            dx: 6, \n            textAnchor: \"start\",\n            sort: {\n                y: \"x\", \n                reverse: true, \n                limit: 50}\n        })\n    ]})"
  }
]