[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "shambolics",
    "section": "",
    "text": "Survey results (early release)\n\n\nPreliminary results from a survey on the cost and benefits of learning to code\n\n\n\n\nSurvey\n\n\nProgramming\n\n\n\n\nThe costs and benefits of learning to code in science vary across individuals and disciplines. We should have a better idea of these tradeoffs before selling anyone on coding.\n\n\n\n\n\n\nJan 13, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nThe rise of computational works 2\n\n\n\n\n\n\n\nVisualization\n\n\nNLP\n\n\nS2ORC\n\n\nInteractive\n\n\nSciSci\n\n\n\n\nA new hope.\n\n\n\n\n\n\nJan 13, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nS2ORC Viz\n\n\n\n\n\n\n\nVisualization\n\n\nNLP\n\n\nS2ORC\n\n\nInteractive\n\n\n\n\nI always forget about the s2orc database details. Lets have them here.\n\n\n\n\n\n\nJan 13, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nThe rise of computational works 3\n\n\n\n\n\n\n\nVisualization\n\n\nNLP\n\n\nS2ORC\n\n\nInteractive\n\n\nSciSci\n\n\n\n\nThe computational works strike back\n\n\n\n\n\n\nJan 13, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nThe rise of computational works 1\n\n\n\n\n\n\n\nVisualization\n\n\nNLP\n\n\nS2ORC\n\n\nInteractive\n\n\nSciSci\n\n\n\n\nWhere, how, and when did computational stuff became popular in science?\n\n\n\n\n\n\nJan 13, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nCDAE stats\n\n\n\n\n\n\n\nCDAE\n\n\nInteractive\n\n\n\n\nStatistics is hard. Computational statistics makes it a bit better.\n\n\n\n\n\n\nJan 13, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nALLotaxonometer\n\n\n\n\n\n\n\nAllotaxonometer\n\n\nInteractive\n\n\n\n\nAllotaxonometer was exclusive to matlab. Not anymore.\n\n\n\n\n\n\nJan 13, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/survey-programming/index.html",
    "href": "posts/survey-programming/index.html",
    "title": "Survey results (early release)",
    "section": "",
    "text": "pdata = FileAttachment(\"data_clean.csv\").csv({ typed: true })\n\n// useful vars\ncolnames = d3.sort(Object.keys(pdata[0]))\ncoders = pdata.filter(d => d.is_coder === 'coder')\nnon_coders = pdata.filter(d => d.is_coder === 'non coder')\ncoder_count = tidy(pdata, count(\"is_coder\")).map(d => d.n)\n\n// global filtering\nfiltered_dat = sel_dept == \"\" ? pdata : pdata.filter(d => d[\"dept_students_lab\"] === sel_dept)\nfiltered_coders = sel_dept == \"\" ? coders : coders.filter(d => d[\"dept_students_lab\"] === sel_dept)\nWe currently have  valid responses, from  different departments. There are  coders and  non-coders."
  },
  {
    "objectID": "posts/survey-programming/index.html#profiles",
    "href": "posts/survey-programming/index.html#profiles",
    "title": "Survey results (early release)",
    "section": "Profiles",
    "text": "Profiles\n\nviewof sel_dept = Inputs.select([''].concat(\n  tidy(\n    pdata.filter(d => d[\"dept_students_lab\"] !== null),\n    distinct('dept_students_lab')\n  ).map(d => d[\"dept_students_lab\"])), {label: \"Choose dept\"})\n\nviewof do_pct = Inputs.toggle({label: \"Show %\"}) \nviewof rm_nulls = Inputs.toggle({label: \"Remove nulls\", value: true})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDemographicsDev ProfilesTablesProfile DataQuestions\n\n\n\ncol_dems = [\"is_coder\", \"gender_binary\", \"year_born\", \n             \"ethnicity_binary\",  \"dept_students_binary\", \"academia_status\"]\n\nviewof by_attr = Inputs.radio(col_dems, {label: \"Attributes\"})\n\nhtml`<p class = 'q_styled'>Preferred Pronouns</p>`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_attr_v(filtered_dat, \"pref_pronouns\", by_attr, true,  do_pct)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Academia Status</p>`\n\n\n\n\n\n\n\nplot_attr_v(filtered_dat, \"academia_status\", by_attr, false, do_pct)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Departments</p>`\n\n\n\n\n\n\n\nplot_attr_v(filtered_dat, \"dept_students_lab\", by_attr, false, do_pct)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Year Born</p>`\n\n\n\n\n\n\n\nplot_yr_born(filtered_dat, by_attr)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Duration (sec)</p>`\n\n\n\n\n\n\n\nplot_duration(filtered_dat, by_attr)\n\n\n\n\n\n\n\n\n\n\nOnly showing coders.\n\nviewof by_attr2 = Inputs.radio([\"gender_binary\", \"year_born\", \"ethnicity_binary\", \"dept_students_binary\", \"academia_status\"], {label: \"Attributes\"})\n\n// Do you consider yourself to be a coder/programmer?\n\nplot_attr_v(filtered_coders, \"self_id_as_coder\", by_attr2, true, do_pct)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_attr_v(filtered_coders, \"what_os\", by_attr2, false, do_pct)\n\n\n\n\n\n\n\n\nFirst line code\n\nplot_attr_h(filtered_coders, \"first_line_code\", by_attr2, false, \"first line\")\n\n\n\n\n\n\nYears coding\n\nplot_attr_h(filtered_coders, \"years_coding\", by_attr2, false, \"years coding\")\n\n\n\n\n\n\n\n\n\nviewof form = Inputs.form({\n  extra_filter: Inputs.checkbox(['coder', 'female', 'stem', 'Non-stem'], {label: 'Only'}), \n  dep_var: Inputs.select(colnames, {value: 'self_id_as_coder', label: 'Dep var (DV)'}),\n  indep_var: Inputs.select(colnames, {value: 'gender_binary', label: 'Indep var (IV)'}), \n  do_pct_crosstab: Inputs.radio(['DV (→)', 'IV (↓)'], {label: \"Show %\", value: 'IV (↓)'})\n})\n\n\n\n\n\n\n\nfunction crosstab_filter(x) {\n    switch (x) {\n      case \"\":         return pdata; \n      case 'coder':    return pdata.filter(d => d.is_coder === 'coder'); \n      case 'female':   return pdata.filter(d => d.gender_binary === 'female'); \n      case 'stem':     return pdata.filter(d => d.dept_students_binary === 'STEM'); \n      case 'Non-stem': return pdata.filter(d => d.dept_students_binary === 'Non-STEM'); \n      case 'coder,female': \n        return pdata.filter(d => d.is_coder === 'coder' && d.gender_binary === 'female');\n      \n      case 'coder,stem': \n        return pdata.filter(d => d.is_coder === 'coder' && d.dept_students_binary === 'STEM');\n      \n      case 'female,stem': \n        return pdata.filter(d => d.gender_binary === 'female' && d.dept_students_binary === 'STEM');\n      \n      case 'female,Non-stem': \n        return pdata.filter(d => d.gender_binary === 'female' && d.dept_students_binary === 'Non-STEM');\n      \n      case 'coder,Non-stem': \n        return pdata.filter(d => d.is_coder === 'coder' && d.dept_students_binary === 'Non-STEM');\n      default:\n        return pdata.filter(d => d.is_coder === \"\")\n}}\n\ncrosstab_dat_filter1 = crosstab_filter(form.extra_filter.join(\",\"))\n\ncrosstab_dat_filter2 = {\n  if (rm_nulls) {\n      return crosstab_dat_filter1.filter(d => d[form.dep_var] !== null && d[form.indep_var] !== null) \n  } else {\n      return crosstab_dat_filter1\n  }\n}\n\ncrosstab_dat_long = tidy(\n    crosstab_dat_filter2, \n    select([form.indep_var, form.dep_var]),\n    count([form.indep_var, form.dep_var])\n)\n\ncols_ordered = [form.dep_var].concat(\n  tidy(crosstab_dat_long, distinct(form.indep_var), arrange(form.indep_var)\n  ).map(d => d[form.indep_var])\n)\n\ncrosstab_dat_w_pct = tidy(\n  crosstab_dat_long,\n  groupBy(form.do_pct_crosstab == 'DV (→)' ? form.dep_var : form.indep_var, [\n    mutateWithSummary({total: sum('n')})\n  ]),\n  mutate({ pct: d => (d.n / d.total)*100 }),\n  select(['-n', '-total'])\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThird variable: \nPercentage data\n\nInputs.table(tidy(\n    crosstab_dat_w_pct,\n    pivotWider({ namesFrom: form.indep_var, valuesFrom: 'pct' }),\n    arrange(form.dep_var)\n    ), { columns: cols_ordered })\n\n\n\n\n\n\nRaw data\n\nInputs.table(tidy(\n  crosstab_dat_long, \n  pivotWider({ namesFrom: form.indep_var, valuesFrom: 'n' }),\n  arrange(form.dep_var)\n  ), { columns: cols_ordered }\n)\n\n\n\n\n\n\n\nfunction notes_crosstab(x) {\n  switch (x) {\n        case \"self_id_as_coder ~ gender_binary\":         \n          return \"It seems that female less affirmatively  self-identify as coders, whereas male are less ambiguous about it. When adding `stem`, male respond more affirmatvely to the question, but note that we have still a small sample. Is this something that holds when controlling for experience?\"; \n        default:\n          return \"\"\n  }\n}\n\nnotes = notes_crosstab(`${form.dep_var} ~ ${form.indep_var}`)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes:\n\nThere are  respondents shown in the table.\nDoes this crosstable passes the rule of five? \n\n\n\n\nprofile_cols = ['academia_status', 'nb_advisors', 'dept_students_lab', \"pref_pronouns\", \n                \"year_born\", \"ethnicity_binary\", 'country_origin', 'us_state', 'reason_coding', 'how_did_you_learn_code','first_line_code', 'first_line_code_c','years_coding',  'freq_coding_proj', 'freq_coding_proj_c', 'freq_oss_proj', 'what_os', 'position_industry', 'use_lang']\n\nInputs.table(pdata, { columns: profile_cols })\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n  \n    Question \n    Label \n  \n \n\n  \n    What is your academic status? \n    academia_status \n  \n  \n    Do you agree to the above terms? \n    agree_term \n  \n  \n    Q_RecaptchaScore \n    captcha_score \n  \n  \n    When using other people's code, how do you cite their library/repository? \n    cite_code \n  \n  \n    When using other people's data, do you cite their library/repository? How? \n    cite_data \n  \n  \n    How likely you think that not knowing how to program will have an impact on future professional opportunities? \n    coding_on_future_opportunities \n  \n  \n    If you have any suggestions/comments about the survey, please share them below! \n    comments \n  \n  \n    Now, thinking about the expectations in your field: would you say that the expected level of programming skills in your field is an important factor in your choice to pursue an academic career? \n    comp_skills_factors_pursue_academia \n  \n  \n    How important do you think your programming skills were for your career? \n    comp_skills_pro_benefits_p \n  \n  \n    Do you think programming will bring you professional benefits in the future? \n    comp_skills_pro_benefits_s \n  \n  \n    How important do you think your programming skills are to prospective group members? \n    comp_skills_recruiting \n  \n  \n    How important programming skills are when hiring new group members? - Graduate students \n    comp_skills_recruiting_grad \n  \n  \n    How important programming skills are when hiring new group members? - Postdoctoral researchers \n    comp_skills_recruiting_postdoc \n  \n  \n    How important programming skills are when hiring new group members? - Undergraduate students \n    comp_skills_recruiting_undergrad \n  \n  \n    List of Countries \n    country_origin \n  \n  \n    What is your department? Please select all that apply. \n    dept_prof \n  \n  \n    What is your program? \n    dept_students \n  \n  \n    Do you feel disadvantaged for not knowing how to code? \n    disadv_not_coding \n  \n  \n    Distribution Channel \n    distribution_channel \n  \n  \n    If you wish to delete your survey entry write \"delete\" below \n    do_del \n  \n  \n    Do you share your code online? \n    do_share_code_online \n  \n  \n    Duration (in seconds) \n    duration_sec \n  \n  \n    What is your institutional email address? \n    email \n  \n  \n    End Date \n    end_survey \n  \n  \n    Do you think you have enough institutional support (workshops, mentorships, online labs) to learn to program? \n    enough_instit_support \n  \n  \n    Which categories best describe you? \n    ethnicity \n  \n  \n    At what age did you write your first line of code or program? (e.g., webpage, Hello World, Scratch project) \n    first_line_code \n  \n  \n    When you are working on projects (for school, work, fun)  how often  do you code? \n    freq_coding_proj \n  \n  \n    How much of the software that you use is open source? (please provide your best guess) \n    freq_oss_proj \n  \n  \n    Do you feel you have friends, colleagues, or supervisors who can help with your coding issues? \n    friends_help \n  \n  \n    How did you first learn to code? \n    how_did_you_learn_code \n  \n  \n    Finished \n    is_finished \n  \n  \n    Overall, would you like to have more time to improve your programming skills? \n    more_time_learning_to_code \n  \n  \n    What is the name of your research group? \n    name_research_group \n  \n  \n    How many advisors do you have? \n    nb_advisors \n  \n  \n    What percentage of your social contacts are likely to participate in a project that require programming in the upcoming academic year. Social contacts are classmates and other peers that you have communicated with at least briefly within the last month, either face-to-face, or otherwise. - Click to write Choice 1 \n    pct_social_contacts_coding \n  \n  \n    What benefits do you see in programming? \n    perceived_benefits_coding \n  \n  \n    Which of the following describes industry positions you have held or currently hold? \n    position_industry \n  \n  \n    What is your preferred pronouns? \n    pref_pronouns \n  \n  \n    Progress \n    progress \n  \n  \n    What qualities do you most value in software ? \n    qualities_oss \n  \n  \n    How often do you read programming books, either to improve your long-term coding skills or out of interest? \n    read_prog_book \n  \n  \n    Do you code for any of the following reasons? \n    reason_coding \n  \n  \n    Recorded Date \n    record_date \n  \n  \n    Response ID \n    response_id \n  \n  \n    Response Type \n    response_type \n  \n  \n    Score \n    score \n  \n  \n    For any of your current projects, do you consider that you spent too much time to code? (e.g. data cleaning, data visualization, gathering data, and so on)? \n    self_expect_time_coding \n  \n  \n    Do you consider yourself to be a coder/programmer? \n    self_id_as_coder \n  \n  \n    Start Date \n    start_survey \n  \n  \n    Coursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Cleaning code (debugging, refactoring, renaming variables, etc.) \n    time_cleaning_code \n  \n  \n    Coursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Data cleaning (manually, e.g. using excel) \n    time_data_clean_gui \n  \n  \n    Coursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Data cleaning (programmatically) \n    time_data_clean_prog \n  \n  \n    Coursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Digital data collection (e.g. web scraping) \n    time_digital_data_coll \n  \n  \n    Coursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Experimental manipulation \n    time_exp_manip \n  \n  \n    Coursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Field data collection (e.g. interviews, surveys, questionnaires, observations, ethnographies, etc) \n    time_field_data_coll \n  \n  \n    Coursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Grant writing \n    time_grant_writing \n  \n  \n    Coursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Literature review \n    time_lit_review \n  \n  \n    Coursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Meetings \n    time_meeting \n  \n  \n    Coursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Writing thesis/paper \n    time_paper_writing \n  \n  \n    Coursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Reading software-related content (blogs, books, forums, documentation) \n    time_read_doc \n  \n  \n    Do you consider yourself a member of an underrepresented group? \n    underrep_group \n  \n  \n    50 States, D.C. and Puerto Rico \n    us_state \n  \n  \n    Which programming languages, scripting, and markup languages have you worked in over the past year? \n    use_lang \n  \n  \n    User Language \n    user_lang \n  \n  \n    How important do you think the following items are in academia? - Code associated with an article is easy to find online \n    value_accessibility_paper_code \n  \n  \n    When thinking about whether to use open source software, how important are the following: - Active development \n    value_active \n  \n  \n    When thinking about whether to use open source software, how important are the following: - A contributor's license agreement (CLA) \n    value_cla \n  \n  \n    When thinking about whether to use open source software, how important are the following: - A code of conduct \n    value_coc \n  \n  \n    Compared to your domain expertise, how valued do you think your coding skills are around you today? \n    value_comp_skills_wrt_domain \n  \n  \n    When thinking about whether to use open source software, how important are the following: - A contributing guide \n    value_contrib_guide \n  \n  \n    Overall, how important do you think it is to learn programming in your academic field today? \n    value_learn_code_in_field \n  \n  \n    When thinking about whether to use open source software, how important are the following: - An open source license \n    value_oss_license \n  \n  \n    How important do you think the following items are in academia? - Code associated with an article is citable. \n    value_paper_code_citability \n  \n  \n    When thinking about whether to use open source software, how important are the following: - Responsive maintainers \n    value_responsive_maintainers \n  \n  \n    How important do you think the following items are in academia? - Sharing code associated with an academic paper \n    value_share_code \n  \n  \n    When thinking about whether to use open source software, how important are the following: - A welcoming community \n    value_welcoming_community \n  \n  \n    When thinking about whether to use open source software, how important are the following: - Widespread use \n    value_widespread_use \n  \n  \n    What is the primary operating system in which you work? \n    what_os \n  \n  \n    If you want to learn to program, what's stopping you? \n    why_not_coding \n  \n  \n    In what year were you born? \n    year_born \n  \n  \n    How many years have you been coding? \n    years_coding"
  },
  {
    "objectID": "posts/survey-programming/index.html#costs-benefits",
    "href": "posts/survey-programming/index.html#costs-benefits",
    "title": "Survey results (early release)",
    "section": "Costs & benefits",
    "text": "Costs & benefits\n\nviewof sel_dept_cb = Inputs.select([''].concat(\n  tidy(\n    pdata.filter(d => d[\"dept_students_lab\"] !== null),\n    distinct('dept_students_lab')\n  ).map(d => d[\"dept_students_lab\"])), { label: \"Choose dept\" })\n\nviewof by_attr_cb = Inputs.radio([\"gender_binary\", \"year_born\", \"ethnicity_binary\", \"dept_students_binary\", \"academia_status\"], { label: \"Attributes\" })\nviewof do_pct_2 =   Inputs.toggle({ label: \"Show %\" }) \nviewof show_labs =   Inputs.toggle({ label: \"Show Labels\", value: true }) \nviewof rm_nulls_2 = Inputs.toggle({ label: \"Remove nulls\", value: true })\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCostsBenefitsOSSData\n\n\n\nhtml`<p class = 'q_styled'>Do you think you have enough institutional support (workshops, mentorships, online labs) to learn to program?</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n  show_labs ? \"enough_instit_support\" : \"enough_instit_support_ord\",      \n  by_attr_cb, \n  true,  \n  do_pct_2\n)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Overall, would you like to have more time to improve your programming skills?</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders,   \n  show_labs ? \"more_time_learning_to_code\": \"more_time_learning_to_code_ord\", \n  by_attr_cb, \n  false, \n  do_pct_2\n)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>For any of your current projects, do you consider that you spent too much time to code? (e.g. data cleaning, data visualization, gathering data, and so on)?</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n show_labs ? \"self_expect_time_coding\" : \"self_expect_time_coding_ord\", \n    by_attr_cb, \n    false,  \n    do_pct_2\n)\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'> Compared to your domain expertise, how valued do you think your coding skills are around you today?</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n  show_labs ? \"value_comp_skills_wrt_domain\" : \"value_comp_skills_wrt_domain_ord\", \n  by_attr_cb, \n  false, \n  do_pct_2)\n\n\n\n\n\n\n\n\n\nfunction plot_social_contacts(dat, by_attr) {\n  return Plot.plot({\n    grid: true,\n    x: {\n      label: \"Social contact coding (%)  →\",\n    },\n    y: { \n      label: do_pct_2 ? \" Frequency (%) ↑\" : \"# respondents ↑\", \n      percent: do_pct_2 ? true : false\n    },\n    marks: [\n      Plot.barY(\n        rm_nulls ? dat.filter(d => d[\"pct_social_contacts_coding\"] !== null) : dat, \n        Plot.groupX({ y: do_pct_2 ? \"proportion\" : \"count\" }, { \n            x: \"pct_social_contacts_coding\",  fill: d => by_attr ? d[by_attr] : \"grey\"\n          })\n      ),\n      Plot.ruleY([0])\n    ]\n  })\n}\n\nhtml`<p class = 'q_styled'> What percentage of your social contacts are likely to participate in a project that require programming in the upcoming academic year?</p>`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_social_contacts(coders, by_attr_cb)\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Do you share your code online?</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \"do_share_code_online\", by_attr_cb, true,  do_pct_2)\n\n\n\n\n\n\n\nhtml`<h4>How important do you think the following items are in academia?</h4>`\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Code associated with an article is citable.</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n  show_labs ? \"value_paper_code_citability\" : \"value_paper_code_citability_ord\", \n  by_attr_cb, false, do_pct_2)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Code associated with an article is easy to find online</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n  show_labs ? \"value_accessibility_paper_code\" : \"value_accessibility_paper_code_ord\", \n  by_attr_cb, false, do_pct_2)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Sharing code associated with an academic paper</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n  show_labs ? \"value_share_code\" : \"value_share_code_ord\", \n  by_attr_cb, false, do_pct_2)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>A welcoming community</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n  show_labs ? \"value_welcoming_community\" : \"value_welcoming_community_ord\", \n  by_attr_cb, false, do_pct_2)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Widespread use</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n  show_labs ? \"value_widespread_use\" : \"value_widespread_use_ord\", \n  by_attr_cb, false, do_pct_2)\n\n\n\n\n\n\n\n\n\nBox plots\n\nagg_oss = tidy(\n  coders, \n  select([startsWith(\"value\"), \"gender_binary\"]),\n  filter(d => d.gender_binary != null),\n  pivotLonger({\n    cols: ['-gender_binary'],\n    namesTo: \"catego\",\n    valuesTo: \"val\"\n  }),\n  filter(d => /_ord/.test(d.catego) && d.val != \"NA\" && d.val <= 5)\n)\n\nPlot.plot({\n  height: 400,\n  marginTop: 0,\n  marginLeft: 250,\n  x: { inset: 10, grid: true, label: \"attitude →\" },\n  y: { axis: null, inset: 2},\n  color: {legend: true},\n  fy: {\n    label: \"questions (m) →\",\n    reverse: true\n  },\n  facet: {\n    data: agg_oss,\n    y: \"catego\",\n    marginLeft: 250\n  },\n  marks: [\n    Plot.frame({stroke: \"#aaa\", strokeWidth: 0.5}),\n    Plot.boxX(agg_oss, {x: \"val\", y: \"gender_binary\", stroke: \"gender_binary\", r: 1})\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncost_cols = ['pct_social_contacts_coding', 'comp_skills_factors_pursue_academia',\n             'comp_skills_pro_benefits_s', 'enough_instit_support',\n             'friends_help', 'perceived_benefits_coding', 'first_adv_expect_time_coding',\n             'second_adv_expect_time_coding', 'reason_coding',  'value_comp_skills_wrt_domain', \n             'do_share_code_online', \"value_learn_code_in_field\"]\n\nInputs.table(coders, { columns: cost_cols })"
  },
  {
    "objectID": "posts/survey-programming/index.html#students-typical-week",
    "href": "posts/survey-programming/index.html#students-typical-week",
    "title": "Survey results (early release)",
    "section": "Students typical week",
    "text": "Students typical week\n\n\nagg_dat = tidy(\n  coders, \n  select([startsWith('time')]), \n  pivotLonger({\n    cols: [startsWith('time')],\n    namesTo: 'task',\n    valuesTo: 'val'\n  }),\n  count(['task', 'val']\n))\n\n\nPlot.plot({\n  width: 1000,\n  marginBottom: 100,\n  grid: true,\n  x: {\n    tickRotate: 45,\n    label: \"\",\n    domain: ['0 hour', '1-5 hours', '6-10 hours', '11-15 hours', null]\n  },\n  facet: {\n    data: agg_dat,\n    x: d => d.task.replace('time_', '')\n  },\n  marks: [\n    Plot.barY(\n      agg_dat,\n      {x: \"val\", y: \"n\", fill: d => d.val === null ? 'grey' : 'task'}\n    ),\n    Plot.ruleY([0])\n  ]\n})"
  },
  {
    "objectID": "posts/survey-programming/index.html#what-do-non-coders-think-about-coding",
    "href": "posts/survey-programming/index.html#what-do-non-coders-think-about-coding",
    "title": "Survey results (early release)",
    "section": "What do non-coders think about coding?",
    "text": "What do non-coders think about coding?\n\nviewof by_attr_nc = Inputs.radio([\"gender_binary\", \"year_born\", \"ethnicity_binary\", \"dept_students_binary\", \"academia_status\"], { label: \"Attributes\" })\nviewof do_pct_nc =   Inputs.toggle({ label: \"Show %\", value: true }) \nviewof show_labs_nc =   Inputs.toggle({ label: \"Show Labels\", value: true })\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Overall, how important do you think it is to learn programming in your academic field today?</p>`\n\n\n\n\n\n\n\nplot_attr_v(non_coders, \n  show_labs_nc ? \"value_learn_code_in_field\" : \"value_learn_code_in_field_ord\",\n  by_attr_nc, \n  true, \n  do_pct_nc)\n\n\n\n\n\n\n\n\n\nimport { tidy, select, count, complete, filter, fullSeq, sum, mutateWithSummary, groupBy, pivotWider, startsWith, pivotLonger, mutate, distinct, summarize, arrange, pull } from '@pbeshai/tidyjs'\n\n\n\n\n\n\n\nhtml`<style>\n.q_styled {\n        position: relative; \n        top: 15px;\n        font-size: 14px;\n        font-family: sans-serif;\n        text-align: center;\n        font-style: italic\n    }\n</style>\n`"
  },
  {
    "objectID": "posts/rise_of_comp2/index.html",
    "href": "posts/rise_of_comp2/index.html",
    "title": "The rise of computational works 2",
    "section": "",
    "text": "TODO\n\nAdd the possibility to locate papers containing keywords in title.\nAdd the possibility to locate selected papers in the dot plot.\nAdd the possibility to locate selected topics in the dot plot.\nAdd the possibility to locate papers from selected years.\nAdd the possibility to locate papers from selected venues.\nAdd a table that we can filter based on selected topics and/or title.\n\n\n\nSpecter embeddings of computational papers (early)\n\nviewof type_cat = Inputs.select(['topic', 'field', 'year'], {value: 'field', label: \"cluster type\"})\n\nviewof sel_field = Inputs.select([''].concat(fields), {value: '', label: \"choose field\"})\n\nviewof input_title = html`<input placeholder=\"Title.\">`\n\nviewof c = rangeSlider(background_data, d=>d.year)\n\nviewof t = rangeSlider(background_data, d=>d.topic)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nembedding_plot = {\n  const background_mark = (is_colored) => {  \n            return { x: \"x\", y: \"y\", fill: is_colored ? type_cat : \"grey\",  opacity: 0.2, r: \"citationCount\" }\n        }\n  \n  const subset_mark = {\n          x: \"x\", y: \"y\", fill: type_cat, r: \"citationCount\", \n          opacity: subset === undefined ? 0.3 : 1, \n          title: d => `${d.title} (${d.field})\\n# citations: ${d.citationCount}\\nYear: ${d.year}\\ntopic: ${d.topic}`\n    }\n\n  const field_mark = {\n          x: \"x\", y: \"y\", opacity: 0.3, r: \"citationCount\",\n          // fill: sel_field[0] === undefined ? type_cat : 'red', \n          fill: type_cat === 'field' ? 'red' : type_cat,\n          title: d => `${d.title} (${d.field})\\n# citations: ${d.citationCount}\\nYear: ${d.year}\\ntopic: ${d.topic}`\n        }\n  \n  const year_mark = {\n          x: \"x\", y: \"y\", opacity: 0.8, r: \"citationCount\", \n          fill: type_cat, \n          title: d => `${d.title} (${d.field})\\n# citations: ${d.citationCount}\\nYear: ${d.year}\\ntopic: ${d.topic}`\n        }\n\n  const background_plot = () => Plot.plot({\n      inset: 8, height: 800, width: 1200,\n      y: { label: \"\", ticks: null },\n      x: { label: \"\", ticks: null },\n      r: { range: [1.5, 10] },\n      color: { type: \"categorical\", scheme: \"paired\" },\n      marks: [  Plot.dot(background_data,  background_mark(true)  ) ]\n    })\n  \n  const subset_plot = () => Plot.plot({\n      inset: 8, height: 800,  width: 1200,\n      y: { label: \"\", ticks: null },\n      x: { label: \"\", ticks: null },\n      r: { range: [1.5, 10] },\n      color: { type: \"categorical\", scheme: \"paired\" },\n      marks: [\n        Plot.dot(background_data,  background_mark(false)  ),\n        Plot.dot(subset, subset_mark )\n        ]\n    })\n  \n  const field_plot = () => Plot.plot({\n      inset: 8, height: 800, width: 1200,\n      y: { label: \"\", ticks: null },\n      x: { label: \"\", ticks: null },\n      r: { range: [1.5, 10] },\n      color: { type: \"categorical\", scheme: \"paired\" },\n      marks: [\n        Plot.dot(background_data,  background_mark(false)  ),\n        Plot.dot(data_by_field, field_mark )\n        ]\n    })\n  \n  const year_plot = () => Plot.plot({\n      inset: 8, height: 800, width: 1200,\n      y: { label: \"\", ticks: null },\n      x: { label: \"\", ticks: null },\n      r: { range: [1.5, 10] },\n      marks: [\n        Plot.dot(background_data,  background_mark(false)  ),\n        Plot.dot(yearly_data, year_mark  )\n        ]\n    })  \n\n  const subset_and_field_plot = () => Plot.plot({\n      inset: 8, height: 800, width: 1200,\n      y: { label: \"\", ticks: null },\n      x: { label: \"\", ticks: null },\n      r: { range: [1.5, 10] },\n      color: { type: \"categorical\", scheme: \"paired\" },\n      marks: [\n        Plot.dot(background_data,  background_mark(false)  ),\n        Plot.dot(data_by_field, field_mark  ),\n        Plot.dot(subset, subset_mark )\n        ]\n    })\n\n  if (subset === undefined && sel_field[0] === undefined) {\n    return year_plot()\n  } else if (subset !== undefined && sel_field[0] === undefined) {\n    return subset_plot()\n  } else if (subset === undefined && sel_field[0] !== undefined) {\n    return field_plot()\n  } else {\n    return subset_and_field_plot()\n  }\n}\n\nembedding_plot.legend(\"color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the topics here are custers of documents that are similar with respect to title, abstract, and citations. This is different from vanilla topics that cluster together exclusively based on content.\nBy adding document-level relatedness, we hope that we will be able to distinguish different facets of how the idea of computational is used in the literature. For instance, say that in philosophy there is much discussion about the computational theory of the mind. Philosophers won’t be citing the same papers than computational papers in political science that cite, say, some methodological papers on how best to construct embeddings in their field. If we are right, then conceptual papers should be fairly easy to distinguish and remove.\n\ntable\n\nInputs.table(subset === undefined ? yearly_data : subset, {columns: ['topic', 'field', 'year', 'citationCount', 'title']})\n\n\n\n\n\n\n\n\ntopic heatmap\n\nPlot.plot({\n  width: 1200,\n  marginLeft: 130,\n  x: {tickRotate: 45, label: null},\n  color: {\n    scheme: \"bupu\",\n    legend: true\n  },\n  marks: [\n    Plot.cell(table_data, {x: \"year\", y: \"topic\", fill: \"n\"})\n  ]\n})\n\n\n\n\n\n\n\ndb = DuckDBClient.of({ \n    data: await FileAttachment(\"umap_embedding.csv\").csv({typed:true})\n})\n\nbackground_data = db.sql`\n  SELECT * \n  FROM data \n  WHERE (citationCount > 0 AND x >= -8.5 AND x <= 8.5 AND y >= -10.5 AND y <= 7.5) \n  USING SAMPLE 20000\n`\n\ndata_by_field = db.sql`\n  SELECT * \n  FROM data \n  WHERE (citationCount > 0 AND x >= -8.5 AND x <= 8.5 AND y >= -10.5 AND y <= 7.5 AND field = ${sel_field} AND year >= ${c.range[0]} AND year <= ${c.range[1]} AND topic >= ${t.range[0]} AND topic <= ${t.range[1]})\n`\n\nyearly_data = db.sql`\n  SELECT * \n  FROM data \n  WHERE (x >= -8.5 AND x <= 8.5 AND y >= -10.5 AND y <= 7.5 AND year >= ${c.range[0]} AND year <= ${c.range[1]} AND topic >= ${t.range[0]} AND topic <= ${t.range[1]})\n  USING SAMPLE 20000\n`\n\ntable_data = db.sql`\n  SELECT year, topic, COUNT()::INT as n\n  FROM data \n  WHERE (year >= ${c.range[0]} AND year <= ${c.range[1]} AND topic >= ${t.range[0]} AND topic <= ${t.range[1]})\n  GROUP BY\n    year, topic\n`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntitle_search = debounce(viewof input_title)\n\nsubset = {\n  let s = Inputs.select([{title: 'Type above more than 2 letters...', eft: 0}], \n                         {format: d => d.title, labedatal: \"Titles found\"}) \n  if (title_search.length > 2) {\n    \n    return await db.query(`SELECT * FROM data \n                 WHERE ( contains(lower(title), lower('${title_search}')) AND citationCount > 0 AND x >= -8.5 AND x <= 8.5 AND y >= -10.5 AND y <= 7.5 AND year >= ${c.range[0]} AND year <= ${c.range[1]} AND topic >= ${t.range[0]} AND topic <= ${t.range[1]})`)\n  } \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLimitations\n\nThe backgrond data is a sample of 20K documents (out of 156K).\nWhen searching a title, we look at all the data (not the 20K subset)\nWe overlay filtered data on top of the background data, which is also a subset of 20K.\n\n\n\n“Degree of freedom”\n\nWith hdbscan, we can reduce the number of topics so that we have coarser grain topics.\nWith umap, we can choose the number of dimensions to reduce\n\n\nimport {Plot} from \"@mkfreeman/plot-tooltip\"\nimport {debounce} from \"@mbostock/debouncing-input\"\nimport {rangeSlider} from \"@bumbeishvili/utils\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfields = ['Art', 'Education', 'Economics', 'Environmental-Science', 'Geography', 'Geology', 'History', 'Law', 'Linguistics', 'Mathematics', 'Philosophy', 'Physics', 'Political-Science', 'Psychology', 'Sociology']"
  },
  {
    "objectID": "posts/s2orc_viz/index.html",
    "href": "posts/s2orc_viz/index.html",
    "title": "S2ORC Viz",
    "section": "",
    "text": "Abstracts and pdf parsed shows the total number of papers and the number of papers for which either the abstract or pdf is parsed.\ns2fos v. mag is a showdown between the semantic scholar classification scheme and the microsoft academic graph one. Papers are grouped by fields and year, and we keep track of the parsing extent for each.\n\n\nimport {addTooltips} from \"@mkfreeman/plot-tooltip\"\n\n\n\n\n\n\n\ndata = FileAttachment(\"count_field_and_decade.csv\").csv({ typed: true })\n\n\n\n\n\n\n\nviewof schema = Inputs.select([\"s2fos\", \"mag\", \"both\"], { label: \"Schema\" })\nviewof show_pct = Inputs.toggle({label: \"Normalize\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbstracts and pdf parseds2fos v. magraw data\n\n\n\n\np1 = Plot.plot({\n    marginLeft: 100,\n    height: 600,\n    width: 1200,\n    marginBottom: 100,\n    x: { label: null, tickRotate: 45 },\n    y: { \n      grid: true, \n      percent: show_pct ? true : false, \n      label: show_pct ? \"↑ rep. (%)\" : \"↑ n papers\"\n     },\n    marks: [\n      Plot.barY(data.filter(d => d.year < 2020), Plot.groupX({y: \"sum\"}, { x: \"year\", y: d => schema == 'both' ? d[\"n\"] : d[`n_${schema}`], fill: \"parsing\", order: \"sum\", offset: show_pct ? \"expand\" : null }))\n    ]\n})\np1.legend(\"color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof mark_type = Inputs.select([\"stacked\", \"line\"], { label: \"Plot type\" })\nviewof parsing = Inputs.select([\"abstract\", \"pdf\", \"all\"], { label: \"Parsing extent\" })\nviewof chosen_group = Inputs.select([\"STEM\", \"Social Science\", \"Misc\"], { label: \"Group\" })\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata_f = data.filter(d => d.group == chosen_group && d.year < 2020 && d.parsing == parsing)\n\nline_plot = () => {\n  const xy_norm = Plot.normalizeY({\n          x: \"year\",  y:  d => schema == 'both' ? d[\"n\"] : d[`n_${schema}`], stroke: \"field\", title: \"field\", basis: \"first\"\n      })\n  const xy = {\n          x: \"year\",  y:  d => schema == 'both' ? d[\"n\"] : d[`n_${schema}`], stroke: \"field\", title: \"field\"\n      }\n  return addTooltips(Plot.plot({\n      marginLeft: 100,\n      height: 600,\n      width: 1200,\n      marginBottom: 50,\n      x: { label: null, tickRotate: 45 },\n      y: { \n        percent: show_pct ? true : false, \n        grid: true, \n        label: show_pct ? \"↑ Norm. over first value, showing relative growth\" : \"↑ n papers\" },\n      marks: [\n        Plot.line(data_f, show_pct ? xy_norm : xy)\n      ]\n  }))\n}\n\nstack_plot = () => {\n  const xy = { x: \"year\", y: d => schema == 'both' ? d[\"n\"] : d[`n_${schema}`], z: \"field\", order: \"sum\",  offset: show_pct ? \"expand\" : null }\n\n  return addTooltips(Plot.plot({\n        marginLeft: 100,\n        height: 600,\n        width: 1200,\n        marginBottom: 50,\n        x: { label: null, tickRotate: 45 },\n        y: { \n          percent: show_pct ? true : false, \n          grid: true, \n          label: show_pct ? \"↑ rep. (%)\" : \"↑ tot papers\" },\n        marks: [\n          Plot.barY(data_f, Plot.stackY({...xy, fill: \"field\", title: d => `${d.field} (${d.n})` })),\n        ]\n    }))\n}\n\n\np3 = mark_type == 'stacked' ? stack_plot() : line_plot()\n\np3.legend(\"color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInputs.table(data)\n\n\n\n\n\n\n\n\n\n\nRemarks\n\nBiology is an encompassing field. I wish we could divide it further.\nIn my opinion the biggest difference beween magand s2fos schema is that the later track the emergerce of computer science, which is nice.\n\n\n\nNotes\nClassifying articles into fields is non-trivial. But at least there is work on the topic.\nFor a while, the microsoft academic graph (mag) was standard. Many researchers used the top mag fields of study as their main taxonomy. Now that the mag project at microsoft is deprecated (Dec. 31, 2021.; see here), AllenAI’s semantic scholar is arguably one of the best contender to become the next standard. Contrary to MAG, they provide a unified api for the citation graphs of papers and they released a fraction of their overall paper nodes as parsed text. Announced after mag deprecation, the semantic scholar databse did some more work on the classification scheme, which can be found here. This is what we use and compare here, as did other before us.\nWe note that all the current schemes agree that papers have overlapping fields and are hierarchical. This is something that the mag field made explicit through the use of a hierarhical algorithm for unsupervised topic classification. The s2_fos scheme drops the hierarchy and focus on the top fields using a (simpler) classifier."
  },
  {
    "objectID": "posts/rise_of_comp3/index.html",
    "href": "posts/rise_of_comp3/index.html",
    "title": "The rise of computational works 3",
    "section": "",
    "text": "import {Plot} from \"@mkfreeman/plot-tooltip\""
  },
  {
    "objectID": "posts/rise_of_comp3/index.html#rise-of-programming",
    "href": "posts/rise_of_comp3/index.html#rise-of-programming",
    "title": "The rise of computational works 3",
    "section": "Rise of programming?",
    "text": "Rise of programming?\n\n\n\nviewof select = Inputs.select(fields, { multiple: true, value: ['linguistics', 'philosophy', 'history'] })\nviewof show_pct = Inputs.toggle({label: \"normalize\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata_f = which(rin(data.map(d => d.field), select)).map(i => data[i]).filter(d => d.year > 1970)\n\nline_plot = () => {\n  \n  const xy_norm = {\n          x: \"year\",  y:  'pct_comp', stroke: \"field\", title: \"field\"\n      }\n    \n  const xy = {\n          x: \"year\",  y:  'n_comp', stroke: \"field\", title: \"field\"\n      }\n\n  return Plot.plot({\n      marginLeft: 50,\n      height: 600,\n      width: 800,\n      marginBottom: 50,\n      x: { label: null, tickRotate: 45 },\n      y: { \n        percent: show_pct ? true : false, \n        grid: true, \n        label: show_pct ? \"↑ %\" : \"↑ n papers\" },\n      marks: [\n        Plot.line(data_f, show_pct ? xy_norm : xy)\n      ]\n  })\n}\n\nlp = line_plot()\n\nlp.legend(\"color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata = FileAttachment(\"comp_normalized.csv\").csv({ typed: true })\n\nfields = ['mathematics', 'geology', 'environmental science', 'art','history', 'linguistics', 'psychology',  'education', 'geography', 'physics', 'law', 'sociology', 'economics', 'political science', 'philosophy']\n\nfunction rin(arr1, arr2) {\n  return Array.from(arr1, (x) => {\n    return arr2.indexOf(x) == -1 ? false : true\n  })\n}\n\nfunction which(x) {\n  return x.reduce(\n      (out, bool, index) => bool ? out.concat(index) : out, \n      []\n    )\n}"
  },
  {
    "objectID": "posts/rise_of_comp/index.html",
    "href": "posts/rise_of_comp/index.html",
    "title": "The rise of computational works 1",
    "section": "",
    "text": "These days there’s a computational version of everything. Computational biology, computational musicology,computational archaeology,and so on, ad infinitum. Even movies are going digital. Dan Jurafsky, 2006\n\nAlready in 2006, there was a feeling that science was becoming more and more computational. But the rise of computational works is not evenly distributed. While some fields have a long history of using computer science to meet their computational needs, others are concerned that the process of representing their objects of study in a digital format may distort them. Yet others think that programming, say in the humanities, is mostly about acquiring marketable skills that transfer to industry, and as such it goes against long-term values of humanists.\nUnlike previous technologies, programming is able to creep into any field as it permeates many facets of scientific work, e.g. statistics, communication, visualization, simulation, data collection, etc. While it is true that some objects of study are more challenging to represent on computers, the rate at which computational methods are nevertheless being adopted by computational enthusiasts could create a wedge between them and more reluctant fields. This imbalance might even lead to a situation where the adoption of computational tools gives certain individuals or groups a disproportionate amount of influence, especially in fields undergoing a significant shift towards computational works. For example, there is evidence that institutions with more resources to fund larger research groups already increase disproportionately faculty productivity. What if institutions with a greater labor advantage also benefit the most from this computational turn. Those who embrace computational tools may have an advantage in terms of visibility and funding, potentially allowing them to more easily disseminate their ideas.\nA strong disparity in methods might have important consequences on the evolution of ideas in science. We ask ourselves, is the rise of computational work is a source of epistemic inequality? Does it benefit groups already favored because of their greater labor resources and institutional prestige? If the cost of learning programming is low enough for some individuals, and those individuals cluster together, will we see a gap forming even within the field? Will field of studies that are more computational spill over into other fields?\nTo assess whether the rise of computational work has an effect on the evolution of ideas in science, we need to be able to identify computational works. Without this first step, we won’t be able to measure the effects of groups adopting computational methods, or how labor advantages interact with the rise of programming. There are surprisingly few studies quantifying the relative adoption of computational works in different fields. Recent advances in the availability of large-scale data and NLP tools make this possibility more accessible than ever before.\nWe define computational works as projects that seek to understand complex systems using visualizations, simulations, and/or inference processes that require computers and programming languages. Here a few examples, slightly biased from my experience, in chronological order:\n\nplot = Plot.plot({\n  y: {ticks: null},\n  width,\n  marks: [\n    Plot.text(\n      data_manual,\n      Plot.dodgeY({\n        x: \"date\",\n        text: \"author\",\n        title: d => `${d.author} (${d.date.getFullYear()}):\\n${d.desc.slice(0, 130).concat('\\n', d.desc.slice(130))}`,\n        r: 30,\n        fill: \"category\",\n        lineWidth: 7,\n        anchor: \"middle\"\n      })\n    ),\n    Plot.ruleY([0], {dy: 175}),\n    Plot.ruleX(d3.range(1960,2020,5), {x:d => (new Date(`${d}-01-01`)), opacity: 0.3, lineType: 'dashed'})\n  ]\n})\n\nplot.legend(\"color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll of these papers share the fact that they could not have been done without extensive computers and related computer skills. By extensive, I mean that these papers wouldn’t be doable without computers and programming skills because it would require too much time or effort.1 In cases where computer science and mathematics have an application, or are used in a field, it typically requires skills that go beyond a traditional curriculum. For example, Burrows’ analysis of Jane Austen’s text required skills that go beyond the literary studies curriculum.\nNote that some articles are computational in the sense that they deal with computational stuff, but they remain conceptual. We include them because papers with computational methods ought to cite them.\nOk, so how are we gonna classify computational works? Here is the plan schema:\n\nFirst, we look at articles containing the computational keyword, which will include variations of it because s2_search is a kind of elastic search. This is imperfect but it should be better than asking for digital or programming. Then, we project these paper embeddings into lower dimensions with umap. Finally, we use hdbscan, a density-based clustering algorithm, to cluster similar papers.\nThat’s it. The hope is that because document embeddings are informed by title, abstract, and citations, papers that are more computational ought to cluster together. We will evaluate the performance of our pipeline on the subset of papers that we manually identified as computational while reviewing the literature.\nWe already did some of that, which you can find here.\n\n\n\n\n\n\nNotes on the pipeline:\n\n\n\n\nWe don’t use full texts at the moment, but we could.\nWe place a lot of hope on Spectre embeddings, as we expect to distinguish between types of computational papers based on citation patterns.\nWe’re going to have a lot of false positives and negatives as a result of the first step. This should be improved.\n\n\n\n\nRefs for the methods:\n\nallenai/s2orc (paper)\nallenai/s2search (blog post)\nallenai/specter (paper)\nWe based our specter2top step on ddangelov/Top2Vec (paper)\n\numap\nhdbScan\n\n\n\ndata_manual = [\n  {'author': 'Lorenz', 'date': new Date('1963-01-01'), 'desc': 'Determining nonperiodic flow by way of numerical solution of convection equations', 'category': 'Numerical simulations'},\n  {'author': 'Gillepsie', 'date': new Date('1976-01-01'), 'desc': 'Simulating the stochastic time evolution of coupled chemical reactions using the Monte Carlo simulation procedure.', 'category': 'Numerical simulations'},\n  {'author': 'Busa', 'date': new Date('1980-01-01'), 'desc': ' Counting words from Thomas Aquinas body of work to better understand Aquinas his philosophical assumptions and truths.', 'category': 'Digital Humanities'},\n  {'author': 'Diaconis & Efron', 'date': new Date('1983-01-01'), 'desc': 'Use of the bootstrap to estimate the correlation coefficient from the data, thus replacing the traditional statistical procedures making assumptions of normality on the data.', 'category': 'Statistical Inference'},\n  {'author': 'Burrows', 'date': new Date('1989-01-01'), 'desc': ' A statistical analysis, or computer-assisted analysis, of literary texts using frequency analysis.', 'category': 'Digital humanities', 'field': 'Literature'},\n    {'author': 'Gelman & Rubin', 'date': new Date('1992-01-01'), 'desc': ' The use and pitfalls of using the Gibbs sampler to summarize multivariate distributions underlying data analysis.', 'category': 'Statistical Inference'},\n  {'author': 'Papert', 'date': new Date('1996-01-01'), 'desc': 'Computational thinking is a popular idea in education of how computers can be the basis of a new type of education in society and science.', 'category': 'Computational thinking', 'field': 'Education'},\n  {'author': 'Tisue & Wilenski', 'date': new Date('2004-01-01'), 'desc': 'Studying complex systems using multi-agent programming languages.', 'category': 'ABMs'},\n  {'author': 'Adamic & Glance', 'date': new Date('2005-01-01'), 'desc': 'Analyzing blog posts from political bloggers as a way to better understand the role of polarization between conservatives and liberals in the U.S Presidential Election of 2004.', 'category': 'Computational Social Science', 'field': 'Political Science'},\n  {'author': 'Kossinets & Watts', 'date': new Date('2006-01-01'), 'desc': 'Analyzing large social networks using tools from network theory.', 'category': 'Computational Social Science'},\n  {'author': 'Hall, Jurafsky & Manning', 'date': new Date('2008-01-01'), 'desc': 'Understanding the evolution of ideas in the field of computational linguistics using topic modeling on all of the ACL anthology', 'category': 'Computational Linguistics', 'field': 'SciSci'},\n  {'author': 'Michel et al.', 'date': new Date('2011-01-01'), 'desc': 'Google books.', 'category': 'Computational Social Science', incorporated: true},\n  {'author': 'Grimmer & Steward', 'date': new Date('2013-01-01'), 'desc': 'It is also an example of replicable work, where the code and data are available on the Harvard Dataverse.', 'category': 'Computational Social Science', 'field': 'Political Science', reproducible: true},\n  {'author': 'Recasens, Danescu-Niculescu-Mizil, & Jurafsky', 'date': new Date('2013-01-01'), 'desc': 'TODO', 'category': 'Computational Linguistics', incorporated: true},\n  {'author': 'Barbera', 'date': new Date('2015-01-01'), 'desc': 'It is also an example of replicable work, where the code and data are available on GITHUB.', 'category': 'Computational Social Science', 'field': 'Political Science', reproducible: true, github: true}\n  ]\n\n\n\n\n\n\n\nimport {Plot} from \"@mkfreeman/plot-tooltip\"\n\n\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\nThere was a time when women were the computers. The original Monte Carlo algorithm was calculated by them during WWII. The calculations were simple enough that it was technically possible for humans to perform them. The trajectory of the first Apollo mission was also computed by women. But as we get deeper into the second half of the 20th century, this is no longer the case↩︎"
  },
  {
    "objectID": "posts/interactive-stats/index.html",
    "href": "posts/interactive-stats/index.html",
    "title": "CDAE stats",
    "section": "",
    "text": "The equation:\n\\[n^* = \\frac{n_0}{1 + \\frac{n_0}{N}}\\]\nwhere \\(n_0 = z_\\alpha^2 \\frac{S^2}{D^2}\\), \\(S^2\\) is our population variance, \\(D^2\\) is the difference between the true value and the estimated value, and \\(z_\\alpha^2\\) is the \\(z\\) value at a given confidence interval.1\n\n\n\nWe know…\n\nThere are 5,000 nonprofits in the city of reference\nFrom a previous study, we know that the the mean value of using new tools is $3,000. We also know from previous studies that the s.d. of this is $3,500.\n\nWe want…\n\nAn error rate of 10%\nA confidence interval of 95%\n\n\n\nfunction calc_n_0(z_alpha_sq, S, D) {\n    return z_alpha_sq * (S**2 / D**2)\n}\n\nfunction effective_sample_size(z_alpha_sq, S, D, N) {\n    const n_0 = calc_n_0(z_alpha_sq, S, D)\n    return +(n_0 / (1 + (n_0 / N))).toFixed(1)\n}\n\nfunction ci2z(ci) {\n     if (ci === \".68\") {\n        return 1\n     } else if (ci === \".95\") {\n        return 2\n     } else if (ci === \".99\") {\n        return 3\n     }\n}\n\nviewof conf_int = Inputs.radio([\".68\", \".95\", \".99\"], {value: \".95\", label: \"Conf. interval\"})\nviewof error_rate = Inputs.range([0.05, 1], {value: 0.1, step: 0.05, label: \"Error rate\"})\nviewof N = Inputs.range([0, 100000], {value: 5000, step: 1000, label: \"N\"})\nviewof prev_mean = Inputs.range([0, 10000], {value: 3000, step: 500, label: \"Prev mean\"})\nviewof prev_std = Inputs.range([0, 10000], {value: 3500, step: 500, label: \"Prev std\"})\nz_alpha_sq = ci2z(conf_int)**2\n\nS = prev_std\nD = error_rate * prev_mean\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, \\(S^2\\) is the previous standard deviation squared and \\(D\\) is the wanted error rate times the previous mean, that is,  x  = .\nWe find that the minimum adequate sample size, or \\(n^*\\):\n\\(n^0\\) = \n\\(n^*\\) =  / (1 +  / ) = \n\\(n_0/N\\) = \nAlso, we saw in class that \\(n^*\\) converges around \\(600\\), with the default parameters. That is, adding more data does not entail a higher \\(n^*\\). You can observe that fact with the following plot:\n\nxs = [...Array(N).keys()];\nys = xs.map(x => effective_sample_size(z_alpha_sq, S, D, x))\nPlot.lineY(ys).plot({height: 200, width: 300, y: {label: \"↑ n*\"}, x: {label: \"N →\"}})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut you can play around with other settings to see how it varies."
  },
  {
    "objectID": "posts/interactive-stats/index.html#power-analysis",
    "href": "posts/interactive-stats/index.html#power-analysis",
    "title": "CDAE stats",
    "section": "Power analysis",
    "text": "Power analysis\nSee Patrick Mineault notebook"
  },
  {
    "objectID": "posts/interactive-stats/index.html#the-many-lives-of-statistical-tests",
    "href": "posts/interactive-stats/index.html#the-many-lives-of-statistical-tests",
    "title": "CDAE stats",
    "section": "The many lives of statistical tests",
    "text": "The many lives of statistical tests\nSometimes I feel that the popularity of statistical testing is about outsourcing statistical work of busy scientists to flow charts. In research methods courses focusing on statistical testing I feel there is an understanding that these are limited, but given time and interest of students, it’s better than nothing. And if you’re sticking to experimental setups, that might be all you need. I am not going to do a rant. But I want to supplement the usual search method class with alternative perspectives explained as simply as possible:\n\nThe Frequentist approach. This if often the first encounter with inferential statistics in social socience. As long as you are in an experimental set-up this might be fine. You need to think about probability as long-run probability.\nThe linear models approach. Instead of starting from statistical tests, we start from linear models and explain which models map onto which tests. This approach promotes flexibility at the costs of having to learn the underlying ideas of linear models.\nHypothesis testing but Bayesian. No need to remember the nonsense that we “fail to reject the null” and that 0.95 confidence interval does not mean that we are “95% confident that our results are significant”.\nThe Bootstrap approach. This is a great coding exercice and saves you time from remembering all the different tests.\n\nNote that we use the following emojis to encode data types:\n\n💡 : Yes/no, 2 levels, success/failure, bias/fair… nominal data.\n📊 : Yes/no/maybe, >2 levels, might be ordinal or nominal.\n📏 : continuous/scalar/uncountable data.\n\n\n💡 ~ 💡📊 ~ 💡📏 ~ 💡\n\n\n\n\n\n\nNHST\n\n\nR code\na <- chisq.test(d_mat) # p-value > 0.05\n\n\n\n\nLinear models\n\n\nR code\n# Using glm to do a log-linear model\nfull = glm(n ~ early_first_line * sex, family = poisson(), data = d_long) \nb = anova(full, test = 'Rao') #  similar to our two-way ANOVA\n\n\n\n\nSummary\n\n\n# A tibble: 2 × 2\n  p.value model     \n    <dbl> <chr>     \n1   0.927 chisq.test\n2   0.647 glm       \n\n\n\n\n\n\n\n\n\nNHST\n\n\nR code\na <- chisq.test(d_mat) # p-value > 0.05\n\n\n\n\nLinear models\n\n\nR code\nfull = glm(n ~ self_id_as_coder * sex, family = poisson(), data = d_long) # log-linear model\nb = anova(full, test = 'Rao') #  similar to our two-way ANOVA\n\n\n\n\nSummary\n\n\n# A tibble: 2 × 2\n  p.value model     \n    <dbl> <chr>     \n1  0.0235 chisq.test\n2  0.0235 glm       \n\n\n\n\n\n\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\nWarning: The `origin` argument of `stat_bin()` is deprecated as of ggplot2 2.1.0.\nℹ Please use the `boundary` argument instead.\n\n\n\n\n\n\n\n\n\nNHST\n\n\n\n\n\nLinear models\n\n\n\n\n\nSummary\n\n\n# A tibble: 3 × 5\n    p.value estimate conf.low conf.high model \n      <dbl>    <dbl>    <dbl>     <dbl> <chr> \n1 0.163        -17.9    -43.4      7.73 t.test\n2 0.0000119     NA       NA       NA    glm   \n3 0.163         NA       NA       NA    glm"
  },
  {
    "objectID": "posts/allotax-in-quarto/index.html",
    "href": "posts/allotax-in-quarto/index.html",
    "title": "ALLotaxonometer",
    "section": "",
    "text": "viewof form = Inputs.form(\n  [\n    Inputs.select(d3.range(elem_names.length), {label: \"System 1\", multiple: true, multiple: 3, value: [0], format: x => elem_names[x]}),\n    Inputs.select(d3.range(elem_names.length), {label: \"System 2\", multiple: true, multiple: 3, value: [1], format: x => elem_names[x]}),\n  ],\n  {\n    template: (inputs) => htl.html`<div style=\"display: flex; gap: 4em\">\n  <br>${inputs}\n</div>`\n  }\n)\n\nviewof alpha = Inputs.range([0, alphas.length-1], {step: 1, label: \"α\", format: x => alphas[x]})\n\nnext_button()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`\n<br>\n<div style=\"display:flex; align-items:center; gap: 10em; margin-left: -75px; justify-content: center; width: 100%; text-align: center; font-size: 22px; \">\n      <div>${ tex`\\Omega_1` }: ${ title(0) }</div> \n      <div>${ tex`\\Omega_2` }: ${ title(1) }</div>  \n</div>\n<div style=\"display:flex; gap: 28em; margin-top: -100px; justify-content: center;\">\n    <div id=\"diamondplot\"></div>\n    <div style=\"margin-top: 110px;\" id=\"wordshift\"></div>\n</div>\n<div style=\"display:flex; align-items:center; gap: 14em;  margin-left:-95px;justify-content: center; margin-top:-300px\">\n    <div id=\"legend\"></div>\n    <div id=\"balance\"></div>\n</div>\n`\n\n\n\n\n\n\n\n\nall = import('https://cdn.skypack.dev/allotaxonometer@1.1.9?min')\n\nelem = FileAttachment(\"data/elem_girls.json\").json()\nelem_names = Object.keys(elem)\nmutable clicks = 0\nsel_sys1 = elem_names[(form[0][0]+clicks) % elem_names.length]\nsel_sys2 = elem_names[(form[1][0]+clicks) % elem_names.length]\nelems1 = elem[sel_sys1]\nelems2 = elem[sel_sys2]\n\nme_class = new all.mixedElems(elems1, elems2)\n\nrtd = me_class.RTD(alphas[alpha])\ndat = me_class.Diamond(rtd)\n\ndiamond_dat = dat.counts\nwordshift = me_class.wordShift(dat)\nbalance_dat = me_class.balanceDat() \n\np1=all.DiamondChart(diamond_dat);\np2=all.WordShiftChart(wordshift, { height: 670 });\np3=all.BalanceChart(balance_dat);\np4=all.LegendChart(diamond_dat);\n\ntitle = (sys) => {\n  const out = sys == 0 ? sel_sys1 : sel_sys2\n  return out.replace(/.((c|t)sv|json)/i, \"\")\n}\n\nalphas = d3.range(0,18).map(v => +(v/12).toFixed(2)).concat([1, 2, 5, Infinity])\n\nnot_zero = tex`\\propto \\sum_\\tau \\Big| \\frac{1}{r_{\\tau,1}^{ ${ alphas[alpha] }} } - \\frac{1}{r_{\\tau,2}^{ ${ alphas[alpha] }}} \\Big| `\n\nzero = tex`\\propto \\sum_\\tau \\Big|\\ln \\frac{r_{\\tau,1}}{r_{\\tau,2}}\\Big| `\n\ninfinity = tex`\\propto \\sum_\\tau (1 - \\delta_{r_{\\tau,1}, r_{\\tau,2}}) \\times\\max\\Big\\{\\frac{1}{r_{\\tau,1}},\\frac{1}{r_{\\tau,2}} \\Big\\} `\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport { button } from \"@bartok32/diy-inputs\"\n\nnext_button = () => button({\n  title: \"\",\n  value: \"Next\",\n  desc: \"\",\n  buttonStyle: {\n    background: \"#7295FF\",\n    color: \"white\"\n  },\n  onclick: (objs) => {\n    mutable clicks += 1;\n\n    d3.select(objs.button)\n      .style(\"background\", \"#6786E5\")\n      .interrupt()\n      .transition()\n      .duration(300)\n      .style(\"background\", \"#7295FF\");\n\n    if (mutable clicks > 0 && objs.output === \"\") {\n      objs.output = d3\n        .select(objs.div)\n        .insert(\"a\", \"div.desc\")\n        .attr(\"class\", \"output\")\n        .style(\"margin-left\", \"5px\")\n        .style(\"font-size\", \"11px\")\n        .style(\"cursor\", \"pointer\")\n        .style(\"border\", \"0.5px solid black\")\n        .style(\"border-radius\", \"5px\")\n        .style(\"padding\", \"5px\")\n        .on(\"click\", function () {\n          this.remove();\n          objs.output = \"\";\n          mutable clicks = 0;\n        })\n        .html(\"RESET\");\n    }\n  }\n})"
  }
]