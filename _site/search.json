[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "shambolics",
    "section": "",
    "text": "CDAE stats\n\n\n\n\n\n\n\nCDAE\n\n\nInteractive\n\n\n\n\nStatistics is hard. Computational statistics makes it a bit better.\n\n\n\n\n\n\nFeb 6, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nS2ORC Viz\n\n\n\n\n\n\n\nVisualization\n\n\nNLP\n\n\nS2ORC\n\n\nInteractive\n\n\n\n\nI always forget about the s2orc database details. Lets have them here.\n\n\n\n\n\n\nFeb 6, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nSurvey results (early release)\n\n\nPreliminary results from a survey on the cost and benefits of learning to code\n\n\n\n\nSurvey\n\n\nProgramming\n\n\n\n\nThe costs and benefits of learning to code in science vary across individuals and disciplines. We should have a better idea of these tradeoffs before selling anyone on coding.\n\n\n\n\n\n\nFeb 6, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nThe rise of computational works 1\n\n\n\n\n\n\n\nVisualization\n\n\nNLP\n\n\nS2ORC\n\n\nInteractive\n\n\nSciSci\n\n\n\n\nWhere, how, and when did computational stuff became popular in science?\n\n\n\n\n\n\nFeb 6, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nThe rise of computational works 2\n\n\n\n\n\n\n\nVisualization\n\n\nNLP\n\n\nS2ORC\n\n\nInteractive\n\n\nSciSci\n\n\n\n\nA new hope.\n\n\n\n\n\n\nFeb 6, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nThe rise of computational works 3\n\n\n\n\n\n\n\nVisualization\n\n\nNLP\n\n\nS2ORC\n\n\nInteractive\n\n\nSciSci\n\n\n\n\nThe computational works strike back\n\n\n\n\n\n\nFeb 6, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to MongoDB\n\n\n\n\n\n\n\nref\n\n\nmongoDB\n\n\n\n\nDatabase, database, database,…\n\n\n\n\n\n\nFeb 6, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/charting_a_phd/index.html",
    "href": "posts/charting_a_phd/index.html",
    "title": "Charting a PhD",
    "section": "",
    "text": "Constellation of ideas from different fields I care about:\n\norange: science of science/bibliometry/sociology of science\nred: open source software (OSS) in science\ngreen: computer and internet history\nblue: OSS studies\n\n\n\n\n\n\n\n1. Computational science is already here, it’s just not evenly distributed.\n\n\n\n\n\nThis is a feeling that many people share, but I did not see many qunatative study about it. We know that the spread of computational works is uneven. Physics, chemistry, materials science and perhaps parts of ecology are at the forefront, while social sciences and humanities have only recently begun to embrace computational works (see [[wing_computational_2006]], [[horn_pragmatics_2006]], [[lazer_social_2009]], [[berry_computational_2011]]).\n\n\n\n\n\n\n\n\n\n2. Prestigious institions have greater labor advantage, leading to larger research group size, which lead to greater productivity\n\n\n\n\n\nscisci/prestige: see [[zhang_labor_2022]]\n\n\n\n\n\n\n\n\n\n3. Women are met with systemic challenges at research university\n\n\n\n\n\nscisci/gender: For instance, we can think of balancing personal life and productivity, more generally especially when it comes to parenting and maternity leaves ( [[cole_productivity_1984]], [[morgan_unequal_2021]]). There are alsosystemic inequalities that remain with respect to citation behaviors ([[lariviere_bibliometrics_2013]]).\n\n\n\n\n\n\n\n\n\n4. Computational thinking education has not caught up in fields traditionally unrelated to computing\n\n\n\n\n\nscisci/training: see [[butcher_persistent_2007]], [[anderson_student_2016]], [[touchon_mismatch_2016]]1\n\n\n\n\n\n\n\n\n\n5. Many people are calling for improved coding practices to make code more reproducible and transparent.\n\n\n\n\n\noss-in-sci: Computational education is no longer limited to techniques. This means that people should not learn the basics of coding but also have a more extensive knowledge of different software engineering tools ([[prlic_ten_2012]], [[wilson_good_2017]],[[minocher_estimating_2021]], [[culina_low_2020]], [[trisovic_large-scale_2022]]).\n\n\n\n\n\n\n\n\n\n6. Universities in general, and the humanities in particular, are confronted with the growing neoliberal vision of knowledge production.\n\n\n\n\n\nscisci x DH: There is a tendency to focus on quantitative metrics such as citations and visibility to quantify research values, which could have an effect on who is hired and how funding is allocated ([[piper_there_2016]], [[piper_publication_2017]], more bibliometric stuff, also in recurrent reason of why to code in DH).\n\n\n\n\n\n\n\n\n\n7. The relationships between more qualitative science and computer science is complicated.\n\n\n\n\n\nscisci x DH: as they don’t necessary share the same goals ( [[wallach_computational_2018]]), coding might play other roles than advancing knowledge ([[allington_neoliberal_2016]], [[gold_scandal_2019]]) and qualitative knowledge, by definition, is not trivial to encode on a computer.\n\n\n\n\n\n\n\n\n\n8. Women+ have been pushed out from computer science in the 1980s\n\n\n\n\n\ncomp-history: See [[abbate_recoding_2012]], [[hicks_programmed_2018]] [[thompson_coders_2020]]. This is visible in gender-biased word embeddings trained on recent language corpora ([[bolukbasi_man_2016]]). In recent years, people have fight to change this state of affairs but this is far from a victory [[laberge_subfield_2022]].\n\n\n\n\n\n\n\n\n\n10. As with other online communities, there is toxicity in the open source world, especially when it comes to identity politics.\n\n\n\n\n\noss-studies: To understand toxicity, one must go back to some facets of the hacker culture and transition from hacker cultures to brogrammers, e.g. humors, showing skills, ethics, etc ([[coleman_coding_2013]], [[miller_did_2022]]). See [[skud_being_2011]], [[balali_newcomers_2018]], [[trinkenreich_womens_2022]], as well as [[lakhani_why_2003]], [[shah_motivation_2006]] for motivation in OSS.\n\n\n\n\n\n\n\n\n\n11. In the 1980s, computing began to be seen as a male activity.\n\n\n\n\n\ncomp-history: The male bias means a generation of girls were less and less in contact with computers.\n\n\n\n\n\n\n\n\n\n12. Many “digital natives” are no hackers.\n\n\n\n\n\ncomp-history: Although computing was seen as male activity, it does not mean that all males were hackers. Starting in the 1980s, computers were tied to videogames and computer became accessible via graphical interfaces. Many “digital natives” are no hackers ([[osullivan_programming_2015]]). This situation is arguably tied to smaller and smaller computer chips that did not invite the newer generations to hack their technologies (Rasberry Pi talk ).\n\n\n\n\n\nConnecting the facts:\n\nTODO\n\n\n\nRQs:\n\nDo we see a rise in computational works across fields? Which fields experienced faster growth? Which fields are lagging?\nIs the rise of programming within field is driven by one or a few institutions? Are these institutions historically related to computing and the Internet development?\n\n\n\n\n\nflowchart LR\nlabor_advantage --> productivity \nlabor_advantage --> computational --> productivity\ninst_prestige --> labor_advantage\n\n\n\n\n\n\nWhat is the effect of groups taking a computational turn on team diversity (e.g. gender and fields of provenance) v. popularity/productivity? Something like:\n\n\n\n\n\nflowchart LR\ncomputational --> fieldOfStudy_team\ncomputational --> gender_team\ncomputational --> citation\n\n\n\n\n\n\nCan we detect a wedge forming between computational groups within fields traditionally unrelated to computer science and traditional research groups?\nDo computational groups are significantly receiving more funding because they are now doing computational works? Can we show something like:\n\n\n\n\n\nflowchart LR\ncomputational --> trustworthiness --> selected \ncomputational --> newsworthiness --> selected\n\n\n\n\n\n\n\n\n\n\n\n1. Is privileged early access to digital infracture gave an early advantage to prestigious institions, even more so with the rise of computational works?\n\n\n\n\n\ncomp-history: Some institutions had privileged access to the internet and digital infrastructure before others, which mean they benefited from digital technologies for a longer time. These institutions might be tied to prestige status. As the internet became delocalized, the relative importance of being the first to be on the internet might have been decreasing.\n\n\n\n\n\n\n\n\nFootnotes\n\n\nNote that some of the articles are about the gap between statistical education in ecology and current methods used nowadays. Underlying this argument lies a gap between traditional statistics and often computational methods requiring to various to know how to program.↩︎"
  },
  {
    "objectID": "posts/survey-programming/index.html",
    "href": "posts/survey-programming/index.html",
    "title": "Survey results (early release)",
    "section": "",
    "text": "pdata = FileAttachment(\"data_clean.csv\").csv({ typed: true })\n\n// useful vars\ncolnames = d3.sort(Object.keys(pdata[0]))\ncoders = pdata.filter(d => d.is_coder === 'coder')\nnon_coders = pdata.filter(d => d.is_coder === 'non coder')\ncoder_count = tidy(pdata, count(\"is_coder\")).map(d => d.n)\n\n// global filtering\nfiltered_dat = sel_dept == \"\" ? pdata : pdata.filter(d => d[\"dept_students_lab\"] === sel_dept)\nfiltered_coders = sel_dept == \"\" ? coders : coders.filter(d => d[\"dept_students_lab\"] === sel_dept)\nWe currently have  valid responses, from  different departments. There are  coders and  non-coders."
  },
  {
    "objectID": "posts/survey-programming/index.html#profiles",
    "href": "posts/survey-programming/index.html#profiles",
    "title": "Survey results (early release)",
    "section": "Profiles",
    "text": "Profiles\n\nviewof sel_dept = Inputs.select([''].concat(\n  tidy(\n    pdata.filter(d => d[\"dept_students_lab\"] !== null),\n    distinct('dept_students_lab')\n  ).map(d => d[\"dept_students_lab\"])), {label: \"Choose dept\"})\n\nviewof do_pct = Inputs.toggle({label: \"Show %\"}) \nviewof rm_nulls = Inputs.toggle({label: \"Remove nulls\", value: true})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDemographicsDev ProfilesTablesProfile DataQuestions\n\n\n\ncol_dems = [\"is_coder\", \"gender_binary\", \"year_born\", \n             \"ethnicity_binary\",  \"dept_students_binary\", \"academia_status\"]\n\nviewof by_attr = Inputs.radio(col_dems, {label: \"Attributes\"})\n\nhtml`<p class = 'q_styled'>Preferred Pronouns</p>`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_attr_v(filtered_dat, \"pref_pronouns\", by_attr, true,  do_pct)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Academia Status</p>`\n\n\n\n\n\n\n\nplot_attr_v(filtered_dat, \"academia_status\", by_attr, false, do_pct)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Departments</p>`\n\n\n\n\n\n\n\nplot_attr_v(filtered_dat, \"dept_students_lab\", by_attr, false, do_pct)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Year Born</p>`\n\n\n\n\n\n\n\nplot_yr_born(filtered_dat, by_attr)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Duration (sec)</p>`\n\n\n\n\n\n\n\nplot_duration(filtered_dat, by_attr)\n\n\n\n\n\n\n\n\n\n\nOnly showing coders.\n\nviewof by_attr2 = Inputs.radio([\"gender_binary\", \"year_born\", \"ethnicity_binary\", \"dept_students_binary\", \"academia_status\"], {label: \"Attributes\"})\n\n// Do you consider yourself to be a coder/programmer?\n\nplot_attr_v(filtered_coders, \"self_id_as_coder\", by_attr2, true, do_pct)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_attr_v(filtered_coders, \"what_os\", by_attr2, false, do_pct)\n\n\n\n\n\n\n\n\nFirst line code\n\nplot_attr_h(filtered_coders, \"first_line_code\", by_attr2, false, \"first line\")\n\n\n\n\n\n\nYears coding\n\nplot_attr_h(filtered_coders, \"years_coding\", by_attr2, false, \"years coding\")\n\n\n\n\n\n\n\n\n\nviewof form = Inputs.form({\n  extra_filter: Inputs.checkbox(['coder', 'female', 'stem', 'Non-stem'], {label: 'Only'}), \n  dep_var: Inputs.select(colnames, {value: 'self_id_as_coder', label: 'Dep var (DV)'}),\n  indep_var: Inputs.select(colnames, {value: 'gender_binary', label: 'Indep var (IV)'}), \n  do_pct_crosstab: Inputs.radio(['DV (→)', 'IV (↓)'], {label: \"Show %\", value: 'IV (↓)'})\n})\n\n\n\n\n\n\n\nfunction crosstab_filter(x) {\n    switch (x) {\n      case \"\":         return pdata; \n      case 'coder':    return pdata.filter(d => d.is_coder === 'coder'); \n      case 'female':   return pdata.filter(d => d.gender_binary === 'female'); \n      case 'stem':     return pdata.filter(d => d.dept_students_binary === 'STEM'); \n      case 'Non-stem': return pdata.filter(d => d.dept_students_binary === 'Non-STEM'); \n      case 'coder,female': \n        return pdata.filter(d => d.is_coder === 'coder' && d.gender_binary === 'female');\n      \n      case 'coder,stem': \n        return pdata.filter(d => d.is_coder === 'coder' && d.dept_students_binary === 'STEM');\n      \n      case 'female,stem': \n        return pdata.filter(d => d.gender_binary === 'female' && d.dept_students_binary === 'STEM');\n      \n      case 'female,Non-stem': \n        return pdata.filter(d => d.gender_binary === 'female' && d.dept_students_binary === 'Non-STEM');\n      \n      case 'coder,Non-stem': \n        return pdata.filter(d => d.is_coder === 'coder' && d.dept_students_binary === 'Non-STEM');\n      default:\n        return pdata.filter(d => d.is_coder === \"\")\n}}\n\ncrosstab_dat_filter1 = crosstab_filter(form.extra_filter.join(\",\"))\n\ncrosstab_dat_filter2 = {\n  if (rm_nulls) {\n      return crosstab_dat_filter1.filter(d => d[form.dep_var] !== null && d[form.indep_var] !== null) \n  } else {\n      return crosstab_dat_filter1\n  }\n}\n\ncrosstab_dat_long = tidy(\n    crosstab_dat_filter2, \n    select([form.indep_var, form.dep_var]),\n    count([form.indep_var, form.dep_var])\n)\n\ncols_ordered = [form.dep_var].concat(\n  tidy(crosstab_dat_long, distinct(form.indep_var), arrange(form.indep_var)\n  ).map(d => d[form.indep_var])\n)\n\ncrosstab_dat_w_pct = tidy(\n  crosstab_dat_long,\n  groupBy(form.do_pct_crosstab == 'DV (→)' ? form.dep_var : form.indep_var, [\n    mutateWithSummary({total: sum('n')})\n  ]),\n  mutate({ pct: d => (d.n / d.total)*100 }),\n  select(['-n', '-total'])\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThird variable: \nPercentage data\n\nInputs.table(tidy(\n    crosstab_dat_w_pct,\n    pivotWider({ namesFrom: form.indep_var, valuesFrom: 'pct' }),\n    arrange(form.dep_var)\n    ), { columns: cols_ordered })\n\n\n\n\n\n\nRaw data\n\nInputs.table(tidy(\n  crosstab_dat_long, \n  pivotWider({ namesFrom: form.indep_var, valuesFrom: 'n' }),\n  arrange(form.dep_var)\n  ), { columns: cols_ordered }\n)\n\n\n\n\n\n\n\nfunction notes_crosstab(x) {\n  switch (x) {\n        case \"self_id_as_coder ~ gender_binary\":         \n          return \"It seems that female less affirmatively  self-identify as coders, whereas male are less ambiguous about it. When adding `stem`, male respond more affirmatvely to the question, but note that we have still a small sample. Is this something that holds when controlling for experience?\"; \n        default:\n          return \"\"\n  }\n}\n\nnotes = notes_crosstab(`${form.dep_var} ~ ${form.indep_var}`)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes:\n\nThere are  respondents shown in the table.\nDoes this crosstable passes the rule of five? \n\n\n\n\nprofile_cols = ['academia_status', 'nb_advisors', 'dept_students_lab', \"pref_pronouns\", \n                \"year_born\", \"ethnicity_binary\", 'country_origin', 'us_state', 'reason_coding', 'how_did_you_learn_code','first_line_code', 'first_line_code_c','years_coding',  'freq_coding_proj', 'freq_coding_proj_c', 'freq_oss_proj', 'what_os', 'position_industry', 'use_lang']\n\nInputs.table(pdata, { columns: profile_cols })\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\nLabel\n\n\n\n\nWhat is your academic status?\nacademia_status\n\n\nDo you agree to the above terms?\nagree_term\n\n\nQ_RecaptchaScore\ncaptcha_score\n\n\nWhen using other people's code, how do you cite their library/repository?\ncite_code\n\n\nWhen using other people's data, do you cite their library/repository? How?\ncite_data\n\n\nHow likely you think that not knowing how to program will have an impact on future professional opportunities?\ncoding_on_future_opportunities\n\n\nIf you have any suggestions/comments about the survey, please share them below!\ncomments\n\n\nNow, thinking about the expectations in your field: would you say that the expected level of programming skills in your field is an important factor in your choice to pursue an academic career?\ncomp_skills_factors_pursue_academia\n\n\nHow important do you think your programming skills were for your career?\ncomp_skills_pro_benefits_p\n\n\nDo you think programming will bring you professional benefits in the future?\ncomp_skills_pro_benefits_s\n\n\nHow important do you think your programming skills are to prospective group members?\ncomp_skills_recruiting\n\n\nHow important programming skills are when hiring new group members? - Graduate students\ncomp_skills_recruiting_grad\n\n\nHow important programming skills are when hiring new group members? - Postdoctoral researchers\ncomp_skills_recruiting_postdoc\n\n\nHow important programming skills are when hiring new group members? - Undergraduate students\ncomp_skills_recruiting_undergrad\n\n\nList of Countries\ncountry_origin\n\n\nWhat is your department? Please select all that apply.\ndept_prof\n\n\nWhat is your program?\ndept_students\n\n\nDo you feel disadvantaged for not knowing how to code?\ndisadv_not_coding\n\n\nDistribution Channel\ndistribution_channel\n\n\nIf you wish to delete your survey entry write \"delete\" below\ndo_del\n\n\nDo you share your code online?\ndo_share_code_online\n\n\nDuration (in seconds)\nduration_sec\n\n\nWhat is your institutional email address?\nemail\n\n\nEnd Date\nend_survey\n\n\nDo you think you have enough institutional support (workshops, mentorships, online labs) to learn to program?\nenough_instit_support\n\n\nWhich categories best describe you?\nethnicity\n\n\nAt what age did you write your first line of code or program? (e.g., webpage, Hello World, Scratch project)\nfirst_line_code\n\n\nWhen you are working on projects (for school, work, fun) how often do you code?\nfreq_coding_proj\n\n\nHow much of the software that you use is open source? (please provide your best guess)\nfreq_oss_proj\n\n\nDo you feel you have friends, colleagues, or supervisors who can help with your coding issues?\nfriends_help\n\n\nHow did you first learn to code?\nhow_did_you_learn_code\n\n\nFinished\nis_finished\n\n\nOverall, would you like to have more time to improve your programming skills?\nmore_time_learning_to_code\n\n\nWhat is the name of your research group?\nname_research_group\n\n\nHow many advisors do you have?\nnb_advisors\n\n\nWhat percentage of your social contacts are likely to participate in a project that require programming in the upcoming academic year. Social contacts are classmates and other peers that you have communicated with at least briefly within the last month, either face-to-face, or otherwise. - Click to write Choice 1\npct_social_contacts_coding\n\n\nWhat benefits do you see in programming?\nperceived_benefits_coding\n\n\nWhich of the following describes industry positions you have held or currently hold?\nposition_industry\n\n\nWhat is your preferred pronouns?\npref_pronouns\n\n\nProgress\nprogress\n\n\nWhat qualities do you most value in software ?\nqualities_oss\n\n\nHow often do you read programming books, either to improve your long-term coding skills or out of interest?\nread_prog_book\n\n\nDo you code for any of the following reasons?\nreason_coding\n\n\nRecorded Date\nrecord_date\n\n\nResponse ID\nresponse_id\n\n\nResponse Type\nresponse_type\n\n\nScore\nscore\n\n\nFor any of your current projects, do you consider that you spent too much time to code? (e.g. data cleaning, data visualization, gathering data, and so on)?\nself_expect_time_coding\n\n\nDo you consider yourself to be a coder/programmer?\nself_id_as_coder\n\n\nStart Date\nstart_survey\n\n\nCoursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Cleaning code (debugging, refactoring, renaming variables, etc.)\ntime_cleaning_code\n\n\nCoursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Data cleaning (manually, e.g. using excel)\ntime_data_clean_gui\n\n\nCoursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Data cleaning (programmatically)\ntime_data_clean_prog\n\n\nCoursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Digital data collection (e.g. web scraping)\ntime_digital_data_coll\n\n\nCoursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Experimental manipulation\ntime_exp_manip\n\n\nCoursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Field data collection (e.g. interviews, surveys, questionnaires, observations, ethnographies, etc)\ntime_field_data_coll\n\n\nCoursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Grant writing\ntime_grant_writing\n\n\nCoursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Literature review\ntime_lit_review\n\n\nCoursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Meetings\ntime_meeting\n\n\nCoursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Writing thesis/paper\ntime_paper_writing\n\n\nCoursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Reading software-related content (blogs, books, forums, documentation)\ntime_read_doc\n\n\nDo you consider yourself a member of an underrepresented group?\nunderrep_group\n\n\n50 States, D.C. and Puerto Rico\nus_state\n\n\nWhich programming languages, scripting, and markup languages have you worked in over the past year?\nuse_lang\n\n\nUser Language\nuser_lang\n\n\nHow important do you think the following items are in academia? - Code associated with an article is easy to find online\nvalue_accessibility_paper_code\n\n\nWhen thinking about whether to use open source software, how important are the following: - Active development\nvalue_active\n\n\nWhen thinking about whether to use open source software, how important are the following: - A contributor's license agreement (CLA)\nvalue_cla\n\n\nWhen thinking about whether to use open source software, how important are the following: - A code of conduct\nvalue_coc\n\n\nCompared to your domain expertise, how valued do you think your coding skills are around you today?\nvalue_comp_skills_wrt_domain\n\n\nWhen thinking about whether to use open source software, how important are the following: - A contributing guide\nvalue_contrib_guide\n\n\nOverall, how important do you think it is to learn programming in your academic field today?\nvalue_learn_code_in_field\n\n\nWhen thinking about whether to use open source software, how important are the following: - An open source license\nvalue_oss_license\n\n\nHow important do you think the following items are in academia? - Code associated with an article is citable.\nvalue_paper_code_citability\n\n\nWhen thinking about whether to use open source software, how important are the following: - Responsive maintainers\nvalue_responsive_maintainers\n\n\nHow important do you think the following items are in academia? - Sharing code associated with an academic paper\nvalue_share_code\n\n\nWhen thinking about whether to use open source software, how important are the following: - A welcoming community\nvalue_welcoming_community\n\n\nWhen thinking about whether to use open source software, how important are the following: - Widespread use\nvalue_widespread_use\n\n\nWhat is the primary operating system in which you work?\nwhat_os\n\n\nIf you want to learn to program, what's stopping you?\nwhy_not_coding\n\n\nIn what year were you born?\nyear_born\n\n\nHow many years have you been coding?\nyears_coding"
  },
  {
    "objectID": "posts/survey-programming/index.html#costs-benefits",
    "href": "posts/survey-programming/index.html#costs-benefits",
    "title": "Survey results (early release)",
    "section": "Costs & benefits",
    "text": "Costs & benefits\n\nviewof sel_dept_cb = Inputs.select([''].concat(\n  tidy(\n    pdata.filter(d => d[\"dept_students_lab\"] !== null),\n    distinct('dept_students_lab')\n  ).map(d => d[\"dept_students_lab\"])), { label: \"Choose dept\" })\n\nviewof by_attr_cb = Inputs.radio([\"gender_binary\", \"year_born\", \"ethnicity_binary\", \"dept_students_binary\", \"academia_status\"], { label: \"Attributes\" })\nviewof do_pct_2 =   Inputs.toggle({ label: \"Show %\" }) \nviewof show_labs =   Inputs.toggle({ label: \"Show Labels\", value: true }) \nviewof rm_nulls_2 = Inputs.toggle({ label: \"Remove nulls\", value: true })\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCostsBenefitsOSSData\n\n\n\nhtml`<p class = 'q_styled'>Do you think you have enough institutional support (workshops, mentorships, online labs) to learn to program?</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n  show_labs ? \"enough_instit_support\" : \"enough_instit_support_ord\",      \n  by_attr_cb, \n  true,  \n  do_pct_2\n)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Overall, would you like to have more time to improve your programming skills?</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders,   \n  show_labs ? \"more_time_learning_to_code\": \"more_time_learning_to_code_ord\", \n  by_attr_cb, \n  false, \n  do_pct_2\n)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>For any of your current projects, do you consider that you spent too much time to code? (e.g. data cleaning, data visualization, gathering data, and so on)?</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n show_labs ? \"self_expect_time_coding\" : \"self_expect_time_coding_ord\", \n    by_attr_cb, \n    false,  \n    do_pct_2\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nDepending on other variables, the answers to these questions can be interepreted both as relative costs or benefits, e.g. thinking that expected level of programming skills is important in your field is a cost when you are not skilled yet.\n\n\n\n\nhtml`<p class = 'q_styled'> Compared to your domain expertise, how valued do you think your coding skills are around you today?</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n  show_labs ? \"value_comp_skills_wrt_domain\" : \"value_comp_skills_wrt_domain_ord\", \n  by_attr_cb, \n  false, \n  do_pct_2)\n\n\n\n\n\n\n\n\n\nfunction plot_social_contacts(dat, by_attr) {\n  return Plot.plot({\n    grid: true,\n    x: {\n      label: \"Social contact coding (%)  →\",\n    },\n    y: { \n      label: do_pct_2 ? \" Frequency (%) ↑\" : \"# respondents ↑\", \n      percent: do_pct_2 ? true : false\n    },\n    marks: [\n      Plot.barY(\n        rm_nulls ? dat.filter(d => d[\"pct_social_contacts_coding\"] !== null) : dat, \n        Plot.groupX({ y: do_pct_2 ? \"proportion\" : \"count\" }, { \n            x: \"pct_social_contacts_coding\",  fill: d => by_attr ? d[by_attr] : \"grey\"\n          })\n      ),\n      Plot.ruleY([0])\n    ]\n  })\n}\n\nhtml`<p class = 'q_styled'> What percentage of your social contacts are likely to participate in a project that require programming in the upcoming academic year?</p>`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_social_contacts(coders, by_attr_cb)\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Do you share your code online?</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \"do_share_code_online\", by_attr_cb, true,  do_pct_2)\n\n\n\n\n\n\n\nhtml`<h4>How important do you think the following items are in academia?</h4>`\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Code associated with an article is citable.</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n  show_labs ? \"value_paper_code_citability\" : \"value_paper_code_citability_ord\", \n  by_attr_cb, false, do_pct_2)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Code associated with an article is easy to find online</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n  show_labs ? \"value_accessibility_paper_code\" : \"value_accessibility_paper_code_ord\", \n  by_attr_cb, false, do_pct_2)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Sharing code associated with an academic paper</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n  show_labs ? \"value_share_code\" : \"value_share_code_ord\", \n  by_attr_cb, false, do_pct_2)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>A welcoming community</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n  show_labs ? \"value_welcoming_community\" : \"value_welcoming_community_ord\", \n  by_attr_cb, false, do_pct_2)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Widespread use</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n  show_labs ? \"value_widespread_use\" : \"value_widespread_use_ord\", \n  by_attr_cb, false, do_pct_2)\n\n\n\n\n\n\n\n\n\nBox plots\n\nagg_oss = tidy(\n  coders, \n  select([startsWith(\"value\"), \"gender_binary\"]),\n  filter(d => d.gender_binary != null),\n  pivotLonger({\n    cols: ['-gender_binary'],\n    namesTo: \"catego\",\n    valuesTo: \"val\"\n  }),\n  filter(d => /_ord/.test(d.catego) && d.val != \"NA\" && d.val <= 5)\n)\n\nPlot.plot({\n  height: 400,\n  marginTop: 0,\n  marginLeft: 250,\n  x: { inset: 10, grid: true, label: \"attitude →\" },\n  y: { axis: null, inset: 2},\n  color: {legend: true},\n  fy: {\n    label: \"questions (m) →\",\n    reverse: true\n  },\n  facet: {\n    data: agg_oss,\n    y: \"catego\",\n    marginLeft: 250\n  },\n  marks: [\n    Plot.frame({stroke: \"#aaa\", strokeWidth: 0.5}),\n    Plot.boxX(agg_oss, {x: \"val\", y: \"gender_binary\", stroke: \"gender_binary\", r: 1})\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncost_cols = ['pct_social_contacts_coding', 'comp_skills_factors_pursue_academia',\n             'comp_skills_pro_benefits_s', 'enough_instit_support',\n             'friends_help', 'perceived_benefits_coding', 'first_adv_expect_time_coding',\n             'second_adv_expect_time_coding', 'reason_coding',  'value_comp_skills_wrt_domain', \n             'do_share_code_online', \"value_learn_code_in_field\"]\n\nInputs.table(coders, { columns: cost_cols })"
  },
  {
    "objectID": "posts/survey-programming/index.html#students-typical-week",
    "href": "posts/survey-programming/index.html#students-typical-week",
    "title": "Survey results (early release)",
    "section": "Students typical week",
    "text": "Students typical week\n\n\nagg_dat = tidy(\n  coders, \n  select([startsWith('time')]), \n  pivotLonger({\n    cols: [startsWith('time')],\n    namesTo: 'task',\n    valuesTo: 'val'\n  }),\n  count(['task', 'val']\n))\n\n\nPlot.plot({\n  width: 1000,\n  marginBottom: 100,\n  grid: true,\n  x: {\n    tickRotate: 45,\n    label: \"\",\n    domain: ['0 hour', '1-5 hours', '6-10 hours', '11-15 hours', null]\n  },\n  facet: {\n    data: agg_dat,\n    x: d => d.task.replace('time_', '')\n  },\n  marks: [\n    Plot.barY(\n      agg_dat,\n      {x: \"val\", y: \"n\", fill: d => d.val === null ? 'grey' : 'task'}\n    ),\n    Plot.ruleY([0])\n  ]\n})"
  },
  {
    "objectID": "posts/survey-programming/index.html#what-do-non-coders-think-about-coding",
    "href": "posts/survey-programming/index.html#what-do-non-coders-think-about-coding",
    "title": "Survey results (early release)",
    "section": "What do non-coders think about coding?",
    "text": "What do non-coders think about coding?\n\nviewof by_attr_nc = Inputs.radio([\"gender_binary\", \"year_born\", \"ethnicity_binary\", \"dept_students_binary\", \"academia_status\"], { label: \"Attributes\" })\nviewof do_pct_nc =   Inputs.toggle({ label: \"Show %\", value: true }) \nviewof show_labs_nc =   Inputs.toggle({ label: \"Show Labels\", value: true })\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Overall, how important do you think it is to learn programming in your academic field today?</p>`\n\n\n\n\n\n\n\nplot_attr_v(non_coders, \n  show_labs_nc ? \"value_learn_code_in_field\" : \"value_learn_code_in_field_ord\",\n  by_attr_nc, \n  true, \n  do_pct_nc)\n\n\n\n\n\n\n\n\n\nimport { tidy, select, count, complete, filter, fullSeq, sum, mutateWithSummary, groupBy, pivotWider, startsWith, pivotLonger, mutate, distinct, summarize, arrange, pull } from '@pbeshai/tidyjs'\n\n\n\n\n\n\n\nhtml`<style>\n.q_styled {\n        position: relative; \n        top: 15px;\n        font-size: 14px;\n        font-family: sans-serif;\n        text-align: center;\n        font-style: italic\n    }\n</style>\n`"
  },
  {
    "objectID": "posts/rise_of_comp2/index.html",
    "href": "posts/rise_of_comp2/index.html",
    "title": "The rise of computational works 2",
    "section": "",
    "text": "Warning\n\n\n\nIt takes a while for everything to load. Be patient.\n\n\n\nTODO\n\nsee project\n\n\n\nSpecter embeddings of computational papers (early)\n\nviewof type_cat = Inputs.select(['topic', 'field', 'year', 'selected'], {value: 'field', label: \"cluster type\"})\nviewof sel_field = Inputs.select([''].concat(Object.keys(lookup)), {label: \"Type field\"})\n// viewof sel_field = Inputs.select([''].concat(fields), {value: '', label: \"choose field\"})\n\n// viewof input_title = html`<input placeholder=\"Title.\">`\n\n// viewof c = rangeSlider(background_data, d => d.year)\n\nviewof year_min = Inputs.range([1950, 2020], {label: 'year min', value: 1950})\nviewof year_max = Inputs.range([1951, 2020], {label: 'year max', value: 2020})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof reset = html`<button>Reset`\n\nplot_embedding = () => {\n  const zoom = d3.zoom()\n      .scaleExtent([0.5, 32])\n      .on(\"zoom\", zoomed);\n\n  const svg = d3.create(\"svg\")\n      .attr(\"viewBox\", [0, 0, width, height]);\n\n  const gGrid = svg.append(\"g\");\n\n  // Background data\n\n  const gDotB = svg.append(\"g\")\n        .attr(\"fill\", \"none\")\n        .attr(\"class\", \"circles\")\n        .attr(\"stroke-linecap\", \"round\");\n\n  gDotB.selectAll(\"circle\")\n      .data(background_data)\n      .join(\"circle\")\n        .attr(\"cx\", d => x(d.x))\n        .attr(\"cy\", d => y(d.y))\n        .attr(\"fill\", 'grey')\n        .attr(\"opacity\", 0.3)\n        .attr(\"r\", d => r(d.citationCount));\n\n  // Yearly data\n\n  const gDot = svg.append(\"g\")\n      .attr(\"fill\", \"none\")\n      .attr(\"class\", \"circles\")\n      .attr(\"stroke-linecap\", \"round\");\n\n  gDot.append(\"style\").text(hover_css);\n\n  gDot.selectAll(\"circle\")\n    .data(yearly_data)\n    .join(\"circle\")\n      .attr(\"cx\", d => x(d.x))\n      .attr(\"cy\", d => y(d.y))\n      .attr(\"fill\", d => sel_field === \"\" ? z(d[type_cat]) : 'grey')\n      .attr(\"opacity\", d => sel_field === \"\" ? 0.5 : 0.3)\n      .attr(\"r\", d => r(d.citationCount))\n    .append(\"title\")\n      .text(d => `${d.title} (${d.field})\\n# citations: ${d.citationCount}\\nYear: ${d.year}\\ntopic: ${d.topic}`);\n  \n  // Field data\n\n  const gDotF = svg.append(\"g\")\n        .attr(\"fill\", \"none\")\n        .attr(\"class\", \"circles\")\n        .attr(\"stroke-linecap\", \"round\");\n\n  gDotF.append(\"style\").text(hover_css);\n\n  if (sel_field !== '') {\n    \n    gDotF.selectAll(\"circle\")\n      .data(data_by_field)\n      .join(\"circle\")\n        .attr(\"cx\", d => x(d.x))\n        .attr(\"cy\", d => y(d.y))\n        .attr(\"fill\", d => sel_field === \"\" ? 'black' : 'red')\n        .attr(\"opacity\", d => sel_field === \"\" ? 0 : 0.4)\n        .attr(\"r\", d => r(d.citationCount))\n      .append(\"title\")\n        .text(d => `${d.title} (${d.field})\\n# citations: ${d.citationCount}\\nYear: ${d.year}\\ntopic: ${d.topic}`);\n  }\n  \n  const gx = svg.append(\"g\");\n\n  const gy = svg.append(\"g\");\n\n  svg.call(zoom).call(zoom.transform, d3.zoomIdentity);\n\n  function zoomed({transform}) {\n    const zx = transform.rescaleX(x).interpolate(d3.interpolateRound);\n    const zy = transform.rescaleY(y).interpolate(d3.interpolateRound);\n    gDot.attr(\"transform\", transform).attr(\"stroke-width\", 2 / transform.k);\n    gDotF.attr(\"transform\", transform).attr(\"stroke-width\", 2 / transform.k);\n    gDotB.attr(\"transform\", transform).attr(\"stroke-width\", 2 / transform.k);\n    gx.call(xAxis, zx);\n    gy.call(yAxis, zy);\n    gGrid.call(grid, zx, zy);\n  }\n\n  return Object.assign(svg.node(), {\n    reset() {\n      svg.transition()\n          .duration(750)\n          .call(zoom.transform, d3.zoomIdentity);\n    }\n  });\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nchart = plot_embedding()\n\n\n\n\n\n\n\n\nreset, chart.reset()\n\n// embedding_plot = {\n \n//   const background_mark = (is_colored) => {  \n//             return { x: \"x\", y: \"y\", fill: is_colored ? type_cat : \"grey\",  opacity: 0.2, r: \"citationCount\" }\n//         }\n  \n//   const subset_mark = {\n//           x: \"x\", y: \"y\", fill: type_cat, r: \"citationCount\", \n//           opacity: subset === undefined ? 0.3 : 1, \n//           title: d => `${d.title} (${d.field})\\n# citations: ${d.citationCount}\\nYear: ${d.year}\\ntopic: ${d.topic}`\n//     }\n\n//   const field_mark = {\n//           x: \"x\", y: \"y\", opacity: 0.3, r: \"citationCount\",\n//           fill: type_cat === 'field' ? 'red' : type_cat,\n//           title: d => `${d.title} (${d.field})\\n# citations: ${d.citationCount}\\nYear: ${d.year}\\ntopic: ${d.topic}`\n//         }\n  \n//   const year_mark = {\n//           x: \"x\", y: \"y\", opacity: 0.8, r: \"citationCount\", \n//           fill: type_cat, \n//           title: d => `${d.title} (${d.field})\\n# citations: ${d.citationCount}\\nYear: ${d.year}\\ntopic: ${d.topic}`\n//         }\n  \n//   const paperIds_mark = {\n//           x: \"x\", y: \"y\", opacity: 1, r: \"citationCount\", fill: 'lab',\n//           title: d => `${d.title} (${d.field})\\n# citations: ${d.citationCount}\\nYear: ${d.year}\\ntopic: ${d.topic}\\nlab: ${d.lab}`\n//         }\n\n//   const background_plot = () => Plot.plot({\n//       inset: 8, height: height, width: width,\n//       y: { label: \"\", ticks: null },\n//       x: { label: \"\", ticks: null },\n//       r: { range: [1.5, 10] },\n//       color: { type: \"categorical\", scheme: \"paired\" },\n//       marks: [  Plot.dot(background_data,  background_mark(true)  ) ]\n//     })\n  \n//   const subset_plot = () => Plot.plot({\n//       inset: 8, height: height,  width: width,\n//       y: { label: \"\", ticks: null },\n//       x: { label: \"\", ticks: null },\n//       r: { range: [1.5, 10] },\n//       color: { type: \"categorical\", scheme: \"paired\" },\n//       marks: [\n//         Plot.dot(background_data,  background_mark(false)  ),\n//         Plot.dot(subset, subset_mark )\n//         ]\n//     })\n  \n//   const field_plot = () => Plot.plot({\n//       inset: 8, height: height, width: width,\n//       y: { label: \"\", ticks: null },\n//       x: { label: \"\", ticks: null },\n//       r: { range: [1.5, 10] },\n//       color: { type: \"categorical\", scheme: \"paired\" },\n//       marks: [\n//         Plot.dot(background_data,  background_mark(false)  ),\n//         Plot.dot(data_by_field, field_mark )\n//         ]\n//     })\n  \n//   const year_plot = () => Plot.plot({\n//       inset: 8, height: height, width: width,\n//       y: { label: \"\", ticks: null },\n//       x: { label: \"\", ticks: null },\n//       r: { range: [1.5, 10] },\n//       marks: [\n//         Plot.dot(background_data,  background_mark(false)  ),\n//         Plot.dot(yearly_data, year_mark  )\n//         ]\n//     })  \n\n//   const paperIds_plot = () => Plot.plot({\n//       inset: 8, height: height, width: width,\n//       y: { label: \"\", ticks: null },\n//       x: { label: \"\", ticks: null },\n//       r: { range: [1.5, 10] },\n//       marks: [\n//         Plot.dot(background_data,  background_mark(false)  ),\n//         Plot.dot(paperIds_data, paperIds_mark)\n//         ]\n//     })  \n\n//   const subset_and_field_plot = () => Plot.plot({\n//       inset: 8, height: height, width: width,\n//       y: { label: \"\", ticks: null },\n//       x: { label: \"\", ticks: null },\n//       r: { range: [1.5, 10] },\n//       color: { type: \"categorical\", scheme: \"paired\" },\n//       marks: [\n//         Plot.dot(background_data,  background_mark(false)  ),\n//         Plot.dot(data_by_field, field_mark),\n//         Plot.dot(subset, subset_mark)\n//         ]\n//     })\n\n//   if (subset === undefined && sel_field[0] === undefined) {\n//     if (toggle) {\n//       return paperIds_plot()\n//     }\n//     return year_plot()\n//   } else if (subset !== undefined && sel_field[0] === undefined) {\n//     return subset_plot()\n//   } else if (subset === undefined && sel_field[0] !== undefined) {\n//     return field_plot()\n//   } else if (subset === undefined && sel_field[0] === undefined && type_cat === 'selected') {\n//     return paperIds_plot()\n//   } else {\n//     return subset_and_field_plot()\n//   } \n\n// }\n\n// type_cat === 'selected' ? svg`` : embedding_plot.legend(\"color\")\n\n\n\n\n\n\nThere are  paper from paperIds.\nNote that the topics here are custers of documents that are similar with respect to title, abstract, and citations. This is different from vanilla topics that cluster together exclusively based on content.\nBy adding document-level relatedness, we hope that we will be able to distinguish different facets of how the idea of computational is used in the literature. For instance, say that in philosophy there is much discussion about the computational theory of the mind. Philosophers won’t be citing the same papers than computational papers in political science that cite, say, some methodological papers on how best to construct embeddings in their field. If we are right, then conceptual papers should be fairly easy to distinguish and remove.\n\ntable\n\nInputs.table(yearly_data, {columns: ['topic', 'field', 'year', 'venue', 'citationCount', 'title']})\n\n\n\n\n\n\n\ndb = DuckDBClient.of({ \n    data: await FileAttachment(\"umap_embedding_2NCOMP.csv\").csv({typed:true}),\n  classifyComp: await FileAttachment(\"classify-comp-proj-tidy.csv\").csv({typed:true})\n})\n\nbackground_data = db.sql`\n  SELECT * \n  FROM data  \n  USING SAMPLE 10000\n`\n\ndata_by_field = db.sql`\n  SELECT * \n  FROM data \n  WHERE (field = ${sel_field} AND year >= ${year_min} AND year <= ${year_max} AND citationCount > 0)\n`\n\nyearly_data = db.sql`\n  SELECT * \n  FROM data \n  WHERE (year >= ${year_min} AND year <= ${year_max} AND citationCount > 0)\n  USING SAMPLE 20000\n`\n\n// paperIds_data = db.query(\n//     `SELECT * \n//      FROM data \n//      INNER JOIN classifyComp ON data.paperId = classifyComp.paper_id;\n// `)\n\ntable_data = db.sql`\n  SELECT year, topic, COUNT()::INT as n\n  FROM data \n  WHERE (year >= ${year_min} AND year <= ${year_max})\n  GROUP BY\n    year, topic\n`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLimitations\n\nThe background data is a sample of 20K documents (out of 156K).\nWhen searching a title, we look at all the data (not the 20K subset)\nWe overlay filtered data on top of the background data, which is also a subset of 20K.\n\n\n\n“Degrees of freedom”\n\nWith hdbscan, we can reduce the number of topics so that we have coarser grain topics.\nWith umap, we can choose the number of dimensions to reduce\n\n\nimport {Plot} from \"@mkfreeman/plot-tooltip\"\n\n\n\n\n\n\n\nfields = ['Art', 'Biology', 'Business', 'Chemistry', 'Computer-Science', 'Education', 'Economics', 'Environmental-Science', 'Geography', 'Geology', 'History', 'Law', 'Linguistics', 'Mathematics', 'Philosophy', 'Physics', 'Political-Science', 'Psychology', 'Sociology', ]\n\nlookup = {\n  const out = {}\n  for (let i=0; i < fields.length; i++) {\n    out[fields[i]] = i\n  }\n  return out\n}\n\nhover_css = `\n    .circles {\n      stroke: transparent;\n      stroke-width: 0.1px;\n    }\n    .circles circle:hover {\n      stroke: black;\n    }\n  `\n\nheight = 600\nwidth = 1200\nk = height / width\nmargin = ({top: 12, right: 10, bottom: 26, left: 26})\n\nxMin = d3.min(yearly_data, d=>d.x)\nxMax = d3.max(yearly_data, d=>d.x)\nyMin = d3.min(yearly_data, d=>d.y)\nyMax = d3.max(yearly_data, d=>d.y)\n\nextent_x = d3.extent(yearly_data, d => d.x)\nextent_y = d3.extent(yearly_data, d => d.y)\n\nx = d3.scaleLinear()\n  .domain([extent_x[0]-3, extent_x[1]+3])\n  .range([0, width])\n\ny = d3.scaleLinear()\n  .domain([(extent_y[0] * k)-3, (extent_y[1] * k)+3])\n  .range([height, 0])\n\nr = d3.scaleLog()\n  .domain(d3.extent(yearly_data.map(d => d.citationCount)))\n  .range( [0.5, 1.3] )\n\nz = d3.scaleOrdinal()\n  .domain(yearly_data.map(d => d[type_cat]))\n  .range(d3.schemePaired)\n\nyAxis = (g, y) => g\n  .call(d3.axisRight(y).ticks(12 * k))\n  .call(g => g.select(\".domain\").attr(\"display\", \"none\"))\n\nxAxis = (g, x) => g\n    .attr(\"transform\", `translate(0,${height})`)\n\ngrid = (g, x, y) => g\n    .attr(\"stroke\", \"currentColor\")\n    .attr(\"stroke-opacity\", 0.1)\n    .call(g => g\n      .selectAll(\".x\")\n      .data(x.ticks(12))\n      .join(\n        enter => enter.append(\"line\").attr(\"class\", \"x\").attr(\"y2\", height),\n        update => update,\n        exit => exit.remove()\n      )\n        .attr(\"x1\", d => 0.5 + x(d))\n        .attr(\"x2\", d => 0.5 + x(d)))\n    .call(g => g\n      .selectAll(\".y\")\n      .data(y.ticks(12 * k))\n      .join(\n        enter => enter.append(\"line\").attr(\"class\", \"y\").attr(\"x2\", width),\n        update => update,\n        exit => exit.remove()\n      )\n        .attr(\"y1\", d => 0.5 + y(d))\n        .attr(\"y2\", d => 0.5 + y(d)));"
  },
  {
    "objectID": "posts/s2orc_viz/index.html",
    "href": "posts/s2orc_viz/index.html",
    "title": "S2ORC Viz",
    "section": "",
    "text": "Abstracts and pdf parsed shows the total number of papers and the number of papers for which either the abstract or pdf is parsed.\ns2fos v. mag is a showdown between the semantic scholar classification scheme and the microsoft academic graph one. Papers are grouped by fields and year, and we keep track of the parsing extent for each.\n\n\nimport {addTooltips} from \"@mkfreeman/plot-tooltip\"\n\n\n\n\n\n\n\ndata = FileAttachment(\"count_field_and_decade.csv\").csv({ typed: true })\n\n\n\n\n\n\n\nviewof schema = Inputs.select([\"s2fos\", \"mag\", \"both\"], { label: \"Schema\" })\nviewof show_pct = Inputs.toggle({label: \"Normalize\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbstracts and pdf parseds2fos v. magraw data\n\n\n\n\np1 = Plot.plot({\n    marginLeft: 100,\n    height: 600,\n    width: 1200,\n    marginBottom: 100,\n    x: { label: null, tickRotate: 45 },\n    y: { \n      grid: true, \n      percent: show_pct ? true : false, \n      label: show_pct ? \"↑ rep. (%)\" : \"↑ n papers\"\n     },\n    marks: [\n      Plot.barY(data.filter(d => d.year < 2020), Plot.groupX({y: \"sum\"}, { x: \"year\", y: d => schema == 'both' ? d[\"n\"] : d[`n_${schema}`], fill: \"parsing\", order: \"sum\", offset: show_pct ? \"expand\" : null }))\n    ]\n})\np1.legend(\"color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof mark_type = Inputs.select([\"stacked\", \"line\"], { label: \"Plot type\" })\nviewof parsing = Inputs.select([\"abstract\", \"pdf\", \"all\"], { label: \"Parsing extent\" })\nviewof chosen_group = Inputs.select([\"STEM\", \"Social Science\", \"Misc\"], { label: \"Group\" })\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata_f = data.filter(d => d.group == chosen_group && d.year < 2020 && d.parsing == parsing)\n\nline_plot = () => {\n  const xy_norm = Plot.normalizeY({\n          x: \"year\",  y:  d => schema == 'both' ? d[\"n\"] : d[`n_${schema}`], stroke: \"field\", title: \"field\", basis: \"first\"\n      })\n  const xy = {\n          x: \"year\",  y:  d => schema == 'both' ? d[\"n\"] : d[`n_${schema}`], stroke: \"field\", title: \"field\"\n      }\n  return addTooltips(Plot.plot({\n      marginLeft: 100,\n      height: 600,\n      width: 1200,\n      marginBottom: 50,\n      x: { label: null, tickRotate: 45 },\n      y: { \n        percent: show_pct ? true : false, \n        grid: true, \n        label: show_pct ? \"↑ Norm. over first value, showing relative growth\" : \"↑ n papers\" },\n      marks: [\n        Plot.line(data_f, show_pct ? xy_norm : xy)\n      ]\n  }))\n}\n\nstack_plot = () => {\n  const xy = { x: \"year\", y: d => schema == 'both' ? d[\"n\"] : d[`n_${schema}`], z: \"field\", order: \"sum\",  offset: show_pct ? \"expand\" : null }\n\n  return addTooltips(Plot.plot({\n        marginLeft: 100,\n        height: 600,\n        width: 1200,\n        marginBottom: 50,\n        x: { label: null, tickRotate: 45 },\n        y: { \n          percent: show_pct ? true : false, \n          grid: true, \n          label: show_pct ? \"↑ rep. (%)\" : \"↑ tot papers\" },\n        marks: [\n          Plot.barY(data_f, Plot.stackY({...xy, fill: \"field\", title: d => `${d.field} (${d.n})` })),\n        ]\n    }))\n}\n\n\np3 = mark_type == 'stacked' ? stack_plot() : line_plot()\n\np3.legend(\"color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInputs.table(data)\n\n\n\n\n\n\n\n\n\n\nRemarks\n\nBiology is an encompassing field. I wish we could divide it further.\nIn my opinion the biggest difference beween magand s2fos schema is that the later track the emergerce of computer science, which is nice.\n\n\n\nNotes\nClassifying articles into fields is non-trivial. But at least there is work on the topic.\nFor a while, the microsoft academic graph (mag) was standard. Many researchers used the top mag fields of study as their main taxonomy. Now that the mag project at microsoft is deprecated (Dec. 31, 2021.; see here), AllenAI’s semantic scholar is arguably one of the best contender to become the next standard. Contrary to MAG, they provide a unified api for the citation graphs of papers and they released a fraction of their overall paper nodes as parsed text. Announced after mag deprecation, the semantic scholar databse did some more work on the classification scheme, which can be found here. This is what we use and compare here, as did other before us.\nWe note that all the current schemes agree that papers have overlapping fields and are hierarchical. This is something that the mag field made explicit through the use of a hierarhical algorithm for unsupervised topic classification. The s2_fos scheme drops the hierarchy and focus on the top fields using a (simpler) classifier."
  },
  {
    "objectID": "posts/rise_of_comp3/index.html",
    "href": "posts/rise_of_comp3/index.html",
    "title": "The rise of computational works 3",
    "section": "",
    "text": "import {Plot} from \"@mkfreeman/plot-tooltip\""
  },
  {
    "objectID": "posts/rise_of_comp3/index.html#rise-of-programming",
    "href": "posts/rise_of_comp3/index.html#rise-of-programming",
    "title": "The rise of computational works 3",
    "section": "Rise of programming?",
    "text": "Rise of programming?\n\n\n\nviewof select = Inputs.select(fields, { multiple: true, value: ['linguistics', 'philosophy', 'history'] })\nviewof show_pct = Inputs.toggle({label: \"normalize\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata_f = which(rin(data.map(d => d.field), select)).map(i => data[i]).filter(d => d.year > 1970)\n\nline_plot = () => {\n  \n  const xy_norm = {\n          x: \"year\",  y:  'pct_comp', stroke: \"field\", title: \"field\"\n      }\n    \n  const xy = {\n          x: \"year\",  y:  'n_comp', stroke: \"field\", title: \"field\"\n      }\n\n  return Plot.plot({\n      marginLeft: 50,\n      height: 600,\n      width: 800,\n      marginBottom: 50,\n      x: { label: null, tickRotate: 45 },\n      y: { \n        percent: show_pct ? true : false, \n        grid: true, \n        label: show_pct ? \"↑ %\" : \"↑ n papers\" },\n      marks: [\n        Plot.line(data_f, show_pct ? xy_norm : xy)\n      ]\n  })\n}\n\nlp = line_plot()\n\nlp.legend(\"color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata = FileAttachment(\"comp_normalized.csv\").csv({ typed: true })\n\nfields = ['mathematics', 'geology', 'environmental science', 'art','history', 'linguistics', 'psychology',  'education', 'geography', 'physics', 'law', 'sociology', 'economics', 'political science', 'philosophy']\n\nfunction rin(arr1, arr2) {\n  return Array.from(arr1, (x) => {\n    return arr2.indexOf(x) == -1 ? false : true\n  })\n}\n\nfunction which(x) {\n  return x.reduce(\n      (out, bool, index) => bool ? out.concat(index) : out, \n      []\n    )\n}"
  },
  {
    "objectID": "posts/rise_of_comp/index.html",
    "href": "posts/rise_of_comp/index.html",
    "title": "The rise of computational works 1",
    "section": "",
    "text": "These days there’s a computational version of everything. Computational biology, computational musicology,computational archaeology,and so on, ad infinitum. Even movies are going digital. Dan Jurafsky, 2006\n\nAlready in 2006, there was a feeling that science was becoming more and more computational. But the rise of computational works is not evenly distributed. While some fields have a long history of using computer science to meet their computational needs, others are concerned that the process of representing their objects of study in a digital format may distort them. Yet others think that programming, say in the humanities, is mostly about acquiring marketable skills that transfer to industry, and as such it goes against long-term values of humanists.\nUnlike previous technologies, programming is able to creep into any field as it permeates many facets of scientific work, e.g. statistics, communication, visualization, simulation, data collection, etc. While it is true that some objects of study are more challenging to represent on computers, the rate at which computational methods are nevertheless being adopted by computational enthusiasts could create a wedge between them and more reluctant fields. This imbalance might even lead to a situation where the adoption of computational tools gives certain individuals or groups a disproportionate amount of influence, especially in fields undergoing a significant shift towards computational works. For example, there is evidence that institutions with more resources to fund larger research groups already increase disproportionately faculty productivity. What if institutions with a greater labor advantage also benefit the most from this computational turn. Those who embrace computational tools may have an advantage in terms of visibility and funding, potentially allowing them to more easily disseminate their ideas.\nA strong disparity in methods might have important consequences on the evolution of ideas in science. We ask ourselves, is the rise of computational work is a source of epistemic inequality? Does it benefit groups already favored because of their greater labor resources and institutional prestige? If the cost of learning programming is low enough for some individuals, and those individuals cluster together, will we see a gap forming even within the field? Will field of studies that are more computational spill over into other fields?\nTo assess whether the rise of computational work has an effect on the evolution of ideas in science, we need to be able to identify computational works. Without this first step, we won’t be able to measure the effects of groups adopting computational methods, or how labor advantages interact with the rise of programming. There are surprisingly few studies quantifying the relative adoption of computational works in different fields. Recent advances in the availability of large-scale data and NLP tools make this possibility more accessible than ever before.\nWe define computational works as projects that seek to understand complex systems using visualizations, simulations, and/or inference processes that require computers and programming languages. Here a few examples, slightly biased from my experience, in chronological order:\n\nplot = Plot.plot({\n  y: {ticks: null},\n  width,\n  marks: [\n    Plot.text(\n      data_manual,\n      Plot.dodgeY({\n        x: \"date\",\n        text: \"author\",\n        title: d => `${d.author} (${d.date.getFullYear()}):\\n${d.desc.slice(0, 130).concat('\\n', d.desc.slice(130))}`,\n        r: 30,\n        fill: \"category\",\n        lineWidth: 7,\n        anchor: \"middle\"\n      })\n    ),\n    Plot.ruleY([0], {dy: 175}),\n    Plot.ruleX(d3.range(1960,2020,5), {x:d => (new Date(`${d}-01-01`)), opacity: 0.3, lineType: 'dashed'})\n  ]\n})\n\nplot.legend(\"color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll of these papers share the fact that they could not have been done without extensive computers and related computer skills. By extensive, I mean that these papers wouldn’t be doable without computers and programming skills because it would require too much time or effort.1 In cases where computer science and mathematics have an application, or are used in a field, it typically requires skills that go beyond a traditional curriculum. For example, Burrows’ analysis of Jane Austen’s text required skills that go beyond the literary studies curriculum.\nNote that some articles are computational in the sense that they deal with computational stuff, but they remain conceptual. We include them because papers with computational methods ought to cite them.\nOk, so how are we gonna classify computational works? Here is the plan schema:\n\nFirst, we look at articles containing the computational keyword, which will include variations of it because s2_search is a kind of elastic search. This is imperfect but it should be better than asking for digital or programming. Then, we project these paper embeddings into lower dimensions with umap. Finally, we use hdbscan, a density-based clustering algorithm, to cluster similar papers.\nThat’s it. The hope is that because document embeddings are informed by title, abstract, and citations, papers that are more computational ought to cluster together. We will evaluate the performance of our pipeline on the subset of papers that we manually identified as computational while reviewing the literature.\nWe already did some of that, which you can find here.\n\n\n\n\n\n\nNotes on the pipeline:\n\n\n\n\nWe don’t use full texts at the moment, but we could.\nWe place a lot of hope on Spectre embeddings, as we expect to distinguish between types of computational papers based on citation patterns.\nWe’re going to have a lot of false positives and negatives as a result of the first step. This should be improved.\n\n\n\n\nRefs for the methods:\n\nallenai/s2orc (paper)\nallenai/s2search (blog post)\nallenai/specter (paper)\nWe based our specter2top step on ddangelov/Top2Vec (paper)\n\numap\nhdbScan\n\n\n\ndata_manual = [\n  {'author': 'Lorenz', 'date': new Date('1963-01-01'), 'desc': 'Determining nonperiodic flow by way of numerical solution of convection equations', 'category': 'Numerical simulations'},\n  {'author': 'Gillepsie', 'date': new Date('1976-01-01'), 'desc': 'Simulating the stochastic time evolution of coupled chemical reactions using the Monte Carlo simulation procedure.', 'category': 'Numerical simulations'},\n  {'author': 'Busa', 'date': new Date('1980-01-01'), 'desc': ' Counting words from Thomas Aquinas body of work to better understand Aquinas his philosophical assumptions and truths.', 'category': 'Digital Humanities'},\n  {'author': 'Diaconis & Efron', 'date': new Date('1983-01-01'), 'desc': 'Use of the bootstrap to estimate the correlation coefficient from the data, thus replacing the traditional statistical procedures making assumptions of normality on the data.', 'category': 'Statistical Inference'},\n  {'author': 'Burrows', 'date': new Date('1989-01-01'), 'desc': ' A statistical analysis, or computer-assisted analysis, of literary texts using frequency analysis.', 'category': 'Digital humanities', 'field': 'Literature'},\n    {'author': 'Gelman & Rubin', 'date': new Date('1992-01-01'), 'desc': ' The use and pitfalls of using the Gibbs sampler to summarize multivariate distributions underlying data analysis.', 'category': 'Statistical Inference'},\n  {'author': 'Papert', 'date': new Date('1996-01-01'), 'desc': 'Computational thinking is a popular idea in education of how computers can be the basis of a new type of education in society and science.', 'category': 'Computational thinking', 'field': 'Education'},\n  {'author': 'Tisue & Wilenski', 'date': new Date('2004-01-01'), 'desc': 'Studying complex systems using multi-agent programming languages.', 'category': 'ABMs'},\n  {'author': 'Adamic & Glance', 'date': new Date('2005-01-01'), 'desc': 'Analyzing blog posts from political bloggers as a way to better understand the role of polarization between conservatives and liberals in the U.S Presidential Election of 2004.', 'category': 'Computational Social Science', 'field': 'Political Science'},\n  {'author': 'Kossinets & Watts', 'date': new Date('2006-01-01'), 'desc': 'Analyzing large social networks using tools from network theory.', 'category': 'Computational Social Science'},\n  {'author': 'Hall, Jurafsky & Manning', 'date': new Date('2008-01-01'), 'desc': 'Understanding the evolution of ideas in the field of computational linguistics using topic modeling on all of the ACL anthology', 'category': 'Computational Linguistics', 'field': 'SciSci'},\n  {'author': 'Michel et al.', 'date': new Date('2011-01-01'), 'desc': 'Google books.', 'category': 'Computational Social Science', incorporated: true},\n  {'author': 'Grimmer & Steward', 'date': new Date('2013-01-01'), 'desc': 'It is also an example of replicable work, where the code and data are available on the Harvard Dataverse.', 'category': 'Computational Social Science', 'field': 'Political Science', reproducible: true},\n  {'author': 'Recasens, Danescu-Niculescu-Mizil, & Jurafsky', 'date': new Date('2013-01-01'), 'desc': 'TODO', 'category': 'Computational Linguistics', incorporated: true},\n  {'author': 'Barbera', 'date': new Date('2015-01-01'), 'desc': 'It is also an example of replicable work, where the code and data are available on GITHUB.', 'category': 'Computational Social Science', 'field': 'Political Science', reproducible: true, github: true}\n  ]\n\n\n\n\n\n\n\nimport {Plot} from \"@mkfreeman/plot-tooltip\"\n\n\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\nThere was a time when women were the computers. The original Monte Carlo algorithm was calculated by them during WWII. The calculations were simple enough that it was technically possible for humans to perform them. The trajectory of the first Apollo mission was also computed by women. But as we get deeper into the second half of the 20th century, this is no longer the case↩︎"
  },
  {
    "objectID": "posts/interactive-stats/index.html",
    "href": "posts/interactive-stats/index.html",
    "title": "CDAE stats",
    "section": "",
    "text": "The equation:\n\\[n^* = \\frac{n_0}{1 + \\frac{n_0}{N}}\\]\nwhere \\(n_0 = z_\\alpha^2 \\frac{S^2}{D^2}\\), \\(S^2\\) is our population variance, \\(D^2\\) is the difference between the true value and the estimated value, and \\(z_\\alpha^2\\) is the \\(z\\) value at a given confidence interval.1\n\n\n\nWe know…\n\nThere are 5,000 nonprofits in the city of reference\nFrom a previous study, we know that the the mean value of using new tools is $3,000. We also know from previous studies that the s.d. of this is $3,500.\n\nWe want…\n\nAn error rate of 10%\nA confidence interval of 95%\n\n\n\nfunction calc_n_0(z_alpha_sq, S, D) {\n    return z_alpha_sq * (S**2 / D**2)\n}\n\nfunction effective_sample_size(z_alpha_sq, S, D, N) {\n    const n_0 = calc_n_0(z_alpha_sq, S, D)\n    return +(n_0 / (1 + (n_0 / N))).toFixed(1)\n}\n\nfunction ci2z(ci) {\n     if (ci === \".68\") {\n        return 1\n     } else if (ci === \".95\") {\n        return 2\n     } else if (ci === \".99\") {\n        return 3\n     }\n}\n\nviewof conf_int = Inputs.radio([\".68\", \".95\", \".99\"], {value: \".95\", label: \"Conf. interval\"})\nviewof error_rate = Inputs.range([0.05, 1], {value: 0.1, step: 0.05, label: \"Error rate\"})\nviewof N = Inputs.range([0, 100000], {value: 5000, step: 1000, label: \"N\"})\nviewof prev_mean = Inputs.range([0, 10000], {value: 3000, step: 500, label: \"Prev mean\"})\nviewof prev_std = Inputs.range([0, 10000], {value: 3500, step: 500, label: \"Prev std\"})\nz_alpha_sq = ci2z(conf_int)**2\n\nS = prev_std\nD = error_rate * prev_mean\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, \\(S^2\\) is the previous standard deviation squared and \\(D\\) is the wanted error rate times the previous mean, that is,  x  = .\nWe find that the minimum adequate sample size, or \\(n^*\\):\n\\(n^0\\) = \n\\(n^*\\) =  / (1 +  / ) = \n\\(n_0/N\\) = \nAlso, we saw in class that \\(n^*\\) converges around \\(600\\), with the default parameters. That is, adding more data does not entail a higher \\(n^*\\). You can observe that fact with the following plot:\n\nxs = [...Array(N).keys()];\nys = xs.map(x => effective_sample_size(z_alpha_sq, S, D, x))\nPlot.lineY(ys).plot({height: 200, width: 300, y: {label: \"↑ n*\"}, x: {label: \"N →\"}})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut you can play around with other settings to see how it varies."
  },
  {
    "objectID": "posts/interactive-stats/index.html#power-analysis",
    "href": "posts/interactive-stats/index.html#power-analysis",
    "title": "CDAE stats",
    "section": "Power analysis",
    "text": "Power analysis\nSee Patrick Mineault notebook"
  },
  {
    "objectID": "posts/interactive-stats/index.html#the-many-lives-of-statistical-tests",
    "href": "posts/interactive-stats/index.html#the-many-lives-of-statistical-tests",
    "title": "CDAE stats",
    "section": "The many lives of statistical tests",
    "text": "The many lives of statistical tests\nSometimes I feel that the popularity of statistical testing is about outsourcing statistical work of busy scientists to flow charts. In research methods courses focusing on statistical testing I feel there is an understanding that these are limited, but given time and interest of students, it’s better than nothing. And if you’re sticking to experimental setups, that might be all you need. I am not going to do a rant. But I want to supplement the usual search method class with alternative perspectives explained as simply as possible:\n\nThe Frequentist approach. This if often the first encounter with inferential statistics in social socience. As long as you are in an experimental set-up this might be fine. You need to think about probability as long-run probability.\nThe linear models approach. Instead of starting from statistical tests, we start from linear models and explain which models map onto which tests. This approach promotes flexibility at the costs of having to learn the underlying ideas of linear models.\nHypothesis testing but Bayesian. No need to remember the nonsense that we “fail to reject the null” and that 0.95 confidence interval does not mean that we are “95% confident that our results are significant”.\nThe Bootstrap approach. This is a great coding exercice and saves you time from remembering all the different tests.\n\nNote that we use the following emojis to encode data types:\n\n💡 : Yes/no, 2 levels, success/failure, bias/fair… nominal data.\n📊 : Yes/no/maybe, >2 levels, might be ordinal or nominal.\n📏 : continuous/scalar/uncountable data.\n\n\n💡 ~ 💡📊 ~ 💡📏 ~ 💡\n\n\n\nNHST\n\n\nR code\na <- chisq.test(d_mat) # p-value > 0.05\n\n\n\n\nLinear models\n\n\nR code\n# Using glm to do a log-linear model\nfull = glm(n ~ early_first_line * sex, family = poisson(), data = d_long) \nb = anova(full, test = 'Rao') #  similar to our two-way ANOVA\n\n\n\n\nSummary\n\n\n# A tibble: 2 × 2\n  p.value model     \n    <dbl> <chr>     \n1   0.927 chisq.test\n2   0.647 glm       \n\n\n\n\n\n\nNHST\n\n\nR code\na <- chisq.test(d_mat) # p-value > 0.05\n\n\n\n\nLinear models\n\n\nR code\nfull = glm(n ~ self_id_as_coder * sex, family = poisson(), data = d_long) # log-linear model\nb = anova(full, test = 'Rao') #  similar to our two-way ANOVA\n\n\n\n\nSummary\n\n\n# A tibble: 2 × 2\n  p.value model     \n    <dbl> <chr>     \n1  0.0235 chisq.test\n2  0.0235 glm       \n\n\n\n\n\n\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\nWarning: The `origin` argument of `stat_bin()` is deprecated as of ggplot2 2.1.0.\nℹ Please use the `boundary` argument instead.\n\n\n\n\n\n\n\n\n\nNHST\n\n\nLinear models\n\n\nSummary\n\n\n# A tibble: 3 × 5\n    p.value estimate conf.low conf.high model \n      <dbl>    <dbl>    <dbl>     <dbl> <chr> \n1 0.163        -17.9    -43.4      7.73 t.test\n2 0.0000119     NA       NA       NA    glm   \n3 0.163         NA       NA       NA    glm"
  },
  {
    "objectID": "posts/mongoDB/index.html",
    "href": "posts/mongoDB/index.html",
    "title": "How to MongoDB",
    "section": "",
    "text": "I currently use the vscode plug-in to interact with mongoDB. I like it b/c it is a middle between mongoDB Compass and the commandline.\nKey refs:"
  },
  {
    "objectID": "posts/mongoDB/index.html#basics",
    "href": "posts/mongoDB/index.html#basics",
    "title": "How to MongoDB",
    "section": "Basics",
    "text": "Basics\nConnect to papersDB\n\nmongoshpymongo\n\n\n[direct: mongos] test> use('papersDB');\n\n\nfrom pymongo import MongoClient\nuri = f\"mongodb://cwward:{pw}@wranglerdb01a.uvm.edu:27017/?authSource=admin&readPreference=primary&appname=MongoDB%20Compass&directConnection=true&ssl=false\"\nclient = MongoClient(uri)\ndb = client[\"papersDB\"]\n\n\n\nLooking at collections within our DB\n[direct: mongos] test> show collections;\nLook at already existing indexes\n[direct: mongos] papersDB> db.metadata.getIndexes()\nestimate count document based on year is super quick\n[direct: mongos] papersDB> db.metadata.estimatedDocumentCount()\nyou can ‘explain’ queries. Super useful to understand how mongoDB works and query performances.\n[direct: mongos] papersDB> var exp = db.metadata.explain(\"executionStats\")\n[direct: mongos] papersDB> exp.find({title: \"Scale-free networks are rare\"}) // executionTimeMillis: 14827; totalDocsExamined: 19,786,006\n[direct: mongos] papersDB> db.metadata.createIndex({year: -1}); // create index based on age\n[direct: mongos] papersDB> exp.find({title: \"Scale-free networks are rare\", year: 2018}).limit(1) // executionTimeMillis: 3851; totalKeysExamined: 786,497\ndropping indexes\n[direct: mongos] papersDB> db.metadata.dropIndex(\"year_-1\")"
  },
  {
    "objectID": "posts/mongoDB/index.html#useful-queries",
    "href": "posts/mongoDB/index.html#useful-queries",
    "title": "How to MongoDB",
    "section": "Useful queries",
    "text": "Useful queries\n1-find papers based on paper_id\n\nmongoshpython\n\n\n[direct: mongos] papersDB> db.pdf_parses.findOne({ paper_ID: \"77497072\"});\n\n\ndb.metadata.find_one({ \"paper_ID\": \"77497072\"})\n\n\n\n2-find papers based on paper_id and year\n[direct: mongos] papersDB> db.metadata.findOne({ year: {$gt: 2015, $lt: 2022}, paper_id: \"f1b4361a1978e93018c5fdfe4856250152676ffb\" })\n3-Query papers with body_text see stack overflow\n[direct: mongos] papersDB> db.pdf_parses.findOne({ body_text: { $gt: true, $type: 'array', $ne: [] }})\n4-Query authors in an array\n\nmongoshsql\n\n\n[direct: mongos] papersDB> db.metadata.findOne({ 'authors.0.first': \"Aaron\" })\n[direct: mongos] papersDB> db.metadata.findOne({ 'authors.first': \"Aaron\" })\n[direct: mongos] papersDB> db.metadata.findOne({ 'authors.last': \"Clauset\" })\n[direct: mongos] papersDB> db.metadata.aggregate({ $filter : { authors.last : { $eq : \"Clauset\" } } });\n\n\nSELECT * FROM metadata\nWHERE authors.last = \"Clauset\";"
  },
  {
    "objectID": "posts/mongoDB/index.html#creating-index",
    "href": "posts/mongoDB/index.html#creating-index",
    "title": "How to MongoDB",
    "section": "Creating index",
    "text": "Creating index\nCreate index based on descending year\n[direct: mongos] papersDB> db.metadata.createIndex({year: -1});\nFrom Percona, this allows to improve all the queries that find documents with a condition and the year field, like the following:\n[direct: mongos] papersDB> db.metadata.find( { year : 2018 } ) \n[direct: mongos] papersDB> db.metadata.find( { title : \"Scale-free networks are rare\", year : 2018 } )\n[direct: mongos] papersDB> db.metadata.find( { year : { $gt : 2020} } )\n[direct: mongos] papersDB> db.metadata.find().sort( { year: -1} ).limit(10)\nCreate index based on authors (Multikey indexes)\n[direct: mongos] papersDB> db.metadata.createIndex( { authors: 1 } )\n[direct: mongos] papersDB> db.metadata.find( { authors.last: \"Clauset\" } )\nCreate index based on year and has_body_text (include a Partial indexes and Unique) In order for the partial index to be used the queries must contain a condition on the year and body_text field.\n[direct: mongos] papersDB> db.metadata.createIndex(\n   { \"paper_id\": 1 },\n   { unique: true },\n   { partialFilterExpression: { year : { $gt: 2018 }, body_text: { $gt: true, $type: 'array', $ne: [] } } }\n)\n\n[direct: mongos] papersDB> db.metadata.find( { paper_id: \"77490322\", year: { $gt: 2018}, body_text: { $gt: true, $type: 'array', $ne: []} } )\nCreate index based on year (asc) and bounded by 1950-60\n[direct: mongos] papersDB> exp.find({\"year\": {$gte: 1950, $lte: 1960}, \"paper_id\": \"77490322\"}).limit(1) // executionTimeMillis: 360429; totalKeysExamined: 2024098\n[direct: mongos] papersDB> db.metadata.createIndex({year:1}, { partialFilterExpression: { year : { $gte: 1950, $lte: 1960 } } });\n[direct: mongos] papersDB> exp.find({\"year\": {$gte: 1950, $lte: 1960}, \"paper_id\": \"77490322\"}).limit(1) // executionTimeMillis: 68676; totalKeysExamined: 406162\nCreate index based partial filter expression\n\npython\n\n\n# We use \"$type\" because \"$ne\" not supported when creating PFE\ndb.metadata.create_index(\n [(\"year\", ASCENDING)], \n name=\"bucket 1950-1960\", \n partialFilterExpression={ \"year\" : { \"$gte\": 1950, \"$lte\": 1960 }, \"abstract\": {\"$type\": \"string\"} }\n)"
  },
  {
    "objectID": "posts/mongoDB/index.html#updating-documents",
    "href": "posts/mongoDB/index.html#updating-documents",
    "title": "How to MongoDB",
    "section": "Updating documents",
    "text": "Updating documents\n1- Update s2fos\ndb.metadata.updateOne({paper_id: '84881204', year: {$gte: 1950, $lte: 1960}}, {'$set': {'s2fos_field_of_study': ['Medicine']}}})\n2- Remove a field\ndb.metadata.updateOne({paper_id: '84881204', year: {$gte: 1950, $lte: 1960}}, {$unset: {s2fos: \"\"}})"
  },
  {
    "objectID": "posts/mongoDB/index.html#useful-aggregated-queries",
    "href": "posts/mongoDB/index.html#useful-aggregated-queries",
    "title": "How to MongoDB",
    "section": "Useful aggregated queries",
    "text": "Useful aggregated queries\nFind duplicated rows (not sure it is working yet)\n[direct: mongos] papersDB> const aggregation = [\n    {\"$group\" : { \"_id\": \"$paper_id\", \"count\": { \"$sum\": 1 } } },\n    {\"$match\": {\"_id\" :{ \"$ne\" : null } , \"count\" : {\"$gt\": 1} } }, \n    {\"$project\": {\"paper_id\" : \"$_id\", \"_id\" : 0} }\n]\n\n[direct: mongos] papersDB> db.pdf_parses.aggregate(aggregation);"
  },
  {
    "objectID": "posts/mongoDB/index.html#document-embeddings",
    "href": "posts/mongoDB/index.html#document-embeddings",
    "title": "How to MongoDB",
    "section": "Document embeddings",
    "text": "Document embeddings\nEmbed one collection into a second collection not sure it is working yet, this is a chatgpt answer\n[direct: mongos] papersDB> db.collection1.update({name: \"John Doe\"}, {$set: {address: db.collection2.findOne({address: \"123 Main St\"})}})"
  },
  {
    "objectID": "posts/allotax-in-quarto/index.html",
    "href": "posts/allotax-in-quarto/index.html",
    "title": "ALLotaxonometer",
    "section": "",
    "text": "viewof form = Inputs.form(\n  [\n    Inputs.select(d3.range(elem_names.length), {label: \"System 1\", multiple: true, multiple: 3, value: [0], format: x => elem_names[x]}),\n    Inputs.select(d3.range(elem_names.length), {label: \"System 2\", multiple: true, multiple: 3, value: [1], format: x => elem_names[x]}),\n  ],\n  {\n    template: (inputs) => htl.html`<div style=\"display: flex; gap: 4em\">\n  <br>${inputs}\n</div>`\n  }\n)\n\nviewof alpha = Inputs.range([0, alphas.length-1], {step: 1, label: \"α\", format: x => alphas[x]})\n\nnext_button()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`\n<br>\n<div style=\"display:flex; align-items:center; gap: 10em; margin-left: -75px; justify-content: center; width: 100%; text-align: center; font-size: 22px; \">\n      <div>${ tex`\\Omega_1` }: ${ title(0) }</div> \n      <div>${ tex`\\Omega_2` }: ${ title(1) }</div>  \n</div>\n<div style=\"display:flex; gap: 28em; margin-top: -100px; justify-content: center;\">\n    <div id=\"diamondplot\"></div>\n    <div style=\"margin-top: 110px;\" id=\"wordshift\"></div>\n</div>\n<div style=\"display:flex; align-items:center; gap: 14em;  margin-left:-95px;justify-content: center; margin-top:-300px\">\n    <div id=\"legend\"></div>\n    <div id=\"balance\"></div>\n</div>\n`\n\n\n\n\n\n\n\n\nall = import('https://cdn.skypack.dev/allotaxonometer@1.1.9?min')\n\nelem = FileAttachment(\"data/elem_girls.json\").json()\nelem_names = Object.keys(elem)\nmutable clicks = 0\nsel_sys1 = elem_names[(form[0][0]+clicks) % elem_names.length]\nsel_sys2 = elem_names[(form[1][0]+clicks) % elem_names.length]\nelems1 = elem[sel_sys1]\nelems2 = elem[sel_sys2]\n\nme_class = new all.mixedElems(elems1, elems2)\n\nrtd = me_class.RTD(alphas[alpha])\ndat = me_class.Diamond(rtd)\n\ndiamond_dat = dat.counts\nwordshift = me_class.wordShift(dat)\nbalance_dat = me_class.balanceDat() \n\np1=all.DiamondChart(diamond_dat);\np2=all.WordShiftChart(wordshift, { height: 670 });\np3=all.BalanceChart(balance_dat);\np4=all.LegendChart(diamond_dat);\n\ntitle = (sys) => {\n  const out = sys == 0 ? sel_sys1 : sel_sys2\n  return out.replace(/.((c|t)sv|json)/i, \"\")\n}\n\nalphas = d3.range(0,18).map(v => +(v/12).toFixed(2)).concat([1, 2, 5, Infinity])\n\nnot_zero = tex`\\propto \\sum_\\tau \\Big| \\frac{1}{r_{\\tau,1}^{ ${ alphas[alpha] }} } - \\frac{1}{r_{\\tau,2}^{ ${ alphas[alpha] }}} \\Big| `\n\nzero = tex`\\propto \\sum_\\tau \\Big|\\ln \\frac{r_{\\tau,1}}{r_{\\tau,2}}\\Big| `\n\ninfinity = tex`\\propto \\sum_\\tau (1 - \\delta_{r_{\\tau,1}, r_{\\tau,2}}) \\times\\max\\Big\\{\\frac{1}{r_{\\tau,1}},\\frac{1}{r_{\\tau,2}} \\Big\\} `\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport { button } from \"@bartok32/diy-inputs\"\n\nnext_button = () => button({\n  title: \"\",\n  value: \"Next\",\n  desc: \"\",\n  buttonStyle: {\n    background: \"#7295FF\",\n    color: \"white\"\n  },\n  onclick: (objs) => {\n    mutable clicks += 1;\n\n    d3.select(objs.button)\n      .style(\"background\", \"#6786E5\")\n      .interrupt()\n      .transition()\n      .duration(300)\n      .style(\"background\", \"#7295FF\");\n\n    if (mutable clicks > 0 && objs.output === \"\") {\n      objs.output = d3\n        .select(objs.div)\n        .insert(\"a\", \"div.desc\")\n        .attr(\"class\", \"output\")\n        .style(\"margin-left\", \"5px\")\n        .style(\"font-size\", \"11px\")\n        .style(\"cursor\", \"pointer\")\n        .style(\"border\", \"0.5px solid black\")\n        .style(\"border-radius\", \"5px\")\n        .style(\"padding\", \"5px\")\n        .on(\"click\", function () {\n          this.remove();\n          objs.output = \"\";\n          mutable clicks = 0;\n        })\n        .html(\"RESET\");\n    }\n  }\n})"
  },
  {
    "objectID": "unlisted_posts/complaints/index.html",
    "href": "unlisted_posts/complaints/index.html",
    "title": "Airlines Complaints TripAdvisors",
    "section": "",
    "text": "No matching items"
  }
]