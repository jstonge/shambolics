[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "shambolics",
    "section": "",
    "text": "CDAE stats\n\n\n\n\n\n\n\nCDAE\n\n\nInteractive\n\n\n\n\nStatistics is hard. Computational statistics makes it a bit better.\n\n\n\n\n\n\nMay 19, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nS2ORC Viz\n\n\n\n\n\n\n\nVisualization\n\n\nNLP\n\n\nS2ORC\n\n\nInteractive\n\n\n\n\nI always forget about the s2orc database details. Lets have them here.\n\n\n\n\n\n\nMay 19, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nSurvey results (early release)\n\n\nPreliminary results from a survey on the cost and benefits of learning to code\n\n\n\n\nSurvey\n\n\nProgramming\n\n\n\n\nThe costs and benefits of learning to code in science vary across individuals and disciplines. We should have a better idea of these tradeoffs before selling anyone on coding.\n\n\n\n\n\n\nMay 19, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nThe rise of computational works 1\n\n\n\n\n\n\n\nVisualization\n\n\nNLP\n\n\nS2ORC\n\n\nInteractive\n\n\nSciSci\n\n\n\n\nWhere, how, and when did computational stuff became popular in science?\n\n\n\n\n\n\nMay 19, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nThe rise of computational works 2\n\n\n\n\n\n\n\nVisualization\n\n\nNLP\n\n\nS2ORC\n\n\nInteractive\n\n\nSciSci\n\n\n\n\nA new hope.\n\n\n\n\n\n\nMay 19, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nThe rise of computational works 3\n\n\n\n\n\n\n\nVisualization\n\n\nNLP\n\n\nS2ORC\n\n\nInteractive\n\n\nSciSci\n\n\n\n\nThe computational works strike back\n\n\n\n\n\n\nMay 19, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nHow to MongoDB\n\n\n\n\n\n\n\nref\n\n\nmongoDB\n\n\n\n\nDatabase, database, database,…\n\n\n\n\n\n\nMay 19, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\n  \n\n\n\n\nData Statements Topics\n\n\n\n\n\n\n\nVisualization\n\n\nNLP\n\n\nS2ORC\n\n\n\n\nCan we find data availability statements?\n\n\n\n\n\n\nMay 19, 2023\n\n\nJonathan St-Onge\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/charting_a_phd/index.html",
    "href": "posts/charting_a_phd/index.html",
    "title": "Charting a PhD",
    "section": "",
    "text": "Summary\nAs computing becomes more affordable, data more accessible, and the Web continues to grow, a growing number of research groups are shifting towards computational approaches. Computational approaches might be attractive to many groups, as they might lead to new kinds of inquiries, and/or draw more attention and funding over traditional methods. The foundation of many computational methods is rooted in (open source) computer programming. On the one hand, more (early-career) researchers face the challenge of learning to code without the proper computational environment to support them, while on the other hand, groups may encourage or implicitly exert pressure to their members to learn to code, without realizing the personal costs involved. We investigate this growing tension between groups and individuals when learning to code, as well as the factors that drive the relative cost and benefits of learning to code at the individual and institional level.\nWe construct a theoretical model to investigate the tension between costs and benefits at both the individual and group level. We aim to develop a model in which we assess whether the prohibitive costs for individuals can be justified from the perspective of the group. Consider, for instance, a situation in the humanities where a lack of computational infrastructure results in a high cost associated with learning to code. This could exacerbate the retention rate of certain individuals in academia, but from the perspective of the group, the cost would be lower as only one individual successfully learning to code could bring long-term benefits to the group.\nWe corroborate our theoretical models with an empirical results. To estimate the relative cost-benefits of learning to code, we conduct a survey on the motivations, challenges, and institutional support for students who undertake a turn in programming at the University of Vermont. This qualitative is complemented by a bibliometric study in which we quantify the impact of the computational transition of research groups on scientific productivity, work reproducibility, and group diversity. This empirical inquiry let us highlight how the cost and benefits of learning to code depend on multiple levels of organization, that is, how specific individuals are integrated into groups and organizations. To do so, we must identify which groups experience a computational transition. We are in the process of building a database that comprise over 200 million papers and their associated metadata, including citations, authors, venues, and institutions, in order to track the shift towards computational practices. The size and richness of our database is not only relevant to us, but to other research groups interested in studying data luminosity, scientific feuds, or the rise of open source programming practices in science.\n\n\nConstellation of ideas from different fields I care about:\n\norange: science of science/bibliometry/sociology of science\nred: open source software (OSS) in science\ngreen: computer and internet history\nblue: OSS studies\n\n\n\n\n\n\n\n1. Computational science is already here, it’s just not evenly distributed.\n\n\n\n\n\nThis is a feeling that many people share, but I did not see many quantitative study about it. We know that the spread of computational works is uneven. Physics, chemistry, materials science and perhaps parts of ecology are at the forefront, while social sciences and humanities have only recently begun to embrace computational works (see [[wing_computational_2006]], [[horn_pragmatics_2006]], [[lazer_social_2009]], [[berry_computational_2011]]).\n\n\n\n\n\n\n\n\n\n2. Prestigious institions have greater labor advantage, leading to larger research group size, which lead to greater productivity\n\n\n\n\n\nscisci/prestige: see [[zhang_labor_2022]]\n\n\n\n\n\n\n\n\n\n3. Women are met with systemic challenges at research university\n\n\n\n\n\nscisci/gender: For instance, we can think of balancing personal life and productivity, more generally especially when it comes to parenting and maternity leaves ( [[cole_productivity_1984]], [[morgan_unequal_2021]]). There are alsosystemic inequalities that remain with respect to citation behaviors ([[lariviere_bibliometrics_2013]]).\n\n\n\n\n\n\n\n\n\n4. Computational thinking education has not caught up in fields traditionally unrelated to computing\n\n\n\n\n\nscisci/training: see [[butcher_persistent_2007]], [[anderson_student_2016]], [[touchon_mismatch_2016]]1\n\n\n\n\n\n\n\n\n\n5. Many people are calling for improved coding practices to make code more reproducible and transparent.\n\n\n\n\n\noss-in-sci: Computational education is no longer limited to techniques. This means that people should not learn the basics of coding but also have a more extensive knowledge of different software engineering tools ([[prlic_ten_2012]], [[wilson_good_2017]],[[minocher_estimating_2021]], [[culina_low_2020]], [[trisovic_large-scale_2022]]).\n\n\n\n\n\n\n\n\n\n6. Universities in general, and the humanities in particular, are confronted with the growing neoliberal vision of knowledge production.\n\n\n\n\n\nscisci x DH: There is a tendency to focus on quantitative metrics such as citations and visibility to quantify research values, which could have an effect on who is hired and how funding is allocated ([[piper_there_2016]], [[piper_publication_2017]], more bibliometric stuff, also in recurrent reason of why to code in DH).\n\n\n\n\n\n\n\n\n\n7. The relationships between more qualitative science and computer science is complicated.\n\n\n\n\n\nscisci x DH: as they don’t necessary share the same goals ( [[wallach_computational_2018]]), coding might play other roles than advancing knowledge ([[allington_neoliberal_2016]], [[gold_scandal_2019]]) and qualitative knowledge, by definition, is not trivial to encode on a computer.\n\n\n\n\n\n\n\n\n\n8. Women+ have been pushed out from computer science in the 1980s\n\n\n\n\n\ncomp-history: See [[abbate_recoding_2012]], [[hicks_programmed_2018]] [[thompson_coders_2020]]. This is visible in gender-biased word embeddings trained on recent language corpora ([[bolukbasi_man_2016]]). In recent years, people have fight to change this state of affairs but this is far from a victory [[laberge_subfield_2022]].\n\n\n\n\n\n\n\n\n\n10. As with other online communities, there is toxicity in the open source world, especially when it comes to identity politics.\n\n\n\n\n\noss-studies: To understand toxicity, one must go back to some facets of the hacker culture and transition from hacker cultures to brogrammers, e.g. humors, showing skills, ethics, etc ([[coleman_coding_2013]], [[miller_did_2022]]). See [[skud_being_2011]], [[balali_newcomers_2018]], [[trinkenreich_womens_2022]], as well as [[lakhani_why_2003]], [[shah_motivation_2006]] for motivation in OSS.\n\n\n\n\n\n\n\n\n\n11. In the 1980s, computing began to be seen as a male activity.\n\n\n\n\n\ncomp-history: The male bias means a generation of girls were less and less in contact with computers.\n\n\n\n\n\n\n\n\n\n12. Many “digital natives” are no hackers.\n\n\n\n\n\ncomp-history: Although computing was seen as male activity, it does not mean that all males were hackers. Starting in the 1980s, computers were tied to videogames and computer became accessible via graphical interfaces. Many “digital natives” are no hackers ([[osullivan_programming_2015]]). This situation is arguably tied to smaller and smaller computer chips that did not invite the newer generations to hack their technologies (Rasberry Pi talk ).\n\n\n\n\n\nConnecting the facts:\n\nTODO\n\n\n\nRQs:\n\nDo we see a rise in computational works across fields? Which fields experienced faster growth? Which fields are lagging?\nIs the rise of programming within field is driven by one or a few institutions? Are these institutions historically related to computing and the Internet development?\n\n\n\n\n\nflowchart LR\nlabor_advantage --> productivity \nlabor_advantage --> computational --> productivity\ninst_prestige --> labor_advantage\n\n\n\n\n\n\nWhat is the effect of groups taking a computational turn on team diversity (e.g. gender and fields of provenance) v. popularity/productivity? Something like:\n\n\n\n\n\nflowchart LR\ncomputational --> fieldOfStudy_team\ncomputational --> gender_team\ncomputational --> citation\n\n\n\n\n\n\nCan we detect a wedge forming between computational groups within fields traditionally unrelated to computer science and traditional research groups?\nDo computational groups are significantly receiving more funding because they are now doing computational works? Can we show something like:\n\n\n\n\n\nflowchart LR\ncomputational --> trustworthiness --> selected \ncomputational --> newsworthiness --> selected\n\n\n\n\n\n\n\n\n\n\n\n1. Is privileged early access to digital infracture gave an early advantage to prestigious institions, even more so with the rise of computational works?\n\n\n\n\n\ncomp-history: Some institutions had privileged access to the internet and digital infrastructure before others, which mean they benefited from digital technologies for a longer time. These institutions might be tied to prestige status. As the internet became delocalized, the relative importance of being the first to be on the internet might have been decreasing.\n\n\n\n\n\n\n\n\nFootnotes\n\n\nNote that some of the articles are about the gap between statistical education in ecology and current methods used nowadays. Underlying this argument lies a gap between traditional statistics and often computational methods requiring to various to know how to program.↩︎"
  },
  {
    "objectID": "posts/survey-programming/index.html",
    "href": "posts/survey-programming/index.html",
    "title": "Survey results (early release)",
    "section": "",
    "text": "pdata = FileAttachment(\"data_clean.csv\").csv({ typed: true })\n\n// useful vars\ncolnames = d3.sort(Object.keys(pdata[0]))\ncoders = pdata.filter(d => d.is_coder === 'coder')\nnon_coders = pdata.filter(d => d.is_coder === 'non coder')\ncoder_count = tidy(pdata, count(\"is_coder\")).map(d => d.n)\n\n// global filtering\nfiltered_dat = sel_dept == \"\" ? pdata : pdata.filter(d => d[\"dept_students_lab\"] === sel_dept)\nfiltered_coders = sel_dept == \"\" ? coders : coders.filter(d => d[\"dept_students_lab\"] === sel_dept)\nWe currently have  valid responses, from  different departments. There are  coders and  non-coders."
  },
  {
    "objectID": "posts/survey-programming/index.html#profiles",
    "href": "posts/survey-programming/index.html#profiles",
    "title": "Survey results (early release)",
    "section": "Profiles",
    "text": "Profiles\n\nviewof sel_dept = Inputs.select([''].concat(\n  tidy(\n    pdata.filter(d => d[\"dept_students_lab\"] !== null),\n    distinct('dept_students_lab')\n  ).map(d => d[\"dept_students_lab\"])), {label: \"Choose dept\"})\n\nviewof do_pct = Inputs.toggle({label: \"Show %\"}) \nviewof rm_nulls = Inputs.toggle({label: \"Remove nulls\", value: true})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDemographicsDev ProfilesTablesProfile DataQuestions\n\n\n\ncol_dems = [\"is_coder\", \"gender_binary\", \"year_born\", \n             \"ethnicity_binary\",  \"dept_students_binary\", \"academia_status\"]\n\nviewof by_attr = Inputs.radio(col_dems, {label: \"Attributes\"})\n\nhtml`<p class = 'q_styled'>Preferred Pronouns</p>`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_attr_v(filtered_dat, \"pref_pronouns\", by_attr, true,  do_pct)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Academia Status</p>`\n\n\n\n\n\n\n\nplot_attr_v(filtered_dat, \"academia_status\", by_attr, false, do_pct)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Departments</p>`\n\n\n\n\n\n\n\nplot_attr_v(filtered_dat, \"dept_students_lab\", by_attr, false, do_pct)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Year Born</p>`\n\n\n\n\n\n\n\nplot_yr_born(filtered_dat, by_attr)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Duration (sec)</p>`\n\n\n\n\n\n\n\nplot_duration(filtered_dat, by_attr)\n\n\n\n\n\n\n\n\n\n\nOnly showing coders.\n\nviewof by_attr2 = Inputs.radio([\"gender_binary\", \"year_born\", \"ethnicity_binary\", \"dept_students_binary\", \"academia_status\"], {label: \"Attributes\"})\n\n// Do you consider yourself to be a coder/programmer?\n\nplot_attr_v(filtered_coders, \"self_id_as_coder\", by_attr2, true, do_pct)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_attr_v(filtered_coders, \"what_os\", by_attr2, false, do_pct)\n\n\n\n\n\n\n\n\nFirst line code\n\nplot_attr_h(filtered_coders, \"first_line_code\", by_attr2, false, \"first line\")\n\n\n\n\n\n\nYears coding\n\nplot_attr_h(filtered_coders, \"years_coding\", by_attr2, false, \"years coding\")\n\n\n\n\n\n\n\n\n\nviewof form = Inputs.form({\n  extra_filter: Inputs.checkbox(['coder', 'female', 'stem', 'Non-stem'], {label: 'Only'}), \n  dep_var: Inputs.select(colnames, {value: 'self_id_as_coder', label: 'Dep var (DV)'}),\n  indep_var: Inputs.select(colnames, {value: 'gender_binary', label: 'Indep var (IV)'}), \n  do_pct_crosstab: Inputs.radio(['DV (→)', 'IV (↓)'], {label: \"Show %\", value: 'IV (↓)'})\n})\n\n\n\n\n\n\n\nfunction crosstab_filter(x) {\n    switch (x) {\n      case \"\":         return pdata; \n      case 'coder':    return pdata.filter(d => d.is_coder === 'coder'); \n      case 'female':   return pdata.filter(d => d.gender_binary === 'female'); \n      case 'stem':     return pdata.filter(d => d.dept_students_binary === 'STEM'); \n      case 'Non-stem': return pdata.filter(d => d.dept_students_binary === 'Non-STEM'); \n      case 'coder,female': \n        return pdata.filter(d => d.is_coder === 'coder' && d.gender_binary === 'female');\n      \n      case 'coder,stem': \n        return pdata.filter(d => d.is_coder === 'coder' && d.dept_students_binary === 'STEM');\n      \n      case 'female,stem': \n        return pdata.filter(d => d.gender_binary === 'female' && d.dept_students_binary === 'STEM');\n      \n      case 'female,Non-stem': \n        return pdata.filter(d => d.gender_binary === 'female' && d.dept_students_binary === 'Non-STEM');\n      \n      case 'coder,Non-stem': \n        return pdata.filter(d => d.is_coder === 'coder' && d.dept_students_binary === 'Non-STEM');\n      default:\n        return pdata.filter(d => d.is_coder === \"\")\n}}\n\ncrosstab_dat_filter1 = crosstab_filter(form.extra_filter.join(\",\"))\n\ncrosstab_dat_filter2 = {\n  if (rm_nulls) {\n      return crosstab_dat_filter1.filter(d => d[form.dep_var] !== null && d[form.indep_var] !== null) \n  } else {\n      return crosstab_dat_filter1\n  }\n}\n\ncrosstab_dat_long = tidy(\n    crosstab_dat_filter2, \n    select([form.indep_var, form.dep_var]),\n    count([form.indep_var, form.dep_var])\n)\n\ncols_ordered = [form.dep_var].concat(\n  tidy(crosstab_dat_long, distinct(form.indep_var), arrange(form.indep_var)\n  ).map(d => d[form.indep_var])\n)\n\ncrosstab_dat_w_pct = tidy(\n  crosstab_dat_long,\n  groupBy(form.do_pct_crosstab == 'DV (→)' ? form.dep_var : form.indep_var, [\n    mutateWithSummary({total: sum('n')})\n  ]),\n  mutate({ pct: d => (d.n / d.total)*100 }),\n  select(['-n', '-total'])\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThird variable: \nPercentage data\n\nInputs.table(tidy(\n    crosstab_dat_w_pct,\n    pivotWider({ namesFrom: form.indep_var, valuesFrom: 'pct' }),\n    arrange(form.dep_var)\n    ), { columns: cols_ordered })\n\n\n\n\n\n\nRaw data\n\nInputs.table(tidy(\n  crosstab_dat_long, \n  pivotWider({ namesFrom: form.indep_var, valuesFrom: 'n' }),\n  arrange(form.dep_var)\n  ), { columns: cols_ordered }\n)\n\n\n\n\n\n\n\nfunction notes_crosstab(x) {\n  switch (x) {\n        case \"self_id_as_coder ~ gender_binary\":         \n          return \"It seems that female less affirmatively  self-identify as coders, whereas male are less ambiguous about it. When adding `stem`, male respond more affirmatvely to the question, but note that we have still a small sample. Is this something that holds when controlling for experience?\"; \n        default:\n          return \"\"\n  }\n}\n\nnotes = notes_crosstab(`${form.dep_var} ~ ${form.indep_var}`)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes:\n\nThere are  respondents shown in the table.\nDoes this crosstable passes the rule of five? \n\n\n\n\nprofile_cols = ['academia_status', 'nb_advisors', 'dept_students_lab', \"pref_pronouns\", \n                \"year_born\", \"ethnicity_binary\", 'country_origin', 'us_state', 'reason_coding', 'how_did_you_learn_code','first_line_code', 'first_line_code_c','years_coding',  'freq_coding_proj', 'freq_coding_proj_c', 'freq_oss_proj', 'what_os', 'position_industry', 'use_lang']\n\nInputs.table(pdata, { columns: profile_cols })\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\nLabel\n\n\n\n\nWhat is your academic status?\nacademia_status\n\n\nDo you agree to the above terms?\nagree_term\n\n\nQ_RecaptchaScore\ncaptcha_score\n\n\nWhen using other people's code, how do you cite their library/repository?\ncite_code\n\n\nWhen using other people's data, do you cite their library/repository? How?\ncite_data\n\n\nHow likely you think that not knowing how to program will have an impact on future professional opportunities?\ncoding_on_future_opportunities\n\n\nIf you have any suggestions/comments about the survey, please share them below!\ncomments\n\n\nNow, thinking about the expectations in your field: would you say that the expected level of programming skills in your field is an important factor in your choice to pursue an academic career?\ncomp_skills_factors_pursue_academia\n\n\nHow important do you think your programming skills were for your career?\ncomp_skills_pro_benefits_p\n\n\nDo you think programming will bring you professional benefits in the future?\ncomp_skills_pro_benefits_s\n\n\nHow important do you think your programming skills are to prospective group members?\ncomp_skills_recruiting\n\n\nHow important programming skills are when hiring new group members? - Graduate students\ncomp_skills_recruiting_grad\n\n\nHow important programming skills are when hiring new group members? - Postdoctoral researchers\ncomp_skills_recruiting_postdoc\n\n\nHow important programming skills are when hiring new group members? - Undergraduate students\ncomp_skills_recruiting_undergrad\n\n\nList of Countries\ncountry_origin\n\n\nWhat is your department? Please select all that apply.\ndept_prof\n\n\nWhat is your program?\ndept_students\n\n\nDo you feel disadvantaged for not knowing how to code?\ndisadv_not_coding\n\n\nDistribution Channel\ndistribution_channel\n\n\nIf you wish to delete your survey entry write \"delete\" below\ndo_del\n\n\nDo you share your code online?\ndo_share_code_online\n\n\nDuration (in seconds)\nduration_sec\n\n\nWhat is your institutional email address?\nemail\n\n\nEnd Date\nend_survey\n\n\nDo you think you have enough institutional support (workshops, mentorships, online labs) to learn to program?\nenough_instit_support\n\n\nWhich categories best describe you?\nethnicity\n\n\nAt what age did you write your first line of code or program? (e.g., webpage, Hello World, Scratch project)\nfirst_line_code\n\n\nWhen you are working on projects (for school, work, fun) how often do you code?\nfreq_coding_proj\n\n\nHow much of the software that you use is open source? (please provide your best guess)\nfreq_oss_proj\n\n\nDo you feel you have friends, colleagues, or supervisors who can help with your coding issues?\nfriends_help\n\n\nHow did you first learn to code?\nhow_did_you_learn_code\n\n\nFinished\nis_finished\n\n\nOverall, would you like to have more time to improve your programming skills?\nmore_time_learning_to_code\n\n\nWhat is the name of your research group?\nname_research_group\n\n\nHow many advisors do you have?\nnb_advisors\n\n\nWhat percentage of your social contacts are likely to participate in a project that require programming in the upcoming academic year. Social contacts are classmates and other peers that you have communicated with at least briefly within the last month, either face-to-face, or otherwise. - Click to write Choice 1\npct_social_contacts_coding\n\n\nWhat benefits do you see in programming?\nperceived_benefits_coding\n\n\nWhich of the following describes industry positions you have held or currently hold?\nposition_industry\n\n\nWhat is your preferred pronouns?\npref_pronouns\n\n\nProgress\nprogress\n\n\nWhat qualities do you most value in software ?\nqualities_oss\n\n\nHow often do you read programming books, either to improve your long-term coding skills or out of interest?\nread_prog_book\n\n\nDo you code for any of the following reasons?\nreason_coding\n\n\nRecorded Date\nrecord_date\n\n\nResponse ID\nresponse_id\n\n\nResponse Type\nresponse_type\n\n\nScore\nscore\n\n\nFor any of your current projects, do you consider that you spent too much time to code? (e.g. data cleaning, data visualization, gathering data, and so on)?\nself_expect_time_coding\n\n\nDo you consider yourself to be a coder/programmer?\nself_id_as_coder\n\n\nStart Date\nstart_survey\n\n\nCoursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Cleaning code (debugging, refactoring, renaming variables, etc.)\ntime_cleaning_code\n\n\nCoursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Data cleaning (manually, e.g. using excel)\ntime_data_clean_gui\n\n\nCoursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Data cleaning (programmatically)\ntime_data_clean_prog\n\n\nCoursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Digital data collection (e.g. web scraping)\ntime_digital_data_coll\n\n\nCoursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Experimental manipulation\ntime_exp_manip\n\n\nCoursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Field data collection (e.g. interviews, surveys, questionnaires, observations, ethnographies, etc)\ntime_field_data_coll\n\n\nCoursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Grant writing\ntime_grant_writing\n\n\nCoursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Literature review\ntime_lit_review\n\n\nCoursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Meetings\ntime_meeting\n\n\nCoursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Writing thesis/paper\ntime_paper_writing\n\n\nCoursework aside, how many hours you typically spend on the following tasks in a given week? (please provide your best guess) - Reading software-related content (blogs, books, forums, documentation)\ntime_read_doc\n\n\nDo you consider yourself a member of an underrepresented group?\nunderrep_group\n\n\n50 States, D.C. and Puerto Rico\nus_state\n\n\nWhich programming languages, scripting, and markup languages have you worked in over the past year?\nuse_lang\n\n\nUser Language\nuser_lang\n\n\nHow important do you think the following items are in academia? - Code associated with an article is easy to find online\nvalue_accessibility_paper_code\n\n\nWhen thinking about whether to use open source software, how important are the following: - Active development\nvalue_active\n\n\nWhen thinking about whether to use open source software, how important are the following: - A contributor's license agreement (CLA)\nvalue_cla\n\n\nWhen thinking about whether to use open source software, how important are the following: - A code of conduct\nvalue_coc\n\n\nCompared to your domain expertise, how valued do you think your coding skills are around you today?\nvalue_comp_skills_wrt_domain\n\n\nWhen thinking about whether to use open source software, how important are the following: - A contributing guide\nvalue_contrib_guide\n\n\nOverall, how important do you think it is to learn programming in your academic field today?\nvalue_learn_code_in_field\n\n\nWhen thinking about whether to use open source software, how important are the following: - An open source license\nvalue_oss_license\n\n\nHow important do you think the following items are in academia? - Code associated with an article is citable.\nvalue_paper_code_citability\n\n\nWhen thinking about whether to use open source software, how important are the following: - Responsive maintainers\nvalue_responsive_maintainers\n\n\nHow important do you think the following items are in academia? - Sharing code associated with an academic paper\nvalue_share_code\n\n\nWhen thinking about whether to use open source software, how important are the following: - A welcoming community\nvalue_welcoming_community\n\n\nWhen thinking about whether to use open source software, how important are the following: - Widespread use\nvalue_widespread_use\n\n\nWhat is the primary operating system in which you work?\nwhat_os\n\n\nIf you want to learn to program, what's stopping you?\nwhy_not_coding\n\n\nIn what year were you born?\nyear_born\n\n\nHow many years have you been coding?\nyears_coding"
  },
  {
    "objectID": "posts/survey-programming/index.html#costs-benefits",
    "href": "posts/survey-programming/index.html#costs-benefits",
    "title": "Survey results (early release)",
    "section": "Costs & benefits",
    "text": "Costs & benefits\n\nviewof sel_dept_cb = Inputs.select([''].concat(\n  tidy(\n    pdata.filter(d => d[\"dept_students_lab\"] !== null),\n    distinct('dept_students_lab')\n  ).map(d => d[\"dept_students_lab\"])), { label: \"Choose dept\" })\n\nviewof by_attr_cb = Inputs.radio([\"gender_binary\", \"year_born\", \"ethnicity_binary\", \"dept_students_binary\", \"academia_status\"], { label: \"Attributes\" })\nviewof do_pct_2 =   Inputs.toggle({ label: \"Show %\" }) \nviewof show_labs =   Inputs.toggle({ label: \"Show Labels\", value: true }) \nviewof rm_nulls_2 = Inputs.toggle({ label: \"Remove nulls\", value: true })\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCostsBenefitsOSSData\n\n\n\nhtml`<p class = 'q_styled'>Do you think you have enough institutional support (workshops, mentorships, online labs) to learn to program?</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n  show_labs ? \"enough_instit_support\" : \"enough_instit_support_ord\",      \n  by_attr_cb, \n  true,  \n  do_pct_2\n)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Overall, would you like to have more time to improve your programming skills?</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders,   \n  show_labs ? \"more_time_learning_to_code\": \"more_time_learning_to_code_ord\", \n  by_attr_cb, \n  false, \n  do_pct_2\n)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>For any of your current projects, do you consider that you spent too much time to code? (e.g. data cleaning, data visualization, gathering data, and so on)?</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n show_labs ? \"self_expect_time_coding\" : \"self_expect_time_coding_ord\", \n    by_attr_cb, \n    false,  \n    do_pct_2\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nDepending on other variables, the answers to these questions can be interepreted both as relative costs or benefits, e.g. thinking that expected level of programming skills is important in your field is a cost when you are not skilled yet.\n\n\n\n\nhtml`<p class = 'q_styled'> Compared to your domain expertise, how valued do you think your coding skills are around you today?</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n  show_labs ? \"value_comp_skills_wrt_domain\" : \"value_comp_skills_wrt_domain_ord\", \n  by_attr_cb, \n  false, \n  do_pct_2)\n\n\n\n\n\n\n\n\n\nfunction plot_social_contacts(dat, by_attr) {\n  return Plot.plot({\n    grid: true,\n    x: {\n      label: \"Social contact coding (%)  →\",\n    },\n    y: { \n      label: do_pct_2 ? \" Frequency (%) ↑\" : \"# respondents ↑\", \n      percent: do_pct_2 ? true : false\n    },\n    marks: [\n      Plot.barY(\n        rm_nulls ? dat.filter(d => d[\"pct_social_contacts_coding\"] !== null) : dat, \n        Plot.groupX({ y: do_pct_2 ? \"proportion\" : \"count\" }, { \n            x: \"pct_social_contacts_coding\",  fill: d => by_attr ? d[by_attr] : \"grey\"\n          })\n      ),\n      Plot.ruleY([0])\n    ]\n  })\n}\n\nhtml`<p class = 'q_styled'> What percentage of your social contacts are likely to participate in a project that require programming in the upcoming academic year?</p>`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_social_contacts(coders, by_attr_cb)\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Do you share your code online?</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \"do_share_code_online\", by_attr_cb, true,  do_pct_2)\n\n\n\n\n\n\n\nhtml`<h4>How important do you think the following items are in academia?</h4>`\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Code associated with an article is citable.</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n  show_labs ? \"value_paper_code_citability\" : \"value_paper_code_citability_ord\", \n  by_attr_cb, false, do_pct_2)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Code associated with an article is easy to find online</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n  show_labs ? \"value_accessibility_paper_code\" : \"value_accessibility_paper_code_ord\", \n  by_attr_cb, false, do_pct_2)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Sharing code associated with an academic paper</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n  show_labs ? \"value_share_code\" : \"value_share_code_ord\", \n  by_attr_cb, false, do_pct_2)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>A welcoming community</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n  show_labs ? \"value_welcoming_community\" : \"value_welcoming_community_ord\", \n  by_attr_cb, false, do_pct_2)\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Widespread use</p>`\n\n\n\n\n\n\n\nplot_attr_v(coders, \n  show_labs ? \"value_widespread_use\" : \"value_widespread_use_ord\", \n  by_attr_cb, false, do_pct_2)\n\n\n\n\n\n\n\n\n\nBox plots\n\nagg_oss = tidy(\n  coders, \n  select([startsWith(\"value\"), \"gender_binary\"]),\n  filter(d => d.gender_binary != null),\n  pivotLonger({\n    cols: ['-gender_binary'],\n    namesTo: \"catego\",\n    valuesTo: \"val\"\n  }),\n  filter(d => /_ord/.test(d.catego) && d.val != \"NA\" && d.val <= 5)\n)\n\nPlot.plot({\n  height: 400,\n  marginTop: 0,\n  marginLeft: 250,\n  x: { inset: 10, grid: true, label: \"attitude →\" },\n  y: { axis: null, inset: 2},\n  color: {legend: true},\n  fy: {\n    label: \"questions (m) →\",\n    reverse: true\n  },\n  facet: {\n    data: agg_oss,\n    y: \"catego\",\n    marginLeft: 250\n  },\n  marks: [\n    Plot.frame({stroke: \"#aaa\", strokeWidth: 0.5}),\n    Plot.boxX(agg_oss, {x: \"val\", y: \"gender_binary\", stroke: \"gender_binary\", r: 1})\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncost_cols = ['pct_social_contacts_coding', 'comp_skills_factors_pursue_academia',\n             'comp_skills_pro_benefits_s', 'enough_instit_support',\n             'friends_help', 'perceived_benefits_coding', 'first_adv_expect_time_coding',\n             'second_adv_expect_time_coding', 'reason_coding',  'value_comp_skills_wrt_domain', \n             'do_share_code_online', \"value_learn_code_in_field\"]\n\nInputs.table(coders, { columns: cost_cols })"
  },
  {
    "objectID": "posts/survey-programming/index.html#students-typical-week",
    "href": "posts/survey-programming/index.html#students-typical-week",
    "title": "Survey results (early release)",
    "section": "Students typical week",
    "text": "Students typical week\n\n\nagg_dat = tidy(\n  coders, \n  select([startsWith('time')]), \n  pivotLonger({\n    cols: [startsWith('time')],\n    namesTo: 'task',\n    valuesTo: 'val'\n  }),\n  count(['task', 'val']\n))\n\n\nPlot.plot({\n  width: 1000,\n  marginBottom: 100,\n  grid: true,\n  x: {\n    tickRotate: 45,\n    label: \"\",\n    domain: ['0 hour', '1-5 hours', '6-10 hours', '11-15 hours', null]\n  },\n  facet: {\n    data: agg_dat,\n    x: d => d.task.replace('time_', '')\n  },\n  marks: [\n    Plot.barY(\n      agg_dat,\n      {x: \"val\", y: \"n\", fill: d => d.val === null ? 'grey' : 'task'}\n    ),\n    Plot.ruleY([0])\n  ]\n})"
  },
  {
    "objectID": "posts/survey-programming/index.html#what-do-non-coders-think-about-coding",
    "href": "posts/survey-programming/index.html#what-do-non-coders-think-about-coding",
    "title": "Survey results (early release)",
    "section": "What do non-coders think about coding?",
    "text": "What do non-coders think about coding?\n\nviewof by_attr_nc = Inputs.radio([\"gender_binary\", \"year_born\", \"ethnicity_binary\", \"dept_students_binary\", \"academia_status\"], { label: \"Attributes\" })\nviewof do_pct_nc =   Inputs.toggle({ label: \"Show %\", value: true }) \nviewof show_labs_nc =   Inputs.toggle({ label: \"Show Labels\", value: true })\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`<p class = 'q_styled'>Overall, how important do you think it is to learn programming in your academic field today?</p>`\n\n\n\n\n\n\n\nplot_attr_v(non_coders, \n  show_labs_nc ? \"value_learn_code_in_field\" : \"value_learn_code_in_field_ord\",\n  by_attr_nc, \n  true, \n  do_pct_nc)\n\n\n\n\n\n\n\n\n\nimport { tidy, select, count, complete, filter, fullSeq, sum, mutateWithSummary, groupBy, pivotWider, startsWith, pivotLonger, mutate, distinct, summarize, arrange, pull } from '@pbeshai/tidyjs'\n\n\n\n\n\n\n\nhtml`<style>\n.q_styled {\n        position: relative; \n        top: 15px;\n        font-size: 14px;\n        font-family: sans-serif;\n        text-align: center;\n        font-style: italic\n    }\n</style>\n`"
  },
  {
    "objectID": "posts/rise_of_comp2/index.html",
    "href": "posts/rise_of_comp2/index.html",
    "title": "The rise of computational works 2",
    "section": "",
    "text": "Warning\n\n\n\nIt takes a while for everything to load. Be patient.\n\n\n\nTODO\n\nsee project\n\n\n\nSpecter embeddings of computational papers (early)\n\nviewof type_cat = Inputs.select(['topic', 'field', 'year', 'selected'], {value: 'field', label: \"cluster type\"})\nviewof sel_field = Inputs.select([''].concat(Object.keys(lookup)), {label: \"Type field\", multiple: true})\n// viewof sel_field = Inputs.select([''].concat(fields), {value: '', label: \"choose field\"})\n\n// viewof input_title = html`&lt;input placeholder=\"Title.\"&gt;`\n\n// viewof c = rangeSlider(background_data, d =&gt; d.year)\n\nviewof year_min = Inputs.range([1950, 2020], {label: 'year min', value: 1950})\nviewof year_max = Inputs.range([1951, 2020], {label: 'year max', value: 2020})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof reset = html`&lt;button&gt;Reset`\n\nplot_embedding = () =&gt; {\n  const zoom = d3.zoom()\n      .scaleExtent([0.5, 32])\n      .on(\"zoom\", zoomed);\n\n  const svg = d3.create(\"svg\")\n      .attr(\"viewBox\", [0, 0, width, height]);\n\n  const gGrid = svg.append(\"g\");\n\n  // Background data\n\n  const gDotB = svg.append(\"g\")\n        .attr(\"fill\", \"none\")\n        .attr(\"class\", \"circles\")\n        .attr(\"stroke-linecap\", \"round\");\n\n  gDotB.selectAll(\"circle\")\n      .data(background_data)\n      .join(\"circle\")\n        .attr(\"cx\", d =&gt; x(d.x))\n        .attr(\"cy\", d =&gt; y(d.y))\n        .attr(\"fill\", 'grey')\n        .attr(\"opacity\", 0.3)\n        .attr(\"r\", d =&gt; r(d.citationCount));\n\n  // Yearly data\n\n  const gDot = svg.append(\"g\")\n      .attr(\"fill\", \"none\")\n      .attr(\"class\", \"circles\")\n      .attr(\"stroke-linecap\", \"round\");\n\n  gDot.append(\"style\").text(hover_css);\n\n  gDot.selectAll(\"circle\")\n    .data(yearly_data)\n    .join(\"circle\")\n      .attr(\"cx\", d =&gt; x(d.x))\n      .attr(\"cy\", d =&gt; y(d.y))\n      .attr(\"fill\", d =&gt; sel_field === \"\" ? z(d[type_cat]) : 'grey')\n      .attr(\"opacity\", d =&gt; sel_field === \"\" ? 0.5 : 0.3)\n      .attr(\"r\", d =&gt; r(d.citationCount))\n    .append(\"title\")\n      .text(d =&gt; `${d.title} (${d.field})\\n# citations: ${d.citationCount}\\nYear: ${d.year}\\ntopic: ${d.topic}`);\n  \n  // Field data\n\n  const gDotF = svg.append(\"g\")\n        .attr(\"fill\", \"none\")\n        .attr(\"class\", \"circles\")\n        .attr(\"stroke-linecap\", \"round\");\n\n  gDotF.append(\"style\").text(hover_css);\n\n  if (sel_field !== '') {\n    \n    gDotF.selectAll(\"circle\")\n      .data(data_by_field)\n      .join(\"circle\")\n        .attr(\"cx\", d =&gt; x(d.x))\n        .attr(\"cy\", d =&gt; y(d.y))\n        .attr(\"fill\", d =&gt; sel_field === \"\" ? 'black' : 'red')\n        .attr(\"opacity\", d =&gt; sel_field === \"\" ? 0 : 0.4)\n        .attr(\"r\", d =&gt; r(d.citationCount))\n      .append(\"title\")\n        .text(d =&gt; `${d.title} (${d.field})\\n# citations: ${d.citationCount}\\nYear: ${d.year}\\ntopic: ${d.topic}`);\n  }\n  \n  const gx = svg.append(\"g\");\n\n  const gy = svg.append(\"g\");\n\n  svg.call(zoom).call(zoom.transform, d3.zoomIdentity);\n\n  function zoomed({transform}) {\n    const zx = transform.rescaleX(x).interpolate(d3.interpolateRound);\n    const zy = transform.rescaleY(y).interpolate(d3.interpolateRound);\n    gDot.attr(\"transform\", transform).attr(\"stroke-width\", 2 / transform.k);\n    gDotF.attr(\"transform\", transform).attr(\"stroke-width\", 2 / transform.k);\n    gDotB.attr(\"transform\", transform).attr(\"stroke-width\", 2 / transform.k);\n    gx.call(xAxis, zx);\n    gy.call(yAxis, zy);\n    gGrid.call(grid, zx, zy);\n  }\n\n  return Object.assign(svg.node(), {\n    reset() {\n      svg.transition()\n          .duration(750)\n          .call(zoom.transform, d3.zoomIdentity);\n    }\n  });\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nchart = plot_embedding()\n\n\n\n\n\n\n\n\nreset, chart.reset()\n\n// embedding_plot = {\n \n//   const background_mark = (is_colored) =&gt; {  \n//             return { x: \"x\", y: \"y\", fill: is_colored ? type_cat : \"grey\",  opacity: 0.2, r: \"citationCount\" }\n//         }\n  \n//   const subset_mark = {\n//           x: \"x\", y: \"y\", fill: type_cat, r: \"citationCount\", \n//           opacity: subset === undefined ? 0.3 : 1, \n//           title: d =&gt; `${d.title} (${d.field})\\n# citations: ${d.citationCount}\\nYear: ${d.year}\\ntopic: ${d.topic}`\n//     }\n\n//   const field_mark = {\n//           x: \"x\", y: \"y\", opacity: 0.3, r: \"citationCount\",\n//           fill: type_cat === 'field' ? 'red' : type_cat,\n//           title: d =&gt; `${d.title} (${d.field})\\n# citations: ${d.citationCount}\\nYear: ${d.year}\\ntopic: ${d.topic}`\n//         }\n  \n//   const year_mark = {\n//           x: \"x\", y: \"y\", opacity: 0.8, r: \"citationCount\", \n//           fill: type_cat, \n//           title: d =&gt; `${d.title} (${d.field})\\n# citations: ${d.citationCount}\\nYear: ${d.year}\\ntopic: ${d.topic}`\n//         }\n  \n//   const paperIds_mark = {\n//           x: \"x\", y: \"y\", opacity: 1, r: \"citationCount\", fill: 'lab',\n//           title: d =&gt; `${d.title} (${d.field})\\n# citations: ${d.citationCount}\\nYear: ${d.year}\\ntopic: ${d.topic}\\nlab: ${d.lab}`\n//         }\n\n//   const background_plot = () =&gt; Plot.plot({\n//       inset: 8, height: height, width: width,\n//       y: { label: \"\", ticks: null },\n//       x: { label: \"\", ticks: null },\n//       r: { range: [1.5, 10] },\n//       color: { type: \"categorical\", scheme: \"paired\" },\n//       marks: [  Plot.dot(background_data,  background_mark(true)  ) ]\n//     })\n  \n//   const subset_plot = () =&gt; Plot.plot({\n//       inset: 8, height: height,  width: width,\n//       y: { label: \"\", ticks: null },\n//       x: { label: \"\", ticks: null },\n//       r: { range: [1.5, 10] },\n//       color: { type: \"categorical\", scheme: \"paired\" },\n//       marks: [\n//         Plot.dot(background_data,  background_mark(false)  ),\n//         Plot.dot(subset, subset_mark )\n//         ]\n//     })\n  \n//   const field_plot = () =&gt; Plot.plot({\n//       inset: 8, height: height, width: width,\n//       y: { label: \"\", ticks: null },\n//       x: { label: \"\", ticks: null },\n//       r: { range: [1.5, 10] },\n//       color: { type: \"categorical\", scheme: \"paired\" },\n//       marks: [\n//         Plot.dot(background_data,  background_mark(false)  ),\n//         Plot.dot(data_by_field, field_mark )\n//         ]\n//     })\n  \n//   const year_plot = () =&gt; Plot.plot({\n//       inset: 8, height: height, width: width,\n//       y: { label: \"\", ticks: null },\n//       x: { label: \"\", ticks: null },\n//       r: { range: [1.5, 10] },\n//       marks: [\n//         Plot.dot(background_data,  background_mark(false)  ),\n//         Plot.dot(yearly_data, year_mark  )\n//         ]\n//     })  \n\n//   const paperIds_plot = () =&gt; Plot.plot({\n//       inset: 8, height: height, width: width,\n//       y: { label: \"\", ticks: null },\n//       x: { label: \"\", ticks: null },\n//       r: { range: [1.5, 10] },\n//       marks: [\n//         Plot.dot(background_data,  background_mark(false)  ),\n//         Plot.dot(paperIds_data, paperIds_mark)\n//         ]\n//     })  \n\n//   const subset_and_field_plot = () =&gt; Plot.plot({\n//       inset: 8, height: height, width: width,\n//       y: { label: \"\", ticks: null },\n//       x: { label: \"\", ticks: null },\n//       r: { range: [1.5, 10] },\n//       color: { type: \"categorical\", scheme: \"paired\" },\n//       marks: [\n//         Plot.dot(background_data,  background_mark(false)  ),\n//         Plot.dot(data_by_field, field_mark),\n//         Plot.dot(subset, subset_mark)\n//         ]\n//     })\n\n//   if (subset === undefined && sel_field[0] === undefined) {\n//     if (toggle) {\n//       return paperIds_plot()\n//     }\n//     return year_plot()\n//   } else if (subset !== undefined && sel_field[0] === undefined) {\n//     return subset_plot()\n//   } else if (subset === undefined && sel_field[0] !== undefined) {\n//     return field_plot()\n//   } else if (subset === undefined && sel_field[0] === undefined && type_cat === 'selected') {\n//     return paperIds_plot()\n//   } else {\n//     return subset_and_field_plot()\n//   } \n\n// }\n\n// type_cat === 'selected' ? svg`` : embedding_plot.legend(\"color\")\n\n\n\n\n\n\nThere are  paper from paperIds.\nNote that the topics here are custers of documents that are similar with respect to title, abstract, and citations. This is different from vanilla topics that cluster together exclusively based on content.\nBy adding document-level relatedness, we hope that we will be able to distinguish different facets of how the idea of computational is used in the literature. For instance, say that in philosophy there is much discussion about the computational theory of the mind. Philosophers won’t be citing the same papers than computational papers in political science that cite, say, some methodological papers on how best to construct embeddings in their field. If we are right, then conceptual papers should be fairly easy to distinguish and remove.\n\ntable\n\nInputs.table(yearly_data, {columns: ['topic', 'field', 'year', 'venue', 'citationCount', 'title']})\n\n\n\n\n\n\n\ndb = DuckDBClient.of({ \n    data: await FileAttachment(\"umap_embedding_2NCOMP.csv\").csv({typed:true}),\n  classifyComp: await FileAttachment(\"classify-comp-proj-tidy.csv\").csv({typed:true})\n})\n\nbackground_data = db.sql`\n  SELECT * \n  FROM data  \n  USING SAMPLE 10000\n`\n\ndata_by_field = db.sql`\n  SELECT * \n  FROM data \n  WHERE (field = ${sel_field} AND year &gt;= ${year_min} AND year &lt;= ${year_max} AND citationCount &gt; 0)\n`\n\nyearly_data = db.sql`\n  SELECT * \n  FROM data \n  WHERE (year &gt;= ${year_min} AND year &lt;= ${year_max} AND citationCount &gt; 0)\n  USING SAMPLE 20000\n`\n\n// paperIds_data = db.query(\n//     `SELECT * \n//      FROM data \n//      INNER JOIN classifyComp ON data.paperId = classifyComp.paper_id;\n// `)\n\ntable_data = db.sql`\n  SELECT year, topic, COUNT()::INT as n\n  FROM data \n  WHERE (year &gt;= ${year_min} AND year &lt;= ${year_max})\n  GROUP BY\n    year, topic\n`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLimitations\n\nThe background data is a sample of 20K documents (out of 156K).\nWhen searching a title, we look at all the data (not the 20K subset)\nWe overlay filtered data on top of the background data, which is also a subset of 20K.\n\n\n\n“Degrees of freedom”\n\nWith hdbscan, we can reduce the number of topics so that we have coarser grain topics.\nWith umap, we can choose the number of dimensions to reduce\n\n\nimport {Plot} from \"@mkfreeman/plot-tooltip\"\n\n\n\n\n\n\n\nfields = ['Art', 'Biology', 'Business', 'Chemistry', 'Computer-Science', 'Education', 'Economics', 'Environmental-Science', 'Geography', 'Geology', 'History', 'Law', 'Linguistics', 'Mathematics', 'Philosophy', 'Physics', 'Political-Science', 'Psychology', 'Sociology', ]\n\nlookup = {\n  const out = {}\n  for (let i=0; i &lt; fields.length; i++) {\n    out[fields[i]] = i\n  }\n  return out\n}\n\nhover_css = `\n    .circles {\n      stroke: transparent;\n      stroke-width: 0.1px;\n    }\n    .circles circle:hover {\n      stroke: black;\n    }\n  `\n\nheight = 600\nwidth = 1200\nk = height / width\nmargin = ({top: 12, right: 10, bottom: 26, left: 26})\n\nxMin = d3.min(yearly_data, d=&gt;d.x)\nxMax = d3.max(yearly_data, d=&gt;d.x)\nyMin = d3.min(yearly_data, d=&gt;d.y)\nyMax = d3.max(yearly_data, d=&gt;d.y)\n\nextent_x = d3.extent(yearly_data, d =&gt; d.x)\nextent_y = d3.extent(yearly_data, d =&gt; d.y)\n\nx = d3.scaleLinear()\n  .domain([extent_x[0]-3, extent_x[1]+3])\n  .range([0, width])\n\ny = d3.scaleLinear()\n  .domain([(extent_y[0] * k)-3, (extent_y[1] * k)+3])\n  .range([height, 0])\n\nr = d3.scaleLog()\n  .domain(d3.extent(yearly_data.map(d =&gt; d.citationCount)))\n  .range( [0.5, 1.3] )\n\nz = d3.scaleOrdinal()\n  .domain(yearly_data.map(d =&gt; d[type_cat]))\n  .range(d3.schemePaired)\n\nyAxis = (g, y) =&gt; g\n  .call(d3.axisRight(y).ticks(12 * k))\n  .call(g =&gt; g.select(\".domain\").attr(\"display\", \"none\"))\n\nxAxis = (g, x) =&gt; g\n    .attr(\"transform\", `translate(0,${height})`)\n\ngrid = (g, x, y) =&gt; g\n    .attr(\"stroke\", \"currentColor\")\n    .attr(\"stroke-opacity\", 0.1)\n    .call(g =&gt; g\n      .selectAll(\".x\")\n      .data(x.ticks(12))\n      .join(\n        enter =&gt; enter.append(\"line\").attr(\"class\", \"x\").attr(\"y2\", height),\n        update =&gt; update,\n        exit =&gt; exit.remove()\n      )\n        .attr(\"x1\", d =&gt; 0.5 + x(d))\n        .attr(\"x2\", d =&gt; 0.5 + x(d)))\n    .call(g =&gt; g\n      .selectAll(\".y\")\n      .data(y.ticks(12 * k))\n      .join(\n        enter =&gt; enter.append(\"line\").attr(\"class\", \"y\").attr(\"x2\", width),\n        update =&gt; update,\n        exit =&gt; exit.remove()\n      )\n        .attr(\"y1\", d =&gt; 0.5 + y(d))\n        .attr(\"y2\", d =&gt; 0.5 + y(d)));"
  },
  {
    "objectID": "posts/s2orc_viz/index.html",
    "href": "posts/s2orc_viz/index.html",
    "title": "S2ORC Viz",
    "section": "",
    "text": "Abstracts and pdf parsed shows the total number of papers and the number of papers for which either the abstract or pdf is parsed.\ns2fos v. mag is a showdown between the semantic scholar classification scheme and the microsoft academic graph one. Papers are grouped by fields and year, and we keep track of the parsing extent for each.\n\n\nimport {addTooltips} from \"@mkfreeman/plot-tooltip\"\n\n\n\n\n\n\n\ndata = FileAttachment(\"count_field_and_decade.csv\").csv({ typed: true })\n\n\n\n\n\n\n\nviewof schema = Inputs.select([\"s2fos\", \"mag\", \"both\"], { label: \"Schema\" })\nviewof show_pct = Inputs.toggle({label: \"Normalize\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbstracts and pdf parseds2fos v. magraw data\n\n\n\n\np1 = Plot.plot({\n    marginLeft: 100,\n    height: 600,\n    width: 1200,\n    marginBottom: 100,\n    x: { label: null, tickRotate: 45 },\n    y: { \n      grid: true, \n      percent: show_pct ? true : false, \n      label: show_pct ? \"↑ rep. (%)\" : \"↑ n papers\"\n     },\n    marks: [\n      Plot.barY(data.filter(d =&gt; d.year &lt; 2020), Plot.groupX({y: \"sum\"}, { x: \"year\", y: d =&gt; schema == 'both' ? d[\"n\"] : d[`n_${schema}`], fill: \"parsing\", order: \"sum\", offset: show_pct ? \"expand\" : null }))\n    ]\n})\np1.legend(\"color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof mark_type = Inputs.select([\"stacked\", \"line\"], { label: \"Plot type\" })\nviewof parsing = Inputs.select([\"abstract\", \"pdf\", \"all\"], { label: \"Parsing extent\" })\nviewof chosen_group = Inputs.select([\"STEM\", \"Social Science\", \"Misc\"], { label: \"Group\" })\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata_f = data.filter(d =&gt; d.group == chosen_group && d.year &lt; 2020 && d.parsing == parsing)\n\nline_plot = () =&gt; {\n  const xy_norm = Plot.normalizeY({\n          x: \"year\",  y:  d =&gt; schema == 'both' ? d[\"n\"] : d[`n_${schema}`], stroke: \"field\", title: \"field\", basis: \"first\"\n      })\n  const xy = {\n          x: \"year\",  y:  d =&gt; schema == 'both' ? d[\"n\"] : d[`n_${schema}`], stroke: \"field\", title: \"field\"\n      }\n  return addTooltips(Plot.plot({\n      marginLeft: 100,\n      height: 600,\n      width: 1200,\n      marginBottom: 50,\n      x: { label: null, tickRotate: 45 },\n      y: { \n        percent: show_pct ? true : false, \n        grid: true, \n        label: show_pct ? \"↑ Norm. over first value, showing relative growth\" : \"↑ n papers\" },\n      marks: [\n        Plot.line(data_f, show_pct ? xy_norm : xy)\n      ]\n  }))\n}\n\nstack_plot = () =&gt; {\n  const xy = { x: \"year\", y: d =&gt; schema == 'both' ? d[\"n\"] : d[`n_${schema}`], z: \"field\", order: \"sum\",  offset: show_pct ? \"expand\" : null }\n\n  return addTooltips(Plot.plot({\n        marginLeft: 100,\n        height: 600,\n        width: 1200,\n        marginBottom: 50,\n        x: { label: null, tickRotate: 45 },\n        y: { \n          percent: show_pct ? true : false, \n          grid: true, \n          label: show_pct ? \"↑ rep. (%)\" : \"↑ tot papers\" },\n        marks: [\n          Plot.barY(data_f, Plot.stackY({...xy, fill: \"field\", title: d =&gt; `${d.field} (${d.n})` })),\n        ]\n    }))\n}\n\n\np3 = mark_type == 'stacked' ? stack_plot() : line_plot()\n\np3.legend(\"color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInputs.table(data)\n\n\n\n\n\n\n\n\n\n\nRemarks\n\nBiology is an encompassing field. I wish we could divide it further.\nIn my opinion the biggest difference beween magand s2fos schema is that the later track the emergerce of computer science, which is nice.\n\n\n\nNotes\nClassifying articles into fields is non-trivial. But at least there is work on the topic.\nFor a while, the microsoft academic graph (mag) was standard. Many researchers used the top mag fields of study as their main taxonomy. Now that the mag project at microsoft is deprecated (Dec. 31, 2021.; see here), AllenAI’s semantic scholar is arguably one of the best contender to become the next standard. Contrary to MAG, they provide a unified api for the citation graphs of papers and they released a fraction of their overall paper nodes as parsed text. Announced after mag deprecation, the semantic scholar databse did some more work on the classification scheme, which can be found here. This is what we use and compare here, as did other before us.\nWe note that all the current schemes agree that papers have overlapping fields and are hierarchical. This is something that the mag field made explicit through the use of a hierarhical algorithm for unsupervised topic classification. The s2_fos scheme drops the hierarchy and focus on the top fields using a (simpler) classifier."
  },
  {
    "objectID": "posts/rise_of_comp3/index.html",
    "href": "posts/rise_of_comp3/index.html",
    "title": "The rise of computational works 3",
    "section": "",
    "text": "import {Plot} from \"@mkfreeman/plot-tooltip\""
  },
  {
    "objectID": "posts/rise_of_comp3/index.html#rise-of-programming",
    "href": "posts/rise_of_comp3/index.html#rise-of-programming",
    "title": "The rise of computational works 3",
    "section": "Rise of programming?",
    "text": "Rise of programming?\n\n\n\nviewof select = Inputs.select(fields, { multiple: true, value: ['linguistics', 'philosophy', 'history'] })\nviewof show_pct = Inputs.toggle({label: \"normalize\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata_f = which(rin(data.map(d => d.field), select)).map(i => data[i]).filter(d => d.year > 1970)\n\nline_plot = () => {\n  \n  const xy_norm = {\n          x: \"year\",  y:  'pct_comp', stroke: \"field\", title: \"field\"\n      }\n    \n  const xy = {\n          x: \"year\",  y:  'n_comp', stroke: \"field\", title: \"field\"\n      }\n\n  return Plot.plot({\n      marginLeft: 50,\n      height: 600,\n      width: 800,\n      marginBottom: 50,\n      x: { label: null, tickRotate: 45 },\n      y: { \n        percent: show_pct ? true : false, \n        grid: true, \n        label: show_pct ? \"↑ %\" : \"↑ n papers\" },\n      marks: [\n        Plot.line(data_f, show_pct ? xy_norm : xy)\n      ]\n  })\n}\n\nlp = line_plot()\n\nlp.legend(\"color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata = FileAttachment(\"comp_normalized.csv\").csv({ typed: true })\n\nfields = ['mathematics', 'geology', 'environmental science', 'art','history', 'linguistics', 'psychology',  'education', 'geography', 'physics', 'law', 'sociology', 'economics', 'political science', 'philosophy']\n\nfunction rin(arr1, arr2) {\n  return Array.from(arr1, (x) => {\n    return arr2.indexOf(x) == -1 ? false : true\n  })\n}\n\nfunction which(x) {\n  return x.reduce(\n      (out, bool, index) => bool ? out.concat(index) : out, \n      []\n    )\n}"
  },
  {
    "objectID": "posts/rise_of_comp/index.html",
    "href": "posts/rise_of_comp/index.html",
    "title": "The rise of computational works 1",
    "section": "",
    "text": "These days there’s a computational version of everything. Computational biology, computational musicology,computational archaeology,and so on, ad infinitum. Even movies are going digital. Dan Jurafsky, 2006\n\nAlready in 2006, there was a feeling that science was becoming more and more computational. But the rise of computational works is not evenly distributed. While some fields have a long history of using computer science to meet their computational needs, others are concerned that the process of representing their objects of study in a digital format may distort them. Yet others think that programming, say in the humanities, is mostly about acquiring marketable skills that transfer to industry, and as such it goes against long-term values of humanists.\nUnlike previous technologies, programming is able to creep into any field as it permeates many facets of scientific work, e.g. statistics, communication, visualization, simulation, data collection, etc. While it is true that some objects of study are more challenging to represent on computers, the rate at which computational methods are nevertheless being adopted by computational enthusiasts could create a wedge between them and more reluctant fields. This imbalance might even lead to a situation where the adoption of computational tools gives certain individuals or groups a disproportionate amount of influence, especially in fields undergoing a significant shift towards computational works. For example, there is evidence that institutions with more resources to fund larger research groups already increase disproportionately faculty productivity. What if institutions with a greater labor advantage also benefit the most from this computational turn. Those who embrace computational tools may have an advantage in terms of visibility and funding, potentially allowing them to more easily disseminate their ideas.\nA strong disparity in methods might have important consequences on the evolution of ideas in science. We ask ourselves, is the rise of computational work is a source of epistemic inequality? Does it benefit groups already favored because of their greater labor resources and institutional prestige? If the cost of learning programming is low enough for some individuals, and those individuals cluster together, will we see a gap forming even within the field? Will field of studies that are more computational spill over into other fields?\nTo assess whether the rise of computational work has an effect on the evolution of ideas in science, we need to be able to identify computational works. Without this first step, we won’t be able to measure the effects of groups adopting computational methods, or how labor advantages interact with the rise of programming. There are surprisingly few studies quantifying the relative adoption of computational works in different fields. Recent advances in the availability of large-scale data and NLP tools make this possibility more accessible than ever before.\nWe define computational works as projects that seek to understand complex systems using visualizations, simulations, and/or inference processes that require computers and programming languages. Here a few examples, slightly biased from my experience, in chronological order:\n\nplot = Plot.plot({\n  y: {ticks: null},\n  width,\n  marks: [\n    Plot.text(\n      data_manual,\n      Plot.dodgeY({\n        x: \"date\",\n        text: \"author\",\n        title: d => `${d.author} (${d.date.getFullYear()}):\\n${d.desc.slice(0, 130).concat('\\n', d.desc.slice(130))}`,\n        r: 30,\n        fill: \"category\",\n        lineWidth: 7,\n        anchor: \"middle\"\n      })\n    ),\n    Plot.ruleY([0], {dy: 175}),\n    Plot.ruleX(d3.range(1960,2020,5), {x:d => (new Date(`${d}-01-01`)), opacity: 0.3, lineType: 'dashed'})\n  ]\n})\n\nplot.legend(\"color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll of these papers share the fact that they could not have been done without extensive computers and related computer skills. By extensive, I mean that these papers wouldn’t be doable without computers and programming skills because it would require too much time or effort.1 In cases where computer science and mathematics have an application, or are used in a field, it typically requires skills that go beyond a traditional curriculum. For example, Burrows’ analysis of Jane Austen’s text required skills that go beyond the literary studies curriculum.\nNote that some articles are computational in the sense that they deal with computational stuff, but they remain conceptual. We include them because papers with computational methods ought to cite them.\nOk, so how are we gonna classify computational works? Here is the plan schema:\n\nFirst, we look at articles containing the computational keyword, which will include variations of it because s2_search is a kind of elastic search. This is imperfect but it should be better than asking for digital or programming. Then, we project these paper embeddings into lower dimensions with umap. Finally, we use hdbscan, a density-based clustering algorithm, to cluster similar papers.\nThat’s it. The hope is that because document embeddings are informed by title, abstract, and citations, papers that are more computational ought to cluster together. We will evaluate the performance of our pipeline on the subset of papers that we manually identified as computational while reviewing the literature.\nWe already did some of that, which you can find here.\n\n\n\n\n\n\nNotes on the pipeline:\n\n\n\n\nWe don’t use full texts at the moment, but we could.\nWe place a lot of hope on Spectre embeddings, as we expect to distinguish between types of computational papers based on citation patterns.\nWe’re going to have a lot of false positives and negatives as a result of the first step. This should be improved.\n\n\n\n\nRefs for the methods:\n\nallenai/s2orc (paper)\nallenai/s2search (blog post)\nallenai/specter (paper)\nWe based our specter2top step on ddangelov/Top2Vec (paper)\n\numap\nhdbScan\n\n\n\ndata_manual = [\n  {'author': 'Lorenz', 'date': new Date('1963-01-01'), 'desc': 'Determining nonperiodic flow by way of numerical solution of convection equations', 'category': 'Numerical simulations'},\n  {'author': 'Gillepsie', 'date': new Date('1976-01-01'), 'desc': 'Simulating the stochastic time evolution of coupled chemical reactions using the Monte Carlo simulation procedure.', 'category': 'Numerical simulations'},\n  {'author': 'Busa', 'date': new Date('1980-01-01'), 'desc': ' Counting words from Thomas Aquinas body of work to better understand Aquinas his philosophical assumptions and truths.', 'category': 'Digital Humanities'},\n  {'author': 'Diaconis & Efron', 'date': new Date('1983-01-01'), 'desc': 'Use of the bootstrap to estimate the correlation coefficient from the data, thus replacing the traditional statistical procedures making assumptions of normality on the data.', 'category': 'Statistical Inference'},\n  {'author': 'Burrows', 'date': new Date('1989-01-01'), 'desc': ' A statistical analysis, or computer-assisted analysis, of literary texts using frequency analysis.', 'category': 'Digital humanities', 'field': 'Literature'},\n    {'author': 'Gelman & Rubin', 'date': new Date('1992-01-01'), 'desc': ' The use and pitfalls of using the Gibbs sampler to summarize multivariate distributions underlying data analysis.', 'category': 'Statistical Inference'},\n  {'author': 'Papert', 'date': new Date('1996-01-01'), 'desc': 'Computational thinking is a popular idea in education of how computers can be the basis of a new type of education in society and science.', 'category': 'Computational thinking', 'field': 'Education'},\n  {'author': 'Tisue & Wilenski', 'date': new Date('2004-01-01'), 'desc': 'Studying complex systems using multi-agent programming languages.', 'category': 'ABMs'},\n  {'author': 'Adamic & Glance', 'date': new Date('2005-01-01'), 'desc': 'Analyzing blog posts from political bloggers as a way to better understand the role of polarization between conservatives and liberals in the U.S Presidential Election of 2004.', 'category': 'Computational Social Science', 'field': 'Political Science'},\n  {'author': 'Kossinets & Watts', 'date': new Date('2006-01-01'), 'desc': 'Analyzing large social networks using tools from network theory.', 'category': 'Computational Social Science'},\n  {'author': 'Hall, Jurafsky & Manning', 'date': new Date('2008-01-01'), 'desc': 'Understanding the evolution of ideas in the field of computational linguistics using topic modeling on all of the ACL anthology', 'category': 'Computational Linguistics', 'field': 'SciSci'},\n  {'author': 'Michel et al.', 'date': new Date('2011-01-01'), 'desc': 'Google books.', 'category': 'Computational Social Science', incorporated: true},\n  {'author': 'Grimmer & Steward', 'date': new Date('2013-01-01'), 'desc': 'It is also an example of replicable work, where the code and data are available on the Harvard Dataverse.', 'category': 'Computational Social Science', 'field': 'Political Science', reproducible: true},\n  {'author': 'Recasens, Danescu-Niculescu-Mizil, & Jurafsky', 'date': new Date('2013-01-01'), 'desc': 'TODO', 'category': 'Computational Linguistics', incorporated: true},\n  {'author': 'Barbera', 'date': new Date('2015-01-01'), 'desc': 'It is also an example of replicable work, where the code and data are available on GITHUB.', 'category': 'Computational Social Science', 'field': 'Political Science', reproducible: true, github: true}\n  ]\n\n\n\n\n\n\n\nimport {Plot} from \"@mkfreeman/plot-tooltip\"\n\n\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\nThere was a time when women were the computers. The original Monte Carlo algorithm was calculated by them during WWII. The calculations were simple enough that it was technically possible for humans to perform them. The trajectory of the first Apollo mission was also computed by women. But as we get deeper into the second half of the 20th century, this is no longer the case↩︎"
  },
  {
    "objectID": "posts/interactive-stats/index.html",
    "href": "posts/interactive-stats/index.html",
    "title": "CDAE stats",
    "section": "",
    "text": "The equation:\n\\[n^* = \\frac{n_0}{1 + \\frac{n_0}{N}}\\]\nwhere \\(n_0 = z_\\alpha^2 \\frac{S^2}{D^2}\\), \\(S^2\\) is our population variance, \\(D^2\\) is the difference between the true value and the estimated value, and \\(z_\\alpha^2\\) is the \\(z\\) value at a given confidence interval.1\n\n\n\nWe know…\n\nThere are 5,000 nonprofits in the city of reference\nFrom a previous study, we know that the the mean value of using new tools is $3,000. We also know from previous studies that the s.d. of this is $3,500.\n\nWe want…\n\nAn error rate of 10%\nA confidence interval of 95%\n\n\n\nfunction calc_n_0(z_alpha_sq, S, D) {\n    return z_alpha_sq * (S**2 / D**2)\n}\n\nfunction effective_sample_size(z_alpha_sq, S, D, N) {\n    const n_0 = calc_n_0(z_alpha_sq, S, D)\n    return +(n_0 / (1 + (n_0 / N))).toFixed(1)\n}\n\nfunction ci2z(ci) {\n     if (ci === \".68\") {\n        return 1\n     } else if (ci === \".95\") {\n        return 2\n     } else if (ci === \".99\") {\n        return 3\n     }\n}\n\nviewof conf_int = Inputs.radio([\".68\", \".95\", \".99\"], {value: \".95\", label: \"Conf. interval\"})\nviewof error_rate = Inputs.range([0.05, 1], {value: 0.1, step: 0.05, label: \"Error rate\"})\nviewof N = Inputs.range([0, 100000], {value: 5000, step: 1000, label: \"N\"})\nviewof prev_mean = Inputs.range([0, 10000], {value: 3000, step: 500, label: \"Prev mean\"})\nviewof prev_std = Inputs.range([0, 10000], {value: 3500, step: 500, label: \"Prev std\"})\nz_alpha_sq = ci2z(conf_int)**2\n\nS = prev_std\nD = error_rate * prev_mean\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, \\(S^2\\) is the previous standard deviation squared and \\(D\\) is the wanted error rate times the previous mean, that is,  x  = .\nWe find that the minimum adequate sample size, or \\(n^*\\):\n\\(n^0\\) = \n\\(n^*\\) =  / (1 +  / ) = \n\\(n_0/N\\) = \nAlso, we saw in class that \\(n^*\\) converges around \\(600\\), with the default parameters. That is, adding more data does not entail a higher \\(n^*\\). You can observe that fact with the following plot:\n\nxs = [...Array(N).keys()];\nys = xs.map(x => effective_sample_size(z_alpha_sq, S, D, x))\nPlot.lineY(ys).plot({height: 200, width: 300, y: {label: \"↑ n*\"}, x: {label: \"N →\"}})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut you can play around with other settings to see how it varies."
  },
  {
    "objectID": "posts/interactive-stats/index.html#power-analysis",
    "href": "posts/interactive-stats/index.html#power-analysis",
    "title": "CDAE stats",
    "section": "Power analysis",
    "text": "Power analysis\nSee Patrick Mineault notebook"
  },
  {
    "objectID": "posts/interactive-stats/index.html#the-many-lives-of-statistical-tests",
    "href": "posts/interactive-stats/index.html#the-many-lives-of-statistical-tests",
    "title": "CDAE stats",
    "section": "The many lives of statistical tests",
    "text": "The many lives of statistical tests\nSometimes I feel that the popularity of statistical testing is about outsourcing statistical work of busy scientists to flow charts. In research methods courses focusing on statistical testing I feel there is an understanding that these are limited, but given time and interest of students, it’s better than nothing. And if you’re sticking to experimental setups, that might be all you need. I am not going to do a rant. But I want to supplement the usual search method class with alternative perspectives explained as simply as possible:\n\nThe Frequentist approach. This if often the first encounter with inferential statistics in social socience. As long as you are in an experimental set-up this might be fine. You need to think about probability as long-run probability.\nThe linear models approach. Instead of starting from statistical tests, we start from linear models and explain which models map onto which tests. This approach promotes flexibility at the costs of having to learn the underlying ideas of linear models.\nHypothesis testing but Bayesian. No need to remember the nonsense that we “fail to reject the null” and that 0.95 confidence interval does not mean that we are “95% confident that our results are significant”.\nThe Bootstrap approach. This is a great coding exercice and saves you time from remembering all the different tests.\n\nNote that we use the following emojis to encode data types:\n\n💡 : Yes/no, 2 levels, success/failure, bias/fair… nominal data.\n📊 : Yes/no/maybe, >2 levels, might be ordinal or nominal.\n📏 : continuous/scalar/uncountable data.\n\n\n💡 ~ 💡📊 ~ 💡📏 ~ 💡\n\n\n\nNHST\n\n\nR code\na <- chisq.test(d_mat) # p-value > 0.05\n\n\n\n\nLinear models\n\n\nR code\n# Using glm to do a log-linear model\nfull = glm(n ~ early_first_line * sex, family = poisson(), data = d_long) \nb = anova(full, test = 'Rao') #  similar to our two-way ANOVA\n\n\n\n\nSummary\n\n\n# A tibble: 2 × 2\n  p.value model     \n    <dbl> <chr>     \n1   0.927 chisq.test\n2   0.647 glm       \n\n\n\n\n\n\nNHST\n\n\nR code\na <- chisq.test(d_mat) # p-value > 0.05\n\n\n\n\nLinear models\n\n\nR code\nfull = glm(n ~ self_id_as_coder * sex, family = poisson(), data = d_long) # log-linear model\nb = anova(full, test = 'Rao') #  similar to our two-way ANOVA\n\n\n\n\nSummary\n\n\n# A tibble: 2 × 2\n  p.value model     \n    <dbl> <chr>     \n1  0.0235 chisq.test\n2  0.0235 glm       \n\n\n\n\n\n\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\nWarning: The `origin` argument of `stat_bin()` is deprecated as of ggplot2 2.1.0.\nℹ Please use the `boundary` argument instead.\n\n\n\n\n\n\n\n\n\nNHST\n\n\nLinear models\n\n\nSummary\n\n\n# A tibble: 3 × 5\n    p.value estimate conf.low conf.high model \n      <dbl>    <dbl>    <dbl>     <dbl> <chr> \n1 0.163        -17.9    -43.4      7.73 t.test\n2 0.0000119     NA       NA       NA    glm   \n3 0.163         NA       NA       NA    glm"
  },
  {
    "objectID": "posts/mongoDB/index.html",
    "href": "posts/mongoDB/index.html",
    "title": "How to MongoDB",
    "section": "",
    "text": "To work with mongoDB, I highly recommend using the mongoDB Compass, mongossh and pymongo.\nKey refs:"
  },
  {
    "objectID": "posts/mongoDB/index.html#basics",
    "href": "posts/mongoDB/index.html#basics",
    "title": "How to MongoDB",
    "section": "Basics",
    "text": "Basics\nConnect to papersDB\n\nmongoshpymongo\n\n\n[direct: mongos] test&gt; use('papersDB');\n\n\nfrom pymongo import MongoClient\nuri = f\"mongodb://cwward:{pw}@wranglerdb01a.uvm.edu:27017/?authSource=admin&readPreference=primary&appname=MongoDB%20Compass&directConnection=true&ssl=false\"\nclient = MongoClient(uri)\ndb = client[\"papersDB\"]\n\n\n\nLooking at collections within our DB\n[direct: mongos] test&gt; show collections;\nLook at already existing indexes\n\nmongoshpython\n\n\n[direct: mongos] papersDB&gt; db.metadata.getIndexes()\n\n\ndb.metadata.index_information()\n\n\n\nestimate total count documents…\n[direct: mongos] papersDB&gt; db.metadata.estimatedDocumentCount()\nor count documents based on a particular query:\n[direct: mongos] papersDB&gt; db.metadata.countDocuments({year: 2018})\nyou can ‘explain’ queries. Super useful to understand how mongoDB works and query performances.\n[direct: mongos] papersDB&gt; var exp = db.metadata.explain(\"executionStats\")\n[direct: mongos] papersDB&gt; exp.find({title: \"Scale-free networks are rare\"}) //  totalDocsExamined: 19,786,006\n[direct: mongos] papersDB&gt; db.metadata.createIndex({year: -1}); // create index based on \n[direct: mongos] papersDB&gt; exp.find({title: \"Scale-free networks are rare\", year: 2018}).limit(1) // executionTimeMillis: 3851; totalKeysExamined: 786,497\ndropping indexes\n\nmongoshpython\n\n\n[direct: mongos] papersDB&gt; db.metadata.dropIndex(\"year_-1\")\n\n\ndb.metadata.drop_index(\"year_-1\")"
  },
  {
    "objectID": "posts/mongoDB/index.html#useful-queries",
    "href": "posts/mongoDB/index.html#useful-queries",
    "title": "How to MongoDB",
    "section": "Useful queries",
    "text": "Useful queries\n1-find papers based on paper_id\n\nmongoshpython\n\n\n[direct: mongos] papersDB&gt; db.pdf_parses.findOne({ paper_ID: \"77497072\"});\n\n\ndb.metadata.find_one({ \"paper_ID\": \"77497072\"})\n\n\n\n2-find papers based on paper_id and year\n[direct: mongos] papersDB&gt; db.metadata.findOne({ year: {$gt: 2015, $lt: 2022}, paper_id: \"f1b4361a1978e93018c5fdfe4856250152676ffb\" })\n3-Query papers with body_text see stack overflow\n[direct: mongos] papersDB&gt; db.pdf_parses.findOne({ body_text: { $gt: true, $type: 'array', $ne: [] }})\n4-Query authors in an array\n\nmongoshsql\n\n\n[direct: mongos] papersDB&gt; db.metadata.findOne({ 'authors.0.first': \"Aaron\" })\n[direct: mongos] papersDB&gt; db.metadata.findOne({ 'authors.first': \"Aaron\" })\n[direct: mongos] papersDB&gt; db.metadata.findOne({ 'authors.last': \"Clauset\" })\n[direct: mongos] papersDB&gt; db.metadata.aggregate({ $filter : { authors.last : { $eq : \"Clauset\" } } });\n\n\nSELECT * FROM metadata\nWHERE authors.last = \"Clauset\";\n\n\n\n5-Query concept_oa based on common ancestors\n\ncompass\n\n\n{$and: [{'ancestors.display_name': 'Ecology'}, {'ancestors.display_name': 'Computer science'}]}\n\n\n\n\nRegex queries\n1-Testing regex with sample in a pipeline\ndb.s2orc.aggregate([{ $sample: { size: 1 } }, { $addFields: { result: { $regexFindAll: { input: '$content.text', regex: /data/i } } } }])\ndb.s2orc.find({\n   content.text: {\n      $regex: /data availability statement/i\n   }\n})"
  },
  {
    "objectID": "posts/mongoDB/index.html#creating-index",
    "href": "posts/mongoDB/index.html#creating-index",
    "title": "How to MongoDB",
    "section": "Creating index",
    "text": "Creating index\nCreate index based on descending year\n[direct: mongos] papersDB&gt; db.metadata.createIndex({year: -1});\nFrom Percona, this allows to improve all the queries that find documents with a condition and the year field, like the following:\n[direct: mongos] papersDB&gt; db.metadata.find( { year : 2018 } ) \n[direct: mongos] papersDB&gt; db.metadata.find( { title : \"Scale-free networks are rare\", year : 2018 } )\n[direct: mongos] papersDB&gt; db.metadata.find( { year : { $gt : 2020} } )\n[direct: mongos] papersDB&gt; db.metadata.find().sort( { year: -1} ).limit(10)\nCreate index based on authors (Multikey indexes)\n[direct: mongos] papersDB&gt; db.metadata.createIndex( { authors: 1 } )\n[direct: mongos] papersDB&gt; db.metadata.find( { authors.last: \"Clauset\" } )\nCreate index based on year and has_body_text (include a Partial indexes and Unique) In order for the partial index to be used the queries must contain a condition on the year and body_text field.\n[direct: mongos] papersDB&gt; db.metadata.createIndex(\n   { \"paper_id\": 1 },\n   { unique: true },\n   { partialFilterExpression: { year : { $gt: 2018 }, body_text: { $gt: true, $type: 'array', $ne: [] } } }\n)\n\n[direct: mongos] papersDB&gt; db.metadata.find( { paper_id: \"77490322\", year: { $gt: 2018}, body_text: { $gt: true, $type: 'array', $ne: []} } )\nCreate index based on year (asc) and bounded by 1950-60\n[direct: mongos] papersDB&gt; exp.find({\"year\": {$gte: 1950, $lte: 1960}, \"paper_id\": \"77490322\"}).limit(1) // executionTimeMillis: 360429; totalKeysExamined: 2024098\n[direct: mongos] papersDB&gt; db.metadata.createIndex({year:1}, { partialFilterExpression: { year : { $gte: 1950, $lte: 1960 } } });\n[direct: mongos] papersDB&gt; exp.find({\"year\": {$gte: 1950, $lte: 1960}, \"paper_id\": \"77490322\"}).limit(1) // executionTimeMillis: 68676; totalKeysExamined: 406162\nCreate index with partialFilterExpression\n\npython\n\n\n# We use \"$type\" because \"$ne\" not supported when creating PFE\ndb.metadata.create_index(\n [(\"year\", ASCENDING)], \n name=\"bucket 1950-1960\", \n partialFilterExpression={ \"year\" : { \"$gte\": 1950, \"$lte\": 1960 }, \"abstract\": {\"$type\": \"string\"} }\n)\nWith this one, we get to totalDocsExamined: 79,721 examined (v. totalDocsExamined: 720,475)."
  },
  {
    "objectID": "posts/mongoDB/index.html#text-queries",
    "href": "posts/mongoDB/index.html#text-queries",
    "title": "How to MongoDB",
    "section": "Text queries",
    "text": "Text queries\nTo do text queries, you must create an index first:\n[direct: mongos] papersDB&gt; db['publication-venues'].createIndex( { name: \"text\" } )\nthen you can query as follow\n[direct: mongos] papersDB&gt; db['publication-venues'].find({ $text: {$search: \"ecology\", $caseSensitive: false} }).limit(1)"
  },
  {
    "objectID": "posts/mongoDB/index.html#updating-documents",
    "href": "posts/mongoDB/index.html#updating-documents",
    "title": "How to MongoDB",
    "section": "Updating documents",
    "text": "Updating documents\n1- Update s2fos\n\nmongoshpython\n\n\ndb.metadata.updateOne({paper_id: '84881204', year: {$gte: 1950, $lte: 1960}}, {'$set': {'s2fos_field_of_study': ['Medicine']}}})\n\n\nq = {\"paper_id\": '84881204', \"year\": { \"$gte\": 1950, \"$lte\": 1960 }}\nnew_values = {\"$set\": { \"s2fos_field_of_study\": ['Medicine']} }\ndb.metadata.update_one(q, new_values)\n\n\n\n2- Remove a field\ndb.metadata.updateOne({paper_id: '84881204', year: {$gte: 1950, $lte: 1960}}, {$unset: {s2fos: \"\"}})"
  },
  {
    "objectID": "posts/mongoDB/index.html#useful-aggregated-queries",
    "href": "posts/mongoDB/index.html#useful-aggregated-queries",
    "title": "How to MongoDB",
    "section": "Useful aggregated queries",
    "text": "Useful aggregated queries\nFind duplicated rows (not sure it is working yet)\n[direct: mongos] papersDB&gt; const aggregation = [\n    {\"$group\" : { \"_id\": \"$paper_id\", \"count\": { \"$sum\": 1 } } },\n    {\"$match\": {\"_id\" :{ \"$ne\" : null } , \"count\" : {\"$gt\": 1} } }, \n    {\"$project\": {\"paper_id\" : \"$_id\", \"_id\" : 0} }\n]\n\n[direct: mongos] papersDB&gt; db.pdf_parses.aggregate(aggregation);\n\nLookups is an aggregate query (first way of doing it)\n[direct: mongos]db.s2orc.aggregate([\n   { $lookup: {\n      from: \"papers\", localField: \"corpusid\", foreignField: \"corpusid\", as: \"paper_metadata\"\n      } }\n   ])\n\n\nSetting a new field based on old field\n# here we add https://doi.org/ to externalids.DOI to facilitate lookup\n# with works_oa. \ndb.papers.update_many(\n    {},\n    [\n        { \"$set\": { \n            \"doi\": { \n                \"$cond\": [\n                    { \"$ne\": [\"$externalids.DOI\", None] },\n                    { \"$concat\": [\"https://doi.org/\", \"$externalids.DOI\"] },\n                    None\n                ]\n                }\n            } }\n    ]\n)"
  },
  {
    "objectID": "posts/mongoDB/index.html#document-embeddings",
    "href": "posts/mongoDB/index.html#document-embeddings",
    "title": "How to MongoDB",
    "section": "Document embeddings",
    "text": "Document embeddings\nEmbed one collection into a second collection not sure it is working yet, this is a chatgpt answer\n[direct: mongos] papersDB&gt; db.collection1.update({name: \"John Doe\"}, {$set: {address: db.collection2.findOne({address: \"123 Main St\"})}})"
  },
  {
    "objectID": "posts/data_statements/index.html",
    "href": "posts/data_statements/index.html",
    "title": "Data Statements Topics",
    "section": "",
    "text": "Plot.plot({\n  width: 800, \n  y: {grid: true},\n  marks: [\n    Plot.barY(transpose(data_barchart), {x: \"topic_nums\", y: \"topic_sizes\"}),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\nbeta_df_t = transpose(beta_df)\nInputs.table(beta_df_t)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe are going to search by topic , a topic that appears to be about ?.\n\n\n\n\n\n\n\n\nWe choose the top  topics most similar to to availability, as in data availability statement. You can choose one of them:\n\ndf_search_doc_by_topic_t = transpose(df_search_doc_by_topic)\ntopic_available = df_search_doc_by_topic_t.map(d=>d.topic_nums)\nviewof sel_top = Inputs.select(topic_available)\ndf_search_doc_by_topic_tf = df_search_doc_by_topic_t.filter(d => d.topic_nums == sel_top)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe key words that compose topic  are :\n\ndf_search_doc_by_topic_tf.map(d => d.topic_words)[0]\n\n\n\n\n\n\n\ntopic_size = transpose(data_barchart).filter(d => d.topic_nums == sel_top)[0]['topic_sizes']\ntopic_score = df_search_doc_by_topic_tf.map(d => d.topic_scores)[0]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe cosine similarity with main keyword is . There are  documents associated with this topic. Here are a few examples:\n\nmd`${'\\n - ' + df_search_doc_by_topic_tf.map(d => d.documents)[0].join('\\n - ')}`"
  },
  {
    "objectID": "posts/data_statements/index.html#searh-documents-by-keywords",
    "href": "posts/data_statements/index.html#searh-documents-by-keywords",
    "title": "Data Statements Topics",
    "section": "Searh documents by keywords",
    "text": "Searh documents by keywords\nSearch documents for content semantically similar to availability and github.\n\n\nDocument: 2115, Score: 0.6027655601501465\n-----------\nPerceval retrieves **data** from repositories.\n-----------\n\nDocument: 2142, Score: 0.5959838628768921\n-----------\nFLOSSMetrics (Herraiz et al., 2009) and SQO-OSS (\"Software quality observatory for open source software\" (Gousios et al., 2007)) had similar aims: to collect not only metadata about the projects, but as much **data** as possible about software development (the complete list of commits or issues, for example), via the APIs provided by software forges. Both produced their own software to that aim (MetricsGrimoire and Alitheia Core, mentioned below), which they used to collect **data** from hundreds of software repositories (commit records, issue reports, code review discussions, asynchronous communication via mailing lists, etc.), being some of the first demonstrators of how massive retrieval of **data** from software forges could be performed. Both worked with a diverse collection of **data** sources. FLOSSMetrics and SQO-OSS started the path towards automated collection of many different **data** kinds about software development, which is also the goal of GrimoireLab. Many of the features provided by GrimoireLab were first demonstrated, or at least set as a long-term goal, by those systems: retrieval of **data** from many different **data** sources; automated retrieval, storage, and analysis; fault-tolerance; massive collection and analysis; etc. The tools provided by GrimoireLab could be composed to produce systems similar to FLOSSMetrics and SQO-OSS, and GrimoireLab owes these systems the architecture based on storing **data** for further analysis (instead of having to retrieve it once and again from the original **data** sources), and the idea of incremental retrieval, fundamental for efficient retrieval of **data** from repositories already visited. In all of them, the basic idea was to interfere as little as possible with the infrastructure provided by the original **data** sources. GHTorrent (Gousios & Spinellis, 2012) and GHArchive (Grigorik, 2022) were developed later, focusing on the GitHub platform. Both work by querying the GitHub API to produce a complete dataset including most of the events noticeable in it (code commit records, issue reports, pull requests, changes in repository metadata). Both developed their own software for the retrieval and curation of the data. In the case of GHTorrent, curation includes linking actors of events to GitHub users, and adding metainformation provided by the GitHub repositories API (such as number of stars or programming languages used), that make the dataset more valuable. Given that both are focused on GitHub, both have specialized components tailored to optimize the retrieval of **data** from the GitHub events channel.\n-----------\n\nDocument: 2076, Score: 0.5958185791969299\n-----------\nGrimoireLab pipelines usually start by retrieving **data** from some software development repository. This involves accessing the APIs of the services managing those repositories (such as GitHub, Stack Overflow or Slack, San Francisco, California, USA), or using external tools or libraries to directly access the artifacts (such as Git repositories or mailing list archives). In the specific case of source code management repositories, some tools may also be run to obtain metrics about the source code. For large-scale retrieval, work is organized in jobs that have to be scheduled to minimize impact on the target platform, and to maximize performance. GrimoireLab provides three components for dealing with these issues:\n-----------\n\nDocument: 2122, Score: 0.5879751443862915\n-----------\nGrimoireLab setup: The setup corresponds to the description of the research scenario \"Large-scale, continuously updated dataset\", described in Subsection \"Research scenario: Large-scale, continuously updated dataset\" (Fig. 16), although in this case the **data** retrieval was performed once, and not updated later. All **data** retrieval and analysis was done in a single thread. Everything was run by Mordred, which started with **data** retrieval in two threads: one cloning and then extracting metadata from Git repositories (for all commits in all of them), the other one accessing the GitHub API to retrieve issues and pull requests for all repositories, using three API tokens. In each of the threads, once the retrieval for all repositories is complete, with the production of the corresponding raw index, the analysis of the retrieved **data** starts, until all the items (commits in the case of Git repositories, issues and pull requests in the case of the GitHub API) are analyzed. Table 2 shows when the most relevant stages of this case started and finished, and their duration. The deployment was in a 2.5 GHz CPU with 4 cores, 8 GB of RAM, SSD storage. In both tables, \"git\" refers to the analysis of Git repositories, \"github\" to the analysis of GitHub issues and pull requests retrieved from the GitHub API. Data collection for \"git\" includes cloning of Git repositories for GitHub, and production of the raw index by analyzing those clones. Data collection for \"github\" includes waiting periods while the API tokens are exhausted, and calls to the API to resolve identities, not only retrieval of issues and pull requests. Two API tokens were used in this case.\n-----------\n\nDocument: 2152, Score: 0.5775830745697021\n-----------\nThe main characteristics of GrimoireLab which make it unique when compared to other tools to analyze software development repositories are: Support of many different **data** sources (close to 30).\n-----------"
  },
  {
    "objectID": "posts/data_statements/index.html#similar-keywords",
    "href": "posts/data_statements/index.html#similar-keywords",
    "title": "Data Statements Topics",
    "section": "Similar keywords",
    "text": "Similar keywords\n\n\nrepositories 0.7654658071607169\napi 0.7071680645759014\nretrieval 0.6583115337530652\nrepository 0.592823609766448\nidentities 0.5535036804899642\ngrimoirelab 0.5329156734310911\nissues 0.5177149656112502\nprojects 0.46969717463892435\nraw 0.46540759490694067\nindexes 0.4522078509419137\ntools 0.4214162388579166\nsources 0.41057411225151663\nissue 0.4036555673329467\nusers 0.3967289205090325\netc 0.3924877730479528\nsource 0.3865059159763369\nitems 0.3838340690293331\nrun 0.36278969910205416\ncode 0.36137198561001427\nlater 0.35694208686454587"
  },
  {
    "objectID": "posts/allotax-in-quarto/index.html",
    "href": "posts/allotax-in-quarto/index.html",
    "title": "ALLotaxonometer",
    "section": "",
    "text": "viewof form = Inputs.form(\n  [\n    Inputs.select(d3.range(elem_names.length), {label: \"System 1\", multiple: true, multiple: 3, value: [0], format: x => elem_names[x]}),\n    Inputs.select(d3.range(elem_names.length), {label: \"System 2\", multiple: true, multiple: 3, value: [1], format: x => elem_names[x]}),\n  ],\n  {\n    template: (inputs) => htl.html`<div style=\"display: flex; gap: 4em\">\n  <br>${inputs}\n</div>`\n  }\n)\n\nviewof alpha = Inputs.range([0, alphas.length-1], {step: 1, label: \"α\", format: x => alphas[x]})\n\nnext_button()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`\n<br>\n<div style=\"display:flex; align-items:center; gap: 10em; margin-left: -75px; justify-content: center; width: 100%; text-align: center; font-size: 22px; \">\n      <div>${ tex`\\Omega_1` }: ${ title(0) }</div> \n      <div>${ tex`\\Omega_2` }: ${ title(1) }</div>  \n</div>\n<div style=\"display:flex; gap: 28em; margin-top: -100px; justify-content: center;\">\n    <div id=\"diamondplot\"></div>\n    <div style=\"margin-top: 110px;\" id=\"wordshift\"></div>\n</div>\n<div style=\"display:flex; align-items:center; gap: 14em;  margin-left:-95px;justify-content: center; margin-top:-300px\">\n    <div id=\"legend\"></div>\n    <div id=\"balance\"></div>\n</div>\n`\n\n\n\n\n\n\n\n\nall = import('https://cdn.skypack.dev/allotaxonometer@1.1.9?min')\n\nelem = FileAttachment(\"data/elem_girls.json\").json()\nelem_names = Object.keys(elem)\nmutable clicks = 0\nsel_sys1 = elem_names[(form[0][0]+clicks) % elem_names.length]\nsel_sys2 = elem_names[(form[1][0]+clicks) % elem_names.length]\nelems1 = elem[sel_sys1]\nelems2 = elem[sel_sys2]\n\nme_class = new all.mixedElems(elems1, elems2)\n\nrtd = me_class.RTD(alphas[alpha])\ndat = me_class.Diamond(rtd)\n\ndiamond_dat = dat.counts\nwordshift = me_class.wordShift(dat)\nbalance_dat = me_class.balanceDat() \n\np1=all.DiamondChart(diamond_dat);\np2=all.WordShiftChart(wordshift, { height: 670 });\np3=all.BalanceChart(balance_dat);\np4=all.LegendChart(diamond_dat);\n\ntitle = (sys) => {\n  const out = sys == 0 ? sel_sys1 : sel_sys2\n  return out.replace(/.((c|t)sv|json)/i, \"\")\n}\n\nalphas = d3.range(0,18).map(v => +(v/12).toFixed(2)).concat([1, 2, 5, Infinity])\n\nnot_zero = tex`\\propto \\sum_\\tau \\Big| \\frac{1}{r_{\\tau,1}^{ ${ alphas[alpha] }} } - \\frac{1}{r_{\\tau,2}^{ ${ alphas[alpha] }}} \\Big| `\n\nzero = tex`\\propto \\sum_\\tau \\Big|\\ln \\frac{r_{\\tau,1}}{r_{\\tau,2}}\\Big| `\n\ninfinity = tex`\\propto \\sum_\\tau (1 - \\delta_{r_{\\tau,1}, r_{\\tau,2}}) \\times\\max\\Big\\{\\frac{1}{r_{\\tau,1}},\\frac{1}{r_{\\tau,2}} \\Big\\} `\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport { button } from \"@bartok32/diy-inputs\"\n\nnext_button = () => button({\n  title: \"\",\n  value: \"Next\",\n  desc: \"\",\n  buttonStyle: {\n    background: \"#7295FF\",\n    color: \"white\"\n  },\n  onclick: (objs) => {\n    mutable clicks += 1;\n\n    d3.select(objs.button)\n      .style(\"background\", \"#6786E5\")\n      .interrupt()\n      .transition()\n      .duration(300)\n      .style(\"background\", \"#7295FF\");\n\n    if (mutable clicks > 0 && objs.output === \"\") {\n      objs.output = d3\n        .select(objs.div)\n        .insert(\"a\", \"div.desc\")\n        .attr(\"class\", \"output\")\n        .style(\"margin-left\", \"5px\")\n        .style(\"font-size\", \"11px\")\n        .style(\"cursor\", \"pointer\")\n        .style(\"border\", \"0.5px solid black\")\n        .style(\"border-radius\", \"5px\")\n        .style(\"padding\", \"5px\")\n        .on(\"click\", function () {\n          this.remove();\n          objs.output = \"\";\n          mutable clicks = 0;\n        })\n        .html(\"RESET\");\n    }\n  }\n})"
  },
  {
    "objectID": "unlisted_posts/complaints/index.html",
    "href": "unlisted_posts/complaints/index.html",
    "title": "Airlines Complaints TripAdvisors",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/data_statements/index.html#d-embedding-interactive-visualization",
    "href": "posts/data_statements/index.html#d-embedding-interactive-visualization",
    "title": "Data Statements Topics",
    "section": "2d embedding interactive visualization",
    "text": "2d embedding interactive visualization\n\nviewof reset = html`<button>Reset`\nLegend(z, { title: \"Topics\" })\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nchart = {\n  const zoom = d3.zoom().scaleExtent([0.5, 12]).on(\"zoom\", zoomed);\n\n  const svg = d3.create(\"svg\").attr(\"viewBox\", [0, 0, width, height]);\n\n  const gGrid = svg.append(\"g\");\n  \n  const x = d3.scaleLinear()\n    .domain([extent_x[0]-1, extent_x[1]+1])\n    .range([0, width])\n\n  const y = d3.scaleLinear()\n    .domain([(extent_y[0] * k), (extent_y[1] * k)+8])\n    .range([height, 0])\n\n    \n  const yAxis = (g, y) => g\n    .call(d3.axisRight(y).ticks(12 * k))\n    .call(g => g.select(\".domain\").attr(\"display\", \"none\"))\n\n  const xAxis = (g, x) => g\n      .attr(\"transform\", `translate(0,${height})`)\n  \n  const gDot = svg.append(\"g\")\n      .attr(\"fill\", \"none\")\n      .attr(\"class\", \"circles\")\n      .attr(\"stroke-linecap\", \"round\");\n\n  gDot.append(\"style\").text(`\n    .circles {\n      stroke: transparent;\n      stroke-width: 0.1px;\n    }\n    .circles circle:hover {\n      stroke: black;\n    }\n  `);\n  \n  gDot.selectAll(\"circle\")\n    .data(data)\n    .join(\"circle\")\n      .attr(\"cx\", d => x(d.x))\n      .attr(\"cy\", d => y(d.y))\n      .attr(\"fill\", d => d.labels == -1 ? 'grey' : z(d.labels))\n      .attr(\"opacity\", d => d.labels == -1 ? 0.3 : 0.7)\n      .attr(\"r\", 1.5)\n    .append(\"title\")\n      .text(d => `${d.title} (${d.labels})\\ntext: ${d.text}`);\n\n    const gDotF = svg.append(\"g\")\n        .attr(\"fill\", \"none\")\n        .attr(\"class\", \"circles\")\n        .attr(\"stroke-linecap\", \"round\");\n\n    gDotF.append(\"style\").text(`\n    .circles {\n      stroke: transparent;\n      stroke-width: 0.5px;\n    }\n    .circles circle:hover {\n      stroke: black;\n    }\n  `);\n\n  const gx = svg.append(\"g\");\n  const gy = svg.append(\"g\");\n\n  svg.call(zoom).call(zoom.transform, d3.zoomIdentity);\n\n  function zoomed({transform}) {\n    const zx = transform.rescaleX(x).interpolate(d3.interpolateRound);\n    const zy = transform.rescaleY(y).interpolate(d3.interpolateRound);\n    gDot.attr(\"transform\", transform).attr(\"stroke-width\", 2 / transform.k);\n    gDotF.attr(\"transform\", transform).attr(\"stroke-width\", 2 / transform.k);\n    gx.call(xAxis, zx);\n    gy.call(yAxis, zy);\n    gGrid.call(grid, zx, zy);\n  }\n\n  const tooltip = d3\n    .select(\"body\")\n    .append(\"div\")\n    .style(\"position\", \"absolute\")\n    .style(\"visibility\", \"hidden\")\n    .style(\"opacity\", 0.9)\n    .style(\"background\", \"white\");\n  \n  return Object.assign(svg.node(), {\n    reset() {\n      svg.transition()\n          .duration(750)\n          .call(zoom.transform, d3.zoomIdentity);\n    }\n  });\n}\n\n\n\n\n\n\n\nz = d3.scaleSequential(d3.extent(data, d => d.labels), d3.interpolateSpectral)\n\n\n\n\n\n\n\ngrid = (g, x, y) => g\n    .attr(\"stroke\", \"currentColor\")\n    .attr(\"stroke-opacity\", 0.1)\n    .call(g => g\n      .selectAll(\".x\")\n      .data(x.ticks(12))\n      .join(\n        enter => enter.append(\"line\").attr(\"class\", \"x\").attr(\"y2\", height),\n        update => update,\n        exit => exit.remove()\n      )\n        .attr(\"x1\", d => 0.5 + x(d))\n        .attr(\"x2\", d => 0.5 + x(d)))\n    .call(g => g\n      .selectAll(\".y\")\n      .data(y.ticks(12 * k))\n      .join(\n        enter => enter.append(\"line\").attr(\"class\", \"y\").attr(\"x2\", width),\n        update => update,\n        exit => exit.remove()\n      )\n        .attr(\"y1\", d => 0.5 + y(d))\n        .attr(\"y2\", d => 0.5 + y(d)));\n\n\n\n\n\n\n\ndata = FileAttachment(\"clusteredDataStatements.csv\").csv({ typed: true })\nk = height / width\nheight = 600\nextent_x = d3.extent(data, d => d.x)\nextent_y = d3.extent(data, d => d.y)\nreset, chart.reset()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport {Legend, Swatches} from \"@d3/color-legend\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic distribution\n\nPlot.plot({\n  width: 800, \n  y: {grid: true},\n  marks: [\n    Plot.barY(transpose(data_barchart), {x: \"topic_nums\", y: \"topic_sizes\"}),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\nbeta_df_t = transpose(beta_df)\nInputs.table(beta_df_t)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSearch documents by topic\nWe are going to search by topic , a topic that appears to be about ?.\n\n\n\n\n\n\n\nWe are looking at topic with keywords similar to availability\nWe choose the top  topics most similar to to availability, as in data availability statement. You can choose one of them:\n\ndf_search_doc_by_topic_t = transpose(df_search_doc_by_topic)\ntopic_available = df_search_doc_by_topic_t.map(d=>d.topic_nums)\nviewof sel_top = Inputs.select(topic_available)\ndf_search_doc_by_topic_tf = df_search_doc_by_topic_t.filter(d => d.topic_nums == sel_top)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe key words that compose topic  are :\n\ndf_search_doc_by_topic_tf.map(d => d.topic_words)[0]\n\n\n\n\n\n\n\ntopic_size = transpose(data_barchart).filter(d => d.topic_nums == sel_top)[0]['topic_sizes']\ntopic_score = df_search_doc_by_topic_tf.map(d => d.topic_scores)[0]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe cosine similarity with main keyword is . There are  documents associated with this topic. Here are a few examples:\n\nmd`${'\\n - ' + df_search_doc_by_topic_tf.map(d => d.documents)[0].join('\\n - ')}`"
  },
  {
    "objectID": "posts/prog_and_union/index.html",
    "href": "posts/prog_and_union/index.html",
    "title": "Computer programming is not that different from unionization",
    "section": "",
    "text": "Labor unions are associations of workers who have demands aimed at improving their quality of life. From an organization point of view, the key challenge of unions is to organize themselves and recruit new members to be more effective when bargaining with the employers.\nUnions coevolve with union busting, similar to prey-predators dynamics. As a result, there is often an individual cost-benefit ratio for joining a union, which is determined by the union busting effort, the institutional capacities of groups to promote union behavior and the perceived importance of the cause by involved individuals.\nA key institutional capacity to advance union behavior is the ability to promote a sense of belonging by demonstrating the value of the cause to potentially community members.\nUnions also evolve as a form of cultural group selection, where successful tactics and strategies are copied among union groups. If a group is able to grow and reproduce, it is more able to persist over time. As with traditional cultural group selection, we might think about how competition among groups promote cooperation insofar as there is more variation within groups than across groups [to check that claim and cite relevant refs].\nIn summary, unions are a process with a cost-benefit ratio in which groups with more fitness will perform better.\nHow is all of this related to computer programming? Isn’t programming very personal:\nFor many scientists without computational or geek background, the perception about programming is that of the “lone hacker”. A lonely activity that engage you and your computer. Someone somewhere wrote something we call a programming language that you can download and use to run scripts. These scripts usually do something that scientists are interested in, like calculating the functional diversity index for ecologists or extracting word frequency from a relevant corpus for literary reviews.\nBut for other scientists who grew alongside the world of computer programming, they know that computer programming looks like the following:\nBehind the digital veil, there is often a community (ok, very often this is that one guy who did all the work, but even in this case communities can emerge from popular software) who wrote the code that you use. From the bottom of your hardware to the niche library that map onto scientific concepts, in passing by the scientific computing routines that most people use in whatever programming languages. In the Free and Open-Source Software world (F/OSS), this community is dedicated to write code that is free to use and modify. Programming in science is part of the computing world; it never is an individual activity even if it feels like it.\nIt is just that we don’t always see it, physically.\nWith this entry, I hope to convince you of the following:\nOk, wait. Am I saying that programming is always already a social activity? I hear many people responding that this is not their experience. Even if it is true that scientific software is written by someone else, does that mean the actual activity of coding has to be social too?\nWell, yes and no. This reminds me of the distinction between individual and social learning in the theory of cultural evolution. With our WEIRD brains, we tend to assume many skills are individually learned. For example, running is something you learned on your own. But in many other societies, running is a skill that is socially learn [REF], and arguably everytime they run they embody that knowledge. In our WEIRD societies, people don’t need to run in particular ways to survive. But invididuals who run as a serious hobby or professionally have learned at some point how to run. Thus, in particular modern niche, even running embody a social activity, even if you run by yourself. On top of this embodied perspective, running as a shared experience brings about collective gathering, subreddits, prestige and so on.\nI am claiming that something similar than running happen with respect to computer programming. As scientists, you can open your laptop, download R or Python, and learn to code individually. But most likely your code will be unintelligible to others and even to yourself in the near future. Learning to code is about copying others, building a mental model, and eventually tinkering with the code so that it feels natural.\nNote that you can be an outlooker of your programming community, without being actively involved in the social realm. But sooner or later, there are features or bugs that make sense only in the context of seeing programming as a group-based process, similar to that of an union.\nAs the social realm underlying programming is unveiled, the process of learning to program becomes somehow similar to that of that joining an union.\nA key similarity is that in both case, the process of becoming part of a community changes the new member worldview. As someone becomes convince that unionization is valuable, it is willing to pay stronger cost to defend the idea. As someone becomes a programmer that is actively involved in the open-source world, it changes her perspective on the unfolding history of computer programming and the internet. In both cases, new members learn a new language that reinforce the sense of community. This is not static though. As institutions get bigger, they can both become degenerate or corporate, which is another beast than earlier institutional phase. In both cases, as institutions get bigger, norms and goodwills are replaced by code of conducts and protocols. If the institutions is successful, people might join for other reasons than further the advancement of the cause. Power might become in the hand of a few privileged individuals at the top, administration and bureaucraties might add layers of complexities that make no sense, etc.."
  },
  {
    "objectID": "posts/prog_and_union/index.html#appendix-foss-success-stories-are-as-technical-as-they-are-communal",
    "href": "posts/prog_and_union/index.html#appendix-foss-success-stories-are-as-technical-as-they-are-communal",
    "title": "Computer programming is not that different from unionization",
    "section": "Appendix: F/OSS success stories are as technical as they are communal",
    "text": "Appendix: F/OSS success stories are as technical as they are communal\n\nLinux is making Apple Great Again\nHello Dolly: Democratizing the magic of ChatGPT with open models\nAlpaca: A Strong, Replicable Instruction-Following Model\nA one-year long research workshop on large multilingual models and datasets\nabout openalex"
  },
  {
    "objectID": "posts/mongoDB_opt/index.html",
    "href": "posts/mongoDB_opt/index.html",
    "title": "How to accelerate MongoDB",
    "section": "",
    "text": "We want to perform a $lookup from (s2orc) papersto works_oa (openAlex) to check how many DOIs match across collections.\n\n\n\n# Step in the aggregation pipeline that we'll use often\nreturn_top_5 = { \"$limit\": 5 }\nonly_keep_existing_matches = { \"$match\": { \"matches\": { \"$ne\": [] } } }\n\n\n\n\nIt is a good idea to start simple with simple collections.\nSchema from openalex database:\n\n\nCode\ndb.works_oa_test.insert_many( list(db.works_oa.find(\n    { \"publication_year\": 1960 },\n    { \"concepts\": 1, \"publication_date\": 1, \"doi\": 1, \"ids\": 1 }\n    ))\n)\n\ndb.paper_test.insert_many( list(db.papers.find(\n    { \"year\": 1960, \"s2fieldsofstudy\": { \"$type\": \"array\" } },\n    { \"externalids\": 1, \"s2fieldsofstudy\": 1 }\n    ))\n)\n\n\n\nprint(f\"S2orc has {db.paper_test.count_documents({})} papers with `DOI` in 1960\")\nprint(f\"OpenAlex has {db.works_oa_test.count_documents({})} papers with `DOI` in 1960\")\n\nS2orc has 408248 papers with `DOI` in 1960\n\n\nOpenAlex has 478673 papers with `DOI` in 1960\n\n\n\n\nWe want to match by doi. We start by exmamining the data:\n\npprint(db.paper_test.find_one({ \"externalids.DOI\": { \"$type\": \"string\" } }))\n\n{'_id': ObjectId('63e50688a64b9c3ca0a7a028'),\n 'doi': 'https://doi.org/10.3130/AIJSAXX.66.2.0_73',\n 'externalids': {'ACL': None,\n                 'ArXiv': None,\n                 'CorpusId': '125229466',\n                 'DBLP': None,\n                 'DOI': '10.3130/AIJSAXX.66.2.0_73',\n                 'MAG': '2752866858',\n                 'PubMed': None,\n                 'PubMedCentral': None},\n 's2fieldsofstudy': [{'category': 'Engineering', 'source': 's2-fos-model'},\n                     {'category': 'Geology', 'source': 'external'}]}\n\n\n\npprint(db.works_oa_test.find_one())\n\n{'_id': ObjectId('642044928238317efbef9371'),\n 'authorships': [],\n 'concepts': [{'display_name': 'Political science',\n               'id': 'https://openalex.org/C17744445',\n               'level': 0,\n               'score': 0.39196813,\n               'wikidata': 'https://www.wikidata.org/wiki/Q36442'}],\n 'doi': 'https://doi.org/10.6028/nbs.mp.237',\n 'ids': {'doi': 'https://doi.org/10.6028/nbs.mp.237',\n         'openalex': 'https://openalex.org/W4232940716'},\n 'mag': None,\n 'publication_date': '1960-01-01'}\n\n\nWe’re off to a bad start. We note that the s2orc DB don’t keep the address prefix of the DOI. We find easier to add the missing prefix in s2orc than remove it from openalex.\n\n\nCode\nstart_time = time.time()\ndb.paper_test.update_many(\n    {},\n    [\n        { \"$set\": { \n            \"doi\": { \n                \"$cond\": [\n                    { \"$ne\": [\"$externalids.DOI\", None] },\n                    { \"$concat\": [\"https://doi.org/\", \"$externalids.DOI\"] },\n                    None\n                ]\n                }\n            } }\n    ]\n)\nprint(\"Setting a field on ~400K took --- %s seconds ---\" % (time.time() - start_time))\n\n\nSetting a field on ~400K took --- 1.0850319862365723 seconds ---\n\n\nWe can now proceed to our first lookup. As a very first step, note that we will match only on existing dois on the paper_test (s2orc) side. If not, this is not even gonna run because the output is too large:\n\nstart_time = time.time()\n\ns2orc_doi_is_string = { \"$match\": { \"doi\": { \"$type\": \"string\" }} }\n\ns2orc_to_oa_doi_lookup ={\n      \"$lookup\": {\n         \"from\": \"works_oa_test\",\n         \"localField\": \"doi\",\n         \"foreignField\": \"doi\",\n         \"as\": \"matches\"\n      }\n   }\n\npipeline = [\n   s2orc_doi_is_string,\n   s2orc_to_oa_doi_lookup,\n   only_keep_existing_matches,\n]\n\nassert 'doi' in db.paper_test.find_one().keys(), 'missing field in paper test'\nassert 'doi' in db.works_oa_test.find_one().keys(), 'missing field in oa'\n\nres = list(db.paper_test.aggregate( pipeline ))\nprint(\"Process finished --- %s seconds ---\" % (time.time() - start_time))\n\nWe ended up cancelling the task because it was still too long (&gt; 3mins is too long for me). I just wanted to show this step to mention that when we work with mongoDB (our databases in general), indexes make a world of difference.\n\n\n\n\n\n\nTip\n\n\n\nAssigning descriptive names to variables for each aggregation stage enhances the reader’s comprehension of the purpose of the stage while emphasizing the similarities between the optimization variations being performed. For example, the example below clearly demonstrates that we are executing the same pipeline both with and without indexes.\n\n\n\n\n\n\n\nHere’s how we go about creating indexes\n\ndb.works_oa_test.create_index([(\"doi\", ASCENDING)])\ndb.paper_test.create_index([(\"doi\", ASCENDING)])\n\n'doi_1'\n\n\nMatching paper_test on works_oa again:\n\nstart_time = time.time()\n\ns2orc_doi_is_string = { \"$match\": { \"doi\": { \"$type\": \"string\" }} }\n\ns2orc_to_oa_doi_lookup ={\n      \"$lookup\": {\n         \"from\": \"works_oa_test\",\n         \"localField\": \"doi\",\n         \"foreignField\": \"doi\",\n         \"as\": \"matches\"\n      }\n   }\n\nres = list(db.paper_test.aggregate( [\n    s2orc_doi_is_string,\n    s2orc_to_oa_doi_lookup,\n    only_keep_existing_matches\n] ))\n\nprint(\"Process finished --- %s seconds ---\" % (time.time() - start_time))\nprint(len(res))\n\nProcess finished --- 24.946133136749268 seconds ---\n92287\n\n\nAlso, lets check if the other way around is faster\n\nstart_time = time.time()\n\noa_doi_is_string = { \"$match\": { \"doi\": { \"$type\": \"string\" }} }\n\noa_to_s2orc_doi_lookup = {\n      \"$lookup\": {\n         \"from\": \"paper_test\",\n         \"localField\": \"doi\",\n         \"foreignField\": \"doi\",\n         \"as\": \"matches\"\n      }\n   }\n\nres = list(db.works_oa_test.aggregate( [\n    oa_doi_is_string,\n    oa_to_s2orc_doi_lookup,\n    only_keep_existing_matches\n] ))\n\nprint(\"Process finished --- %s seconds ---\" % (time.time() - start_time))\nprint(len(res))\n\nProcess finished --- 28.506409168243408 seconds ---\n91704\n\n\nLooking up paper_test in works_oa took 28 seconds, while it took 30 seconds for the reverse direction. This make sense considering that paper_test contains fewer papers. If the information we are interested in resides in paper_test, it is better to prioritize that direction. For example, if our focus is on text analysis and we only have text data for papers in paper_test (S2ORC), then the additional papers in openAlex become irrelevant since we know that there is no corresponding text available for those papers.\n\n\n\n\nstart_time = time.time()\n\nconcise_s2orc_to_oa_doi_lookup = {\n      \"$lookup\": {\n         \"from\": \"works_oa_test\",\n         \"localField\": \"doi\",\n         \"foreignField\": \"doi\",\n         \"let\": { \"s2orc_doi\": \"$doi\" },\n         \"pipeline\": [ {\n            \"$match\": {\n               \"$expr\": { \"$eq\": [ \"$$s2orc_doi\", \"$doi\" ] }\n            }\n         } ],\n         \"as\": \"matches\"\n      }\n   }\n\nres = list(db.paper_test.aggregate( [\n    s2orc_doi_is_string,\n    concise_s2orc_to_oa_doi_lookup,\n    only_keep_existing_matches\n] )) \n\nprint(\"Process finished --- %s seconds ---\" % (time.time() - start_time))\nprint(f\"# hit: {len(res)}\")\n\nProcess finished --- 35.80504655838013 seconds ---\n# hit: 92287\n\n\nIt took longer, which is not great. But this approach let us filter doi in the foreign collection, potentially reducing the items we have to search:\n\nstart_time = time.time()\n\noa_doi_is_string_pipeline = [ {\n    \"$match\": {\n        \"doi\": { \"$type\": \"string\" } ,\n        \"$expr\": { \"$eq\": [ \"$$s2orc_doi\", \"$doi\" ] }\n        }\n    } ]\n\nconcise_s2orc_to_oa_doi_lookup = {\n      \"$lookup\": {\n         \"from\": \"works_oa_test\",\n         \"localField\": \"doi\",\n         \"foreignField\": \"doi\",\n         \"let\": { \"s2orc_doi\": \"$doi\" },\n         \"pipeline\": oa_doi_is_string_pipeline,\n         \"as\": \"matches\"\n      }\n   }\n\nres = list(db.paper_test.aggregate( [\n    s2orc_doi_is_string,\n    concise_s2orc_to_oa_doi_lookup,\n    only_keep_existing_matches\n] )) \n\nprint(\"Process finished --- %s seconds ---\" % (time.time() - start_time))\nprint(f\"# hit: {len(res)}\")\n\nProcess finished --- 32.44704747200012 seconds ---\n# hit: 92287\n\n\nFor some reason, it didn’t help that much. Perhaps because of the change in variable done by let? In general, we will prefer the traditional method. The concise method is useful when you need more complex query (see documentation)\n\n\n\nHow does filtering by concept improve our query? Lets start by creating an indexes, as we know this is just better (for now):\n\ndb.works_oa_test.create_index([(\"concepts.display_name\", ASCENDING)])\n\n'concepts.display_name_1'\n\n\n\nstart_time = time.time()\n\noa_filter = { '$match': { 'concepts.display_name': 'Biology',  'doi': { \"$type\" : \"string\"} } }\n\nres = list(db.works_oa_test.aggregate( [\n    oa_filter,\n    oa_to_s2orc_doi_lookup,\n    only_keep_existing_matches\n] ))\n\nprint(\"Process finished --- %s seconds ---\" % (time.time() - start_time))\nprint(f\"# hit: {len(res)}\")\n\nProcess finished --- 5.849428415298462 seconds ---\n# hit: 14085\n\n\nThis is much better, obviously! What if we only look at concepts.display_name at level=0, given that Biology happens only at that level.\n\ndb.works_oa_test.drop_index([(\"concepts.display_name\", ASCENDING)])\ndb.works_oa_test.create_index([(\"concepts.display_name\", ASCENDING)], partialFilterExpression = { \"concepts.level\" : 0})\n\n'concepts.display_name_1'\n\n\n\nstart_time = time.time()\n\nres = list(db.works_oa_test.aggregate( [\n    oa_filter,\n    oa_to_s2orc_doi_lookup,\n    only_keep_existing_matches\n] ))\n\nprint(\"Process finished --- %s seconds ---\" % (time.time() - start_time))\nprint(f\"# hit: {len(res)}\")\n\nProcess finished --- 6.410366535186768 seconds ---\n# hit: 14085\n\n\nFor some reason, it didn’t really help. We’ll go back to the original index:\n\ndb.works_oa_test.drop_index([(\"concepts.display_name\", ASCENDING)])\ndb.works_oa_test.create_index([(\"concepts.display_name\", ASCENDING)])\n\n'concepts.display_name_1'\n\n\nNow, doing the same for s2fieldsofstudy from paper_test collection:\n\ndb.paper_test.create_index([(\"s2fieldsofstudy.category\", ASCENDING)])\n\n's2fieldsofstudy.category_1'\n\n\n\nstart_time = time.time()\n\ns2orc_filter = { '$match': { 's2fieldsofstudy.category': 'Biology', 'doi': { \"$type\" : \"string\"}  } }\n\nres = list(db.paper_test.aggregate( [\n    s2orc_filter,\n    s2orc_to_oa_doi_lookup,\n    only_keep_existing_matches\n] ))\n\nprint(\"Process finished --- %s seconds ---\" % (time.time() - start_time))\nprint(f\"# hit: {len(res)}\")\n\nProcess finished --- 4.417508602142334 seconds ---\n# hit: 9003\n\n\nOk, now that we have indexes dois and fields on both collections, do we save time by using concise method?\n\nstart_time = time.time()\n\noa_filter_pipeline = [ {\n    \"$match\": { \n        \"concepts.display_name\": \"Biology\",  \n        \"$expr\":  {\n            \"$eq\": [ \"$$s2orc_doi\", \"$doi\" ]\n            }\n        }\n    } ]\n\nconcise_s2orc_to_oa_doi_lookup_concise = {\n      \"$lookup\": {\n         \"from\": \"works_oa_test\",\n         \"localField\": \"doi\",\n         \"foreignField\": \"doi\",\n         \"let\": { \"s2orc_doi\": \"$doi\" },\n         \"pipeline\": oa_filter_pipeline,\n         \"as\": \"matches\"\n      }\n   }\n\nres = list(db.paper_test.aggregate( [\n   s2orc_filter,\n   concise_s2orc_to_oa_doi_lookup_concise,\n   only_keep_existing_matches\n] ))\n\nprint(\"Process finished --- %s seconds ---\" % (time.time() - start_time))\nprint(f\"# hit: {len(res)}\")\n\nProcess finished --- 4.238145351409912 seconds ---\n# hit: 6396\n\n\nIt didn’t help that much, but didn’t make it worst either.\n\n\n\nWhat if we add a compound indexes instead of having independent ones?\n\ndb.works_oa_test.drop_index([(\"doi\", ASCENDING)])\ndb.works_oa_test.drop_index([(\"concepts.display_name\", ASCENDING)])\ndb.works_oa_test.create_index([(\"concepts.display_name\", ASCENDING), (\"doi\", ASCENDING)])\n\ndb.paper_test.drop_index([(\"doi\", ASCENDING)])\ndb.paper_test.drop_index([(\"s2fieldsofstudy.category\", ASCENDING)])\ndb.paper_test.create_index([(\"s2fieldsofstudy.category\", ASCENDING), (\"doi\", ASCENDING)])\n\n's2fieldsofstudy.category_1_doi_1'\n\n\n\nstart_time = time.time()\n\nres = list(db.paper_test.aggregate( [\n   s2orc_filter,\n   concise_s2orc_to_oa_doi_lookup_concise,\n   only_keep_existing_matches\n] ))\n\nprint(\"Process finished --- %s seconds ---\" % (time.time() - start_time))\nprint(f\"# hit: {len(res)}\")\n\nProcess finished --- 4.439758777618408 seconds ---\n# hit: 6396\n\n\nAbout the same. Maybe this is because these are small collections. We’ll go back to independent index for the moment:\n\ndb.works_oa_test.drop_index([(\"concepts.display_name\", ASCENDING), (\"doi\", ASCENDING)])\ndb.works_oa_test.create_index([(\"concepts.display_name\", ASCENDING)])\ndb.works_oa_test.create_index([(\"doi\", ASCENDING)])\n\ndb.paper_test.drop_index([(\"s2fieldsofstudy.category\", ASCENDING), (\"doi\", ASCENDING)])\ndb.paper_test.create_index([(\"s2fieldsofstudy.category\", ASCENDING)])\ndb.paper_test.create_index([(\"doi\", ASCENDING)])\n\n'doi_1'\n\n\n\n\n\n\nOk, remember that we have ids other than DOIs:\n\ndb.paper_test.find_one({ \"doi\":  None, \"externalids.MAG\": { \"$ne\": None } })\n\n{'_id': ObjectId('63e50688a64b9c3ca0a79ffd'),\n 'externalids': {'ACL': None,\n  'DBLP': None,\n  'ArXiv': None,\n  'MAG': '580729734',\n  'CorpusId': '231379768',\n  'PubMed': None,\n  'DOI': None,\n  'PubMedCentral': None},\n 's2fieldsofstudy': None,\n 'oa_works': True,\n 'doi': None}\n\n\nIs there papers with mag ids but no dois?\n\nprint(len(list(db.works_oa_test.find({ \"doi\":  None, \"ids.mag\": { \"$ne\": None } }, {\"ids\": 1, \"doi\": 1}))))\n\n204418\n\n\nWe have hits, which is a good sign. But waiit a minute, we also note that both collections don’t have the same types for their MAG field. We’ll fix that by converting works_oa_test field to become string (and create indexes at the same time):\n\ndb.works_oa_test.update_many(\n    {},\n    [\n        { \"$addFields\": { \"mag\": { \"$toString\": \"$ids.mag\" } } }\n    ]\n)\n\ndb.works_oa_test.create_index([(\"mag\", ASCENDING)])\ndb.paper_test.create_index([(\"externalids.MAG\", ASCENDING)])\n\n# helpers\noa_mag_is_string = { \"$match\": { \"mag\": { \"$type\": \"string\" }  } }\n\nWhat we really care about is getting papers that have MAGs but no DOIs:\n\nstart_time = time.time()\n\ns2orc_paper_filter = {\n            \"$match\": { \n               \"doi\": { \"$type\": \"null\" }, \n               \"$expr\":  { \n                    \"$eq\": [ \"$$works_oa_mag\", \"$externalids.MAG\" ] \n                }\n            }\n         }\n\nworks_oa2paper_lookup_concise = {\n      \"$lookup\": {\n         \"from\": \"paper_test\",\n         \"localField\": \"mag\",\n         \"foreignField\": \"externalids.MAG\",\n         \"let\": { \"works_oa_mag\": \"$mag\" },\n         \"pipeline\": [ s2orc_paper_filter ],\n         \"as\": \"matches\"\n      }\n   }\n\noa_bio_filter_mag_is_string = {\"$match\": {\"concepts.display_name\": \"Biology\", \"mag\": { \"$type\": \"string\" }}}\n\nres = list(db.works_oa_test.aggregate( [\n    oa_bio_filter_mag_is_string,\n    works_oa2paper_lookup_concise,\n    only_keep_existing_matches\n] ))\n\nprint(\"Process finished --- %s seconds ---\" % (time.time() - start_time))\nprint(f\"# hit: {len(res)}\")\n\nProcess finished --- 16.333527088165283 seconds ---\n# hit: 43754\n\n\nBecause this is the 1960s, note that we have many papers with MAG but no DOIs. This takes longer. Looking only from the s2orc side, we know it should be better:\n\nstart_time = time.time()\n\nworks_s2orc2oa_lookup = {\n      \"$lookup\": {\n         \"from\": \"works_oa_test\",\n         \"localField\": \"externalids.MAG\",\n         \"foreignField\": \"mag\",\n         \"as\": \"matches\"\n      }\n   }\n\ns2orc_bio_filter_mag_is_string = { \"$match\": {\"s2fieldsofstudy.category\": \"Biology\", \"externalids.MAG\": { \"$type\": \"string\" }, \"doi\": { \"$type\": \"null\" } } }\n\nres = list(db.paper_test.aggregate( [\n    s2orc_bio_filter_mag_is_string,\n    works_s2orc2oa_lookup,\n    only_keep_existing_matches\n] ))\n\nprint(\"Process finished --- %s seconds ---\" % (time.time() - start_time))\nprint(f\"# hit: {len(res)}\")\n\nProcess finished --- 8.601664304733276 seconds ---\n# hit: 26356\n\n\nAgain, using the concise metho and combining the filters:\n\nstart_time = time.time()\n\noa_paper_filter = {\n            \"$match\": { \n               \"doi\": { \"$type\": \"null\" }, \n               \"$expr\":  { \n                    \"$eq\": [ \"$$s2orc_mag\", \"$mag\" ] \n                }\n            }\n         }\n\nworks_s2orc2oa_lookup_concise = {\n      \"$lookup\": {\n         \"from\": \"works_oa_test\",\n         \"localField\": \"externalids.MAG\",\n         \"foreignField\": \"mag\",\n         \"let\": { \"s2orc_mag\": \"$externalids.MAG\" },\n         \"pipeline\": [ oa_paper_filter ],\n         \"as\": \"matches\"\n      }\n   }\n\ns2orc_bio_filter_mag_is_string = { \"$match\": {\"s2fieldsofstudy.category\": \"Biology\", \"externalids.MAG\": { \"$type\": \"string\" }, \"doi\": { \"$type\": \"null\" } } }\n\nres = list(db.paper_test.aggregate( [\n    s2orc_bio_filter_mag_is_string,\n    works_s2orc2oa_lookup,\n    only_keep_existing_matches\n] ))\n\nprint(\"Process finished --- %s seconds ---\" % (time.time() - start_time))\nprint(f\"# hit: {len(res)}\")\n\nProcess finished --- 8.963917016983032 seconds ---\n# hit: 26356\n\n\nIt seems pretty similar, but we’ll stick with that approach.\n\n\n\n\n\n\nTip\n\n\n\nIt is useful to test filters individually on both collections. You want to make sure that something did not went wrong and you actually are giving empty collections to your lookup operation. Also, always check your types. Here externalids.MAG and ids.mag were of different types, I had to add a field to works_oa that converted ids.mag as string.\n\n\n\n\nWe’ll augment S2ORC with OA using DOIs and MAGs in two steps. First using DOIs, without consideration for mag:\n\nstart_time = time.time()\n\ns2orc_bio_filter_doi_is_string = { '$match': { 's2fieldsofstudy.category': 'Biology', 'doi': { \"$type\" : \"string\"}  } }\n\nconcise_s2orc_to_oa_doi_lookup = {\n      \"$lookup\": {\n         \"from\": \"works_oa_test\",\n         \"localField\": \"doi\",\n         \"foreignField\": \"doi\",\n         \"as\": \"matches\"\n      }\n   }\n\nres = list(db.paper_test.aggregate( [\n   s2orc_bio_filter_doi_is_string,\n   concise_s2orc_to_oa_doi_lookup,\n   only_keep_existing_matches,\n   { '$addFields': { 'works_oa': {'$cond': [{'$ne': ['$matches', []]}, \"$matches\", None]} } },\n   { '$project': { 'matches': 0 } }\n] ))\n\nprint(\"Aggregation pipeline finished --- %s seconds ---\" % (time.time() - start_time))\n\ndb.paper_test.bulk_write([\n    UpdateOne( { 'doi': doc['doi'] }, { '$set': {'works_oa': doc['works_oa'][0]} })\n    for doc in res\n])\n\nprint(\"Process finished --- %s seconds ---\" % (time.time() - start_time))\n\nAggregation pipeline finished --- 3.0898585319519043 seconds ---\n\n\nProcess finished --- 4.899336338043213 seconds ---\n\n\nThen, we augment paper with MAG but no DOIs. This is the big operation:\n\nstart_time = time.time()\n\ns2orc_bio_filter_mag_is_string = { \"$match\": {\"s2fieldsofstudy.category\": \"Biology\", \"externalids.MAG\": { \"$type\": \"string\" }, \"doi\": { \"$type\": \"null\" } } }\n\nconcise_s2orc_to_oa_mag_lookup = {\n      \"$lookup\": {\n         \"from\": \"works_oa_test\",\n         \"localField\": \"externalids.MAG\",\n         \"foreignField\": \"mag\",\n         \"as\": \"matches\"\n      }\n   }\n\nres = list(db.paper_test.aggregate( [\n   s2orc_bio_filter_mag_is_string,\n   concise_s2orc_to_oa_mag_lookup,\n   only_keep_existing_matches,\n   { '$addFields': { 'works_oa': {'$cond': [{'$ne': ['$matches', []]}, \"$matches\", None]} } },\n   { '$project': { 'matches': 0 } }\n] ))\n\nprint(\"Aggregation pipeline finished --- %s seconds ---\" % (time.time() - start_time))\nprint(f\"# hit: {len(res)}\")\n\ndb.paper_test.bulk_write([ \n    UpdateOne( {'externalids.MAG': doc['externalids'][\"MAG\"]}, {'$set': {'works_oa': doc['works_oa'][0]}} ) \n    for doc in res\n    ])\n\nprint(\"Process finished --- %s seconds ---\" % (time.time() - start_time))\n\nAggregation pipeline finished --- 5.590967416763306 seconds ---\n# hit: 26356\n\n\nProcess finished --- 10.278563022613525 seconds ---\n\n\nChecking the modified collection:\n\ndb.paper_test.find_one({'externalids.MAG':  res[0]['externalids']['MAG']})\n\n{'_id': ObjectId('63e50688a64b9c3ca0a7a375'),\n 'externalids': {'ACL': None,\n  'DBLP': None,\n  'ArXiv': None,\n  'MAG': '2462552499',\n  'CorpusId': '46587026',\n  'PubMed': '13854036',\n  'DOI': None,\n  'PubMedCentral': None},\n 's2fieldsofstudy': [{'category': 'Biology', 'source': 's2-fos-model'},\n  {'category': 'Chemistry', 'source': 'external'},\n  {'category': 'Medicine', 'source': 'external'}],\n 'oa_works': True,\n 'doi': None,\n 'works_oa': {'_id': ObjectId('64227602249c314cafa2ee23'),\n  'doi': None,\n  'publication_date': '1960-03-01',\n  'ids': {'openalex': 'https://openalex.org/W2462552499',\n   'pmid': 'https://pubmed.ncbi.nlm.nih.gov/13854036',\n   'mag': 2462552499},\n  'authorships': [{'institutions': []},\n   {'institutions': []},\n   {'institutions': []}],\n  'concepts': [{'id': 'https://openalex.org/C2779317461',\n    'wikidata': 'https://www.wikidata.org/wiki/Q16986094',\n    'display_name': 'Mycobacterium phlei',\n    'level': 4,\n    'score': 0.97717744},\n   {'id': 'https://openalex.org/C2776967927',\n    'wikidata': 'https://www.wikidata.org/wiki/Q423169',\n    'display_name': 'Isoniazid',\n    'level': 3,\n    'score': 0.80283976},\n   {'id': 'https://openalex.org/C89423630',\n    'wikidata': 'https://www.wikidata.org/wiki/Q7193',\n    'display_name': 'Microbiology',\n    'level': 1,\n    'score': 0.6403618},\n   {'id': 'https://openalex.org/C181199279',\n    'wikidata': 'https://www.wikidata.org/wiki/Q8047',\n    'display_name': 'Enzyme',\n    'level': 2,\n    'score': 0.59864265},\n   {'id': 'https://openalex.org/C2777975735',\n    'wikidata': 'https://www.wikidata.org/wiki/Q130971',\n    'display_name': 'Mycobacterium tuberculosis',\n    'level': 3,\n    'score': 0.52854174},\n   {'id': 'https://openalex.org/C185592680',\n    'wikidata': 'https://www.wikidata.org/wiki/Q2329',\n    'display_name': 'Chemistry',\n    'level': 0,\n    'score': 0.3759841},\n   {'id': 'https://openalex.org/C86803240',\n    'wikidata': 'https://www.wikidata.org/wiki/Q420',\n    'display_name': 'Biology',\n    'level': 0,\n    'score': 0.37109792},\n   {'id': 'https://openalex.org/C2780374374',\n    'wikidata': 'https://www.wikidata.org/wiki/Q194309',\n    'display_name': 'Mycobacterium',\n    'level': 3,\n    'score': 0.34876233},\n   {'id': 'https://openalex.org/C55493867',\n    'wikidata': 'https://www.wikidata.org/wiki/Q7094',\n    'display_name': 'Biochemistry',\n    'level': 1,\n    'score': 0.2363849},\n   {'id': 'https://openalex.org/C71924100',\n    'wikidata': 'https://www.wikidata.org/wiki/Q11190',\n    'display_name': 'Medicine',\n    'level': 0,\n    'score': 0.22486079},\n   {'id': 'https://openalex.org/C2781069245',\n    'wikidata': 'https://www.wikidata.org/wiki/Q12204',\n    'display_name': 'Tuberculosis',\n    'level': 2,\n    'score': 0.18608537},\n   {'id': 'https://openalex.org/C54355233',\n    'wikidata': 'https://www.wikidata.org/wiki/Q7162',\n    'display_name': 'Genetics',\n    'level': 1,\n    'score': 0.10660368},\n   {'id': 'https://openalex.org/C523546767',\n    'wikidata': 'https://www.wikidata.org/wiki/Q10876',\n    'display_name': 'Bacteria',\n    'level': 2,\n    'score': 0.08527815},\n   {'id': 'https://openalex.org/C142724271',\n    'wikidata': 'https://www.wikidata.org/wiki/Q7208',\n    'display_name': 'Pathology',\n    'level': 1,\n    'score': 0.045282602}],\n  'mag': '2462552499'}}\n\n\nThe next step is to apply what we learn on the full database. As we modify our main databaset, we’ll do it in a script that lives in our directory just for that."
  },
  {
    "objectID": "posts/mongoDB/index.html#advanced-query",
    "href": "posts/mongoDB/index.html#advanced-query",
    "title": "How to MongoDB",
    "section": "Advanced query",
    "text": "Advanced query\n\nComplex lookups\nOkay, we want to perform a $lookup to know which DOI in papers (s2orc) are also in works_oa. This is a costly operation, so we make sure to have the right index first (we created a doi field in papers). We need the index publication_year/year, concepts/s2fieldofstudy, and doi.\ndb.works_oa.create_index([(\"publication_year\", ASCENDING), (\"concepts.diplay_name\", ASCENDING), (\"doi\", ASCENDING)])\ndb.papers.create_index([(\"year\", ASCENDING), (\"s2fieldsofstudy.category\", ASCENDING), (\"doi\", ASCENDING)])\npipeline = [\n    {\n        \"$match\": { \n             \"$and\":  [ \n                { \"year\": 1960 },\n                # { \"s2fieldsofstudy.category\", \"Biology\"  }\n            ]\n        }\n   },\n   {\n      \"$lookup\": {\n         \"from\": \"works_oa\",\n         \"localField\": \"doi\",\n         \"foreignField\": \"doi\",\n         \"let\": { \"col1_doi\": \"$doi\" },\n         \"pipeline\": [ {\n            \"$match\": {\n               \"$expr\": { \n                \"$and\": [\n                    { \"$eq\": [ \"publication_date\", \"1960-12-30\" ] },\n                    { \"$eq\": [ \"concepts.display_name\", \"Biology\" ] },\n                    { \"$in\": [ \"$$col1_doi\", \"$doi\" ] }\n                ]\n                }\n            }\n         } ],\n         \"as\": \"matches\"\n      }\n   },\n   {\n        \"$match\": { \"matches\": { \"$ne\": [] } }\n    }\n] \n\nres = list(db.paper_test.aggregate(pipeline))"
  }
]