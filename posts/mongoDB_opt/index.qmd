---
title: "How to accelerate MongoDB"
description: |
  Doubt everything and verify.
categories:
  - ref
  - mongoDB
execute:
  cache: true
toc: true
number-sections: false
format: 
   html:
      code-line-numbers: true
date: today
author: Jonathan St-Onge
editor: visual
image: Podracing.png
---

In this case study, we want to augment metadata of papers from allenAi's `S2ORC` database with metadata from `openAlex`. Both metadata are represented as mongoDB collections in our local database. Augmented one collection with another in mongoDB amounts to perform a`$lookup` from `papers` (s2orc) to `works_oa` (openAlex).

```{python}
#| echo: false 
from creds import client
from bson.objectid import ObjectId
from pymongo import ASCENDING, UpdateOne
from IPython.display import display
import pandas as pd
# from pympler import asizeof
from pprint import pprint
import time
db = client['papersDB']
```

```{python}
#| echo: false
def reinitialize_fields():
    db.works_oa_test.update_many({}, { '$unset': { 'mag': '' } })
    db.papers_test.update_many({}, { '$unset': { 'doi': '' } })

def reset_indexes():
   [db.papers_test.drop_index(i) for i in db.papers_test.index_information() if i != "_id_"]
   [db.works_oa_test.drop_index(i) for i in db.works_oa_test.index_information() if i != "_id_"]

# reinitialize_fields()
reset_indexes()
```

```{python}
#| code-fold: true
return_top_5 = { "$limit": 5 }
only_keep_existing_matches = { "$match": { "matches": { "$ne": [] } } }
```

### In theory

We'll try to understand the following strategies from the documentation:
 - [Create Queries that Ensure Selectivity](https://www.mongodb.com/docs/manual/tutorial/create-queries-that-ensure-selectivity/#std-label-index-selectivity)
 - [The ESR (Equality, Sort, Range) Rule](https://www.mongodb.com/docs/manual/tutorial/equality-sort-range-rule/)

Basically, `selectivity` is the yes/no question game again. Low selective questions are the one you use to get rid of as many people as you can. Highly selective question are basically asking if it is this guy or this guy. The documentation suggest by starting a compound index that starts with low-selectivity (is this a famous person?), then you get to the more nitti-gritty. This is supposed to be intuitive. 

In our case, we have from lower selectivity to highest selectivity:

 - s2fieldofstudy (n=24)
 - concepts (42K total; but lvl=0/n=21; lvl=0)
 - year (~270, but unevenly distributed)
 - journals/venues
 - doi/mag

```{python}
len(db.papers.distinct("year"))
len(db.papers.distinct("s2fieldsofstudy.category"))
len(db.works_oa_test.distinct("concepts.display_name"))
```

### 1960 {#sec-simplifying}

The first step of optimizing our code is to work with simpler collections. We will limit our analysis to papers from 1960. Here is the pymongo code to create collections: 

```{python}
#| eval: false
#| code-fold: true

db.works_oa_test.insert_many( list(db.works_oa.find(
    { "publication_year": { "$in": [1960, 1961] }},
    { "concepts.display_name": 1, "publication_year": 1, "doi": 1, "ids": 1 }
    )) )

db.papers_test.insert_many( list(db.papers.find(
    { "year": { "$in": [1960, 1961] } },
    { "doi": 1, "externalids.MAG": 1, "s2fieldsofstudy": 1 }
    ))
)
```

```{python}
#| echo: false
count_1960_s2orc = db.papers_test.count_documents({})
tot_s2orc = db.papers.estimated_document_count()
count_1960_oa = db.works_oa_test.count_documents({})
tot_oa = db.works_oa.estimated_document_count()

print(f"S2orc has {count_1960_s2orc} papers with field of study in 1960 ({round(count_1960_s2orc / tot_s2orc * 100, 2)}% of corpus)")
print(f"OpenAlex has {count_1960_oa} papers in 1960 ({round(count_1960_oa / tot_oa * 100, 2)}% of corpus)")

# how many papers with dois in full s2orc corpus?
# len(list(db.papers.find({"doi": {"$type": "string"}})))
# how many papers with dois in full s2orc corpus?
# len(list(db.papers.find({"doi": {"$type": "string"}})))
```

1960 is .2% of our whole corpus. We should keep that in mind as we try to scale up our operations to the full dataset. A document in our simplified `S2ORC` collection looks like:

```{python}
#| echo: false
pprint(db.papers_test.find_one({ "doi": { "$type": "string" } }))
```

A document in our simplified `openAlex` collection looks like:

```{python}
#| echo: false
pprint(db.works_oa_test.find_one())
```

We're off to a bad start. We note that the `s2orc` DB don't keep the address prefix of the DOI. We find easier to add the missing prefix in `s2orc` than remove it from `openalex`.

```{python}
#| code-fold: true
start_time = time.time()
db.papers_test.update_many(
    {},
    [
        { "$set": { 
            "doi": { 
                "$cond": [
                    { "$ne": ["$externalids.DOI", None] },
                    { "$concat": ["https://doi.org/", "$externalids.DOI"] },
                    None
                ]
                }
            } }
    ]
)

print("Setting a field on ~400K took --- %s seconds ---" % (time.time() - start_time))
```

This cell took about 11sec for 400k, this meens that (100/.2)*11 / 60 = 83 minutes to run this on the full corpus. 

```{python}
print("hey")
# res = list(db.works_oa.aggregate( [
#    { '$addFields': { 
#       'main_concept': { 
#          "$filter": { 
#             "input": "$concepts", 
#             "as": "c", 
#             "cond": { "$eq": ["$$c.level", 0 ]}
#             }
#          }
#       }
#    },
#    {
#       "$project": { "main_concept.display_name": 1 }
#    }
# ] ))

# db.works_oa.bulk_write([ 
#     UpdateOne( {'_id': ObjectId(doc['_id'])}, {'$set': {'main_concept': doc['main_concept']['display_name']}} ) 
#     for doc in res
#     ])
```

### What's the worst query we can do?

We can now proceed to our first lookup. As a very first step, note that we will match only on existing dois on the `papers_test` (s2orc) side. If not, this is not even gonna run because the output is too large:

```{python}
#| eval: false
start_time = time.time()

s2orc_doi_is_string = { "$match": { "doi": { "$type": "string" }} }

s2orc_to_oa_doi_lookup ={
      "$lookup": {
         "from": "works_oa_test",
         "localField": "doi",
         "foreignField": "doi",
         "as": "matches"
      }
   }

pipeline = [
   s2orc_doi_is_string,
   s2orc_to_oa_doi_lookup,
   only_keep_existing_matches,
]

assert 'doi' in db.papers_test.find_one().keys(), 'missing field in paper test'
assert 'doi' in db.works_oa_test.find_one().keys(), 'missing field in oa'

res = list(db.papers_test.aggregate( pipeline ))
print("Process finished --- %s seconds ---" % (time.time() - start_time))
```

We ended up cancelling the task because it was still too long (> 3mins is too long for me). I just wanted to show this step to mention that when we work with mongoDB (our databases in general), indexes make a world of difference.

::: {.callout-tip}
Assigning descriptive names to variables for each aggregation stage enhances the reader's comprehension of the purpose of the stage while emphasizing the similarities between the optimization variations being performed. For example, the example below clearly demonstrates that we are executing the same pipeline both with and without indexes.
:::

### Index on `doi`

Here's how we go about creating indexes

```{python}
db.works_oa_test.create_index([("doi", ASCENDING)])
db.papers_test.create_index([("doi", ASCENDING)])
```

Matching `papers_test` on `works_oa` again:

```{python}
start_time = time.time()

s2orc_doi_is_string = { "$match": { "doi": { "$type": "string" }} }

s2orc_to_oa_doi_lookup ={
      "$lookup": {
         "from": "works_oa_test",
         "localField": "doi",
         "foreignField": "doi",
         "as": "matches"
      }
   }

res = list(db.papers_test.aggregate( [
    s2orc_doi_is_string,
    s2orc_to_oa_doi_lookup,
    only_keep_existing_matches
] ))

f3 = "$type(doi)=string"
i3 = "doi"
d3 = "2orc => oa"
time3 = time.time() - start_time
print("Process finished --- %s seconds ---" % time3)
print(f"# hit: {len(res)}")
```

Also, lets check if the other way around is faster

```{python}
start_time = time.time()

oa_doi_is_string = { "$match": { "doi": { "$type": "string" }} }

oa_to_s2orc_doi_lookup = {
      "$lookup": {
         "from": "papers_test",
         "localField": "doi",
         "foreignField": "doi",
         "as": "matches"
      }
   }

res = list(db.works_oa_test.aggregate( [
    oa_doi_is_string,
    oa_to_s2orc_doi_lookup,
    only_keep_existing_matches
] ))

time4 = time.time() - start_time
print("Process finished --- %s seconds ---" % time4)
print(f"# hit: {len(res)}")
```

Looking up `papers_test` in `works_oa` took 28 seconds, while it took 30 seconds for the reverse direction. This make sense considering that `papers_test` contains fewer papers. If the information we are interested in resides in `papers_test`, it is better to prioritize that direction. For example, if our focus is on text analysis and we only have text data for papers in `papers_test` (`S2ORC`), then the additional papers in `openAlex` become irrelevant since we know that there is no corresponding text available for those papers. 

#### Trying out the concise method for lookup introduced in MongoDB 5.0

```{python}
start_time = time.time()

concise_s2orc_to_oa_doi_lookup = {
      "$lookup": {
         "from": "works_oa_test",
         "localField": "doi",
         "foreignField": "doi",
         "let": { "s2orc_doi": "$doi" },
         "pipeline": [ {
            "$match": {
               "$expr": { "$eq": [ "$$s2orc_doi", "$doi" ] }
            }
         } ],
         "as": "matches"
      }
   }

res = list(db.papers_test.aggregate( [
    s2orc_doi_is_string,
    concise_s2orc_to_oa_doi_lookup,
    only_keep_existing_matches
] )) 

time5 = time.time() - start_time
print("Process finished --- %s seconds ---" % time5)
print(f"# hit: {len(res)}")
```

It took longer, which is not great. But this approach let us filter doi in the `foreign` collection, potentially reducing the items we have to search:

```{python}

start_time = time.time()

oa_doi_is_string_pipeline = [ {
    "$match": {
        "doi": { "$type": "string" } ,
        "$expr": { "$eq": [ "$$s2orc_doi", "$doi" ] }
        }
    } ]

concise_s2orc_to_oa_doi_lookup = {
      "$lookup": {
         "from": "works_oa_test",
         "localField": "doi",
         "foreignField": "doi",
         "let": { "s2orc_doi": "$doi" },
         "pipeline": oa_doi_is_string_pipeline,
         "as": "matches"
      }
   }

res = list(db.papers_test.aggregate( [
    s2orc_doi_is_string,
    concise_s2orc_to_oa_doi_lookup,
    only_keep_existing_matches
] )) 

time6 = time.time() - start_time
print("Process finished --- %s seconds ---" % time6)
print(f"# hit: {len(res)}")
```

For some reason, it didn't help that much. Perhaps because of the change in variable done by `let`? In general, we will prefer the traditional method. The concise method is useful when you need more complex query (see [documentation](https://www.mongodb.com/docs/manual/reference/operator/aggregation/lookup/#std-label-lookup-concise-correlated-subquery))

### Filtering by `concepts`

How does filtering by concept improve our query? Lets start by creating an indexes, as we know this is just better (for now):

```{python}
db.works_oa_test.create_index([("concepts.display_name", ASCENDING)])
```

```{python}
start_time = time.time()

oa_filter = { '$match': { 'concepts.display_name': 'Biology',  'doi': { "$type" : "string"} } }

res = list(db.works_oa_test.aggregate( [
    oa_filter,
    oa_to_s2orc_doi_lookup,
    only_keep_existing_matches
] ))

print("Process finished --- %s seconds ---" % (time.time() - start_time))
print(f"# hit: {len(res)}")
```

This is much better, obviously! What if we only look at `concepts.display_name` at `level=0`, given that `Biology` happens only at that level.

```{python}
db.works_oa_test.drop_index([("concepts.display_name", ASCENDING)])
db.works_oa_test.create_index([("concepts.display_name", ASCENDING)], partialFilterExpression = { "concepts.level" : 0})
```

```{python}
start_time = time.time()

res = list(db.works_oa_test.aggregate( [
    oa_filter,
    oa_to_s2orc_doi_lookup,
    only_keep_existing_matches
] ))

time7 = time.time() - start_time
print("Process finished --- %s seconds ---" % time7)
print(f"# hit: {len(res)}")
```


For some reason, it didn't really help. We'll go back to the original index:

```{python}
db.works_oa_test.drop_index([("concepts.display_name", ASCENDING)])
db.works_oa_test.create_index([("concepts.display_name", ASCENDING)])
```

Now, doing the same for `s2fieldsofstudy` from `papers_test` collection:

```{python}
db.papers_test.create_index([("s2fieldsofstudy.category", ASCENDING)])
```

```{python}
start_time = time.time()

s2orc_filter = { '$match': { 's2fieldsofstudy.category': 'Biology', 'doi': { "$type" : "string"}  } }

res = list(db.papers_test.aggregate( [
    s2orc_filter,
    s2orc_to_oa_doi_lookup,
    only_keep_existing_matches
] ))

time8 = time.time() - start_time
print("Process finished --- %s seconds ---" % time8)
print(f"# hit: {len(res)}")
```

Ok, now that we have indexes `dois` and `fields` on both collections, do we save time by using concise method?

```{python}
start_time = time.time()

oa_filter_pipeline = [ {
    "$match": { 
        "concepts.display_name": "Biology",  
        "$expr":  {
            "$eq": [ "$$s2orc_doi", "$doi" ]
            }
        }
    } ]

concise_s2orc_to_oa_doi_lookup_concise = {
      "$lookup": {
         "from": "works_oa_test",
         "localField": "doi",
         "foreignField": "doi",
         "let": { "s2orc_doi": "$doi" },
         "pipeline": oa_filter_pipeline,
         "as": "matches"
      }
   }

res = list(db.papers_test.aggregate( [
   s2orc_filter,
   concise_s2orc_to_oa_doi_lookup_concise,
   only_keep_existing_matches
] ))

time9 = time.time() - start_time
print("Process finished --- %s seconds ---" % time9)
print(f"# hit: {len(res)}")
```

It didn't help that much, but didn't make it worst either. 

### Compound indexes

```{python}
#| echo: false
def time_query():
   start_time = time.time()
   len(list(db.works_oa_test.find({ "concepts.display_name": "Biology", "publication_year": 1960 })))
   print("Bio then year --  finished --- %s seconds ---" % (time.time() - start_time))
   start_time = time.time()
   len(list(db.works_oa_test.find({ "publication_year": 1960, "concepts.display_name": "Biology" })))
   print("Year then bio -- finished --- %s seconds ---" % (time.time() - start_time))

def time_lookup(reverse_order=False):
   
   if reverse_order:
      oa_filter = { '$match': { 
         'publication_year': 1960, 
         'concepts.display_name': 'Biology',  
         'doi': { "$type" : "string"} } 
      }
      msg = "Year then Bio"
   else:
      oa_filter = { '$match': { 
         'concepts.display_name': 'Biology',  
         'publication_year': 1960, 
         'doi': { "$type" : "string"} } 
      }
      msg = "Bio then Year"
   
   oa_to_s2orc_doi_lookup = {
      "$lookup": {
         "from": "papers_test",
         "localField": "doi",
         "foreignField": "doi",
         "as": "matches"
      }
   }

   start_time = time.time()
   res = list(db.works_oa_test.aggregate( [
      oa_filter,
      oa_to_s2orc_doi_lookup,
      only_keep_existing_matches
   ] ))

   print(f"{msg} -- finished --- {(time.time() - start_time)} seconds ---")

```

What if we add a compound indexes instead of having independent ones? 

```{python}
print("year/concept/doi")
reset_indexes()
db.papers_test.create_index([("doi", ASCENDING)])
db.works_oa_test.create_index([("publication_year", ASCENDING), ("concepts.display_name", ASCENDING), ("doi", ASCENDING)])

time_query()
time_lookup()
time_lookup(reverse_order=True)


print("concept/year/doi")
reset_indexes()
db.papers_test.create_index([("doi", ASCENDING)])
db.works_oa_test.create_index([("concepts.display_name", ASCENDING), ("publication_year", ASCENDING), ("doi", ASCENDING)])

time_query()
time_lookup()
time_lookup(reverse_order=True)


# THIS TOOK FOREVER
# print("concept/year, doi")
# reset_indexes()

# db.works_oa_test.create_index([("concepts.display_name", ASCENDING), ("publication_year", ASCENDING)])
# db.works_oa_test.create_index([("doi", ASCENDING)])

# time_query()
# time_lookup()
# time_lookup(reverse_order=True)

print("concept, year, doi")
reset_indexes()
db.papers_test.create_index([("doi", ASCENDING)])
db.works_oa_test.create_index([("publication_year", ASCENDING)])
db.works_oa_test.create_index([("concepts.display_name", ASCENDING)])
db.works_oa_test.create_index([("doi", ASCENDING)])

time_query()
time_lookup()
time_lookup(reverse_order=True)
```

About the same. Maybe this is because these are small collections. We'll go back to independent index for the moment, as it is more flexible:

```{python}
db.works_oa_test.create_index([("publication_year", ASCENDING)])
db.works_oa_test.create_index([("concepts.display_name", ASCENDING)])
db.works_oa_test.create_index([("doi", ASCENDING)])

db.papers_test.create_index([("year", ASCENDING)])
db.papers_test.create_index([("s2fieldsofstudy.category", ASCENDING)])
db.papers_test.create_index([("doi", ASCENDING)])
```


### Beyond DOIs

Ok, remember that we have ids other than DOIs: 

```{python}
db.papers_test.find_one({ "doi":  None, "externalids.MAG": { "$ne": None } })
```

Is there papers with `mag` ids but no `doi`?

```{python}
res = list(db.works_oa_test.find({ "doi":  None, "ids.mag": { "$ne": None } }, {"ids": 1, "doi": 1}))
print(f"# papers with mag id but no doi : {len(res)}")
res[0]
```

We have enough hits that we should take that into account. But waiit a minute, we also note that both collections don't have the same types for their `mag` field. We'll fix that by converting `works_oa_test` field to become string (and create indexes at the same time):

```{python}
db.works_oa_test.update_many(
    {},
    [
        { "$addFields": { "mag": { "$toString": "$ids.mag" } } }
    ]
)

db.works_oa_test.create_index([("mag", ASCENDING)])
db.papers_test.create_index([("externalids.MAG", ASCENDING)])
```

What we really care about is getting papers that have `mag` but no `doi`:

```{python}
start_time = time.time()

s2orc_paper_filter = {
            "$match": { 
               "doi": { "$type": "null" }, 
               "$expr":  { 
                    "$eq": [ "$$works_oa_mag", "$externalids.MAG" ] 
                }
            }
         }

works_oa2paper_lookup_concise = {
      "$lookup": {
         "from": "papers_test",
         "localField": "mag",
         "foreignField": "externalids.MAG",
         "let": { "works_oa_mag": "$mag" },
         "pipeline": [ s2orc_paper_filter ],
         "as": "matches"
      }
   }

oa_bio_filter_mag_is_string = {"$match": {"concepts.display_name": "Biology", "mag": { "$type": "string" }}}

res = list(db.works_oa_test.aggregate( [
    oa_bio_filter_mag_is_string,
    works_oa2paper_lookup_concise,
    only_keep_existing_matches
] ))

time11 = time.time() - start_time
print("Process finished --- %s seconds ---" % time11)
print(f"# hit: {len(res)}")
```

Because this is the 1960s, note that we have many papers with `MAG` but no `DOIs`. This takes longer. Looking only from the s2orc side, we know it should be better:

```{python}
start_time = time.time()

works_s2orc2oa_lookup = {
      "$lookup": {
         "from": "works_oa_test",
         "localField": "externalids.MAG",
         "foreignField": "mag",
         "as": "matches"
      }
   }

s2orc_bio_filter_mag_is_string = { "$match": {"s2fieldsofstudy.category": "Biology", "externalids.MAG": { "$type": "string" }, "doi": { "$type": "null" } } }

res = list(db.papers_test.aggregate( [
    s2orc_bio_filter_mag_is_string,
    works_s2orc2oa_lookup,
    only_keep_existing_matches
] ))

time12 = time.time() - start_time
print("Process finished --- %s seconds ---" % time12)
print(f"# hit: {len(res)}")
```

Again, using the concise metho and combining the filters:

```{python}
start_time = time.time()

s2orc_filter = { "$match": {
   "s2fieldsofstudy.category": "Biology", 
   "externalids.MAG": { "$type": "string" }, 
   "doi": { "$type": "null" } } 
}


oa_paper_filter = {
            "$match": { 
               "doi": { "$type": "null" }, 
               "mag": { "$type": "string"},
               "$expr":  { 
                    "$eq": [ "$$s2orc_mag", "$mag" ] 
                }
            }
         }

works_s2orc2oa_lookup_concise = {
      "$lookup": {
         "from": "works_oa_test",
         "localField": "externalids.MAG",
         "foreignField": "mag",
         "let": { "s2orc_mag": "$externalids.MAG" },
         "pipeline": [ oa_paper_filter ],
         "as": "matches"
      }
   }

res = list(db.papers_test.aggregate( [
    s2orc_filter,
    works_s2orc2oa_lookup_concise,
    only_keep_existing_matches
] ))

time13 = time.time() - start_time
print("Process finished --- %s seconds ---" % time13)
print(f"# hit: {len(res)}")
```


They provide similar time, but we'll need to stick with the concise method when we work with the full dataset because we'll filter on `year` too in `oa_paper_filter` (which ought to make a big difference).

::: {.callout-tip}
It is useful to test filters individually on both collections. You want to make sure that something did not went wrong and you actually are giving empty collections to your lookup operation. Also, always check your types. Here `externalids.MAG` and `ids.mag` were of different types, I had to add a field to `works_oa` that converted `ids.mag` as string.
:::

### Augmenting S2ORC with openAlex metadata

We'll augment S2ORC with OA using DOIs and MAGs in two steps. First using DOIs, without consideration for mag:

```{python}
#| eval: false

start_time = time.time()

s2orc_filter = { "$match": {
   "s2fieldsofstudy.category": "Biology", 
   "doi": { "$type": "string" } 
   }
}

oa_filter_pipeline = [ {
    "$match": { 
        "concepts.display_name": "Biology",
        "doi": { "$type": "string" },
        "$expr":  {
            "$eq": [ "$$s2orc_doi", "$doi" ]
            }
        }
    } ]

concise_s2orc_to_oa_doi_lookup_concise = {
      "$lookup": {
         "from": "works_oa_test",
         "localField": "doi",
         "foreignField": "doi",
         "let": { "s2orc_doi": "$doi" },
         "pipeline": oa_filter_pipeline,
         "as": "matches"
      }
   }

res = list(db.papers_test.aggregate( [
   s2orc_filter,
   concise_s2orc_to_oa_doi_lookup_concise,
   only_keep_existing_matches,
   { '$addFields': { 'works_oa': {'$cond': [{'$ne': ['$matches', []]}, "$matches", None]} } },
   { '$project': { 'matches': 0 } }
] ))

print("Aggregation pipeline finished --- %s seconds ---" % (time.time() - start_time))

db.papers_test.bulk_write([
    UpdateOne( { 'doi': doc['doi'] }, { '$set': {'works_oa': doc['works_oa'][0]} })
    for doc in res
])

print("Process finished --- %s seconds ---" % (time.time() - start_time))
```

Then, we augment paper with MAG but no DOIs. This is the "big" operation:

```{python}
#| eval: false
start_time = time.time()

s2orc_filter = { "$match": {
   "s2fieldsofstudy.category": "Biology", 
   "externalids.MAG": { "$type": "string" }, 
   "doi": { "$type": "null" } } 
}

oa_paper_filter = { "$match": { 
   "doi": { "$type": "null" }, 
   "mag": { "$type": "string"},
   "$expr":  { 
      "$eq": [ "$$s2orc_mag", "$mag" ] 
      }
   }
}

concise_s2orc_to_oa_mag_lookup = {
      "$lookup": {
         "from": "works_oa_test",
         "localField": "externalids.MAG",
         "foreignField": "mag",
         "let": { "s2orc_mag": "$externalids.MAG" },
         "pipeline": [ oa_paper_filter ],
         "as": "matches"
      }
   }

res = list(db.papers_test.aggregate( [
   s2orc_filter,
   concise_s2orc_to_oa_mag_lookup,
   only_keep_existing_matches,
   { '$addFields': { 'works_oa': {'$cond': [{'$ne': ['$matches', []]}, "$matches", None]} } },
   { '$project': { 'matches': 0 } }
] ))

print("Aggregation pipeline finished --- %s seconds ---" % (time.time() - start_time))
print(f"# hit: {len(res)}")

db.papers_test.bulk_write([ 
    UpdateOne( {'externalids.MAG': doc['externalids']["MAG"]}, {'$set': {'works_oa': doc['works_oa'][0]}} ) 
    for doc in res
    ])

print("Process finished --- %s seconds ---" % (time.time() - start_time))
```

Examining the modified collection:

```{python}
db.papers_test.find_one({'externalids.MAG':  res[0]['externalids']['MAG']})
```

The next step is to apply what we learn on the full database. As we modify our main databaset, we'll do it in a script that lives in our directory just for that.

```{python}
print("hey")
# out = list(db.papers.aggregate([
#    { "$match": { 
#       "year": 1960, 
#       "s2fieldsofstudy.category": "Biology",
#       "works_oa": {"$exists": "true"},
#       }
#    },
#    { '$project': { 
#       'main_concept': { 
#          "$filter": { 
#             "input": "$works_oa.concepts", 
#             "as": "c", 
#             "cond": { "$eq": ["$$c.level", 1 ]}
#             }
#          }
#       }
#    }
   # { "$unwind": "$main_concept" }, 
   # {"$group": {
   #    "_id": {
   #       "year": "$year",
   #       "concept": "$main_concept.display_name"
   #    }, 
   #    "n_papers": {"$sum": 1}
   #    }
   # }
   # ]))

# pd.concat([
#    pd.DataFrame([_['_id'] for _ in out]),
#    pd.DataFrame({'n_papers': [_['n_papers'] for _ in out]})
# ], axis=1)

```

### Summary table.

```{python}
all_time = [time3,time4,time5,time6,time7,time8,time9,
            time11,time12,time13]
display(pd.DataFrame({"time (sec)": all_time}).to_html())
```


### How many PubMed without DOI or MAG ids?

```{python}
print("hey")
# pubmed = list(db.papers.aggregate([
#    {
#       "$match": {
#          "doi": None, "externalids.MAG": None, "externalids.PubMed": {"$ne": None}
#       }
#    },
#    {
#       "$group": {
#          "_id": {
#             "year": "$year"
#          },
#          "n_papers": {"$sum": 1} 
#       }
#    },
#    ]))

# df_pubmed = pd.concat([
#    pd.DataFrame([{'year': _['_id']['year']} for _ in pubmed]),
#    pd.DataFrame([{'n': int(_['n_papers'])} for _ in pubmed])
# ], axis=1)

# df_pubmed = df_pubmed[~df_pubmed.year.isna()]
# df_pubmed.year = pd.to_datetime(df_pubmed.year, format="%Y")
# df_pubmed.set_index("year").plot()

# acl = list(db.papers.aggregate([
#    {
#       "$match": {
#          "doi": None, "externalids.MAG": None, "externalids.PubMed": None, "externalids.ACL": {"$ne": None}
#       }
#    },
#    {
#       "$group": {
#          "_id": {
#             "year": "$year"
#          },
#          "n_papers": {"$sum": 1} 
#       }
#    },
#    ]))

# df_acl = pd.concat([
#    pd.DataFrame([{'year': _['_id']['year']} for _ in acl]),
#    pd.DataFrame([{'n': int(_['n_papers'])} for _ in acl])
# ], axis=1)

# df_acl = df_acl[~df_acl.year.isna()]
# df_acl.year = pd.to_datetime(df_acl.year, format="%Y")
# df_acl.set_index("year").plot()
```
